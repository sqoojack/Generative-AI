[
  {
    "title": "How Document Pre-processing affects Keyphrase Extraction Performance",
    "evidence": [
      " dataset BIBREF0 is composed of 244 scientific articles collected from the ACM Digital Library (conference and workshop papers). The input papers ranged from 6 to 8 pages and were converted from PDF format to plain text using an off-the-shelf tool. The only preprocessing applied is a systematic dehyphenation at line breaks and removal of author-assigned keyphrases.",
      " we present the first consistent comparison of different keyphrase extraction models and study their robustness over noisy text. More precisely, our contributions are: Dataset and Preprocessing The SemEval-2010 benchmark dataset BIBREF0 is composed of 244 scientific articles collected from the ACM Digital Library (conference and workshop papers).",
      "elf tool. The only preprocessing applied is a systematic dehyphenation at line breaks and removal of author-assigned keyphrases. Scientific articles were selected from four different research areas as defined in the ACM classification, and were equally distributed into training (144 articles) and test (100 articles) sets.",
      "dense parts within scientific articles BIBREF4 , and select only the most content bearing sentences from the remaining contents. To do this, sentences are ordered using TextRank BIBREF14 and the less informative ones, as determined by their TextRank scores normalized by their lengths in words, are filtered out.",
      "as as defined in the ACM classification, and were equally distributed into training (144 articles) and test (100 articles) sets. Gold standard keyphrases are composed of both author-assigned keyphrases collected from the original PDF files and reader-assigned keyphrases provided by student annotators.",
      " surge of interest in automatic keyphrase extraction, thanks to the availability of the SemEval-2010 benchmark dataset BIBREF0 . This dataset is composed of documents (scientific articles) that were automatically converted from PDF format to plain text. As a result, most documents contain irrelevant pieces of text (e.g.",
      "14 and the less informative ones, as determined by their TextRank scores normalized by their lengths in words, are filtered out. Finding the optimal subset of sentences from already shortened documents is however no trivial task as maximum recall linearly decreases with the number of sentences discarded.",
      "abstract\nintact, as they are the two most keyphrase dense parts within scientific articles BIBREF4 , and select only the most content bearing sentences from the remaining contents.",
      "Abstract\nThe SemEval-2010 benchmark dataset has brought renewed attention to the task of automatic keyphrase extraction. This dataset is made up of scientific articles that were automatically converted from PDF format to plain text and thus require careful preprocessing so that irrevelant spans of text do not negatively affect keyphrase extraction performance.",
      "Introduction\nThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/ Recent years have seen a surge of interest in automatic keyphrase extraction, thanks to the availability of the SemEval-2010 benchmark dataset BIBREF0 .",
      "ranking approach, and the last two were among the top performing systems in the SemEval-2010 keyphrase extraction task BIBREF0 . We note that two of the systems are supervised and rely on the training set to build their classification models. Document frequency counts are also computed on the training set. Stemming is applied to allow more robust matching.",
      " author-assigned keyphrases collected from the original PDF files and reader-assigned keyphrases provided by student annotators. Long documents such as those in the SemEval-2010 benchmark dataset are notoriously difficult to handle due to the large number of keyphrase candidates (i.e. phrases that are eligible to be keyphrases) that the systems have to cope with BIBREF6 .",
      "evel 4 preprocessing are shown in Table . We note that two models, namely TopicRank and TF INLINEFORM0 IDF, lose on performance. These two models mainly rely on frequency counts to rank keyphrase candidates, which in turn become less reliable at level 4 because of the very short length of the documents."
    ],
    "answer": [
      "244"
    ]
  },
  {
    "title": "Comparative Studies of Detecting Abusive Language on Twitter",
    "evidence": [
      "Abstract\nThe context-dependent nature of online aggression makes annotating large collections of data extremely difficult. Previously studied datasets in abusive language detection have been insufficient in size to efficiently train deep learning models.",
      "tc.—in effort to censor offensive language, yet it seems nearly impossible to successfully resolve the issue BIBREF7 , BIBREF8 . The major reason of the failure in abusive language detection comes from its subjectivity and context-dependent characteristics BIBREF9 .",
      " reason of the failure in abusive language detection comes from its subjectivity and context-dependent characteristics BIBREF9 . For instance, a message can be regarded as harmless on its own, but when taking previous threads into account it may be seen as abusive, and vice versa.",
      "can be regarded as harmless on its own, but when taking previous threads into account it may be seen as abusive, and vice versa. This aspect makes detecting abusive language extremely laborious even for human annotators; therefore it is difficult to build a large and reliable dataset BIBREF10 .",
      "nguage extremely laborious even for human annotators; therefore it is difficult to build a large and reliable dataset BIBREF10 . Previously, datasets openly available in abusive language detection research on Twitter ranged from 10K to 35K in size BIBREF9 , BIBREF11 . This quantity is not sufficient to train the significant number of parameters in deep learning models.",
      "ing at the tweet one has replied to or has quoted, provides significant contextual information. We call these, “context tweets\". As humans can better understand a tweet with the reference of its context, our assumption is that computers also benefit from taking context tweets into account in detecting abusive language.",
      "ts context, our assumption is that computers also benefit from taking context tweets into account in detecting abusive language. As shown in the examples below, (2) is labeled abusive due to the use of vulgar language. However, the intention of the user can be better understood with its context tweet (1).",
      "greater in size and reliability, has been released. However, this dataset has not been comprehensively studied to its potential. In this paper, we conduct the first comparative study of various learning models on Hate and Abusive Speech on Twitter, and discuss the possibility of using additional features and context data for improvements. Experimental",
      "viously studied datasets in abusive language detection have been insufficient in size to efficiently train deep learning models. Recently, Hate and Abusive Speech on Twitter, a dataset much greater in size and reliability, has been released. However, this dataset has not been comprehensively studied to its potential.",
      "ers have been subjected to harassment, or have witnessed offensive behaviors online BIBREF6 . Major social media companies (i.e. Facebook, Twitter) have utilized multiple resources—artificial intelligence, human reviewers, user reporting processes, etc.—in effort to censor offensive language, yet it seems nearly impossible to successfully resolve the issue BIBREF7 , BIBREF8 .",
      "omebody with a wheelchair get on. INLINEFORM0 (2) I hate it when I'm trying to board a bus and there's already an as**ole on it. Similarly, context tweet (3) is important in understanding the abusive tweet (4), especially in identifying the target of the malice. (3) Survivors of #Syria Gas Attack Recount `a Cruel Scene'. INLINEFORM0 (4) Who the HELL is “LIKE\" ING this post?",
      "results\nin detecting abusive language. Additionally, we present the possibility of using ensemble models of variant models and features for further improvements.",
      "ffect of different features and variants, and describe the possibility for further improvements with the use of ensemble models. Related Work The research community introduced various approaches on abusive language detection. Razavi et al."
    ],
    "answer": [
      "The examples of the difficulties presented by the context-dependent nature of online aggression given by the authors include a message being regarded as harmless on its own but abusive when taking previous threads into account, and a tweet being labeled as abusive due to the use of vulgar language but its intention being better understood with its context tweet."
    ]
  },
  {
    "title": "Prepositional Attachment Disambiguation Using Bilingual Parsing and Alignments",
    "evidence": [
      "em into a dual decomposition based algorithm that enforces agreement between the parse trees from two languages as a constraint. Experiments were performed on the English-Hindi language pair and the performance improved by 10% over the baseline, where the baseline is the attachment predicted by the MSTParser model trained for English.",
      "he agreement between the English and Hindi parse tree when the Hindi parse tree is fixed, and likewise for the Hindi parse tree. The algorithm converges when the trees no longer change, Let us now look at the Project algorithm (Algorithm 2) in detail.",
      "itialsiation step. The DD algorithm then tries to enforce agreement between the two parse trees subject to the given alignments. Let us take a closer look at what we mean by agreement between the two parse trees.",
      "parse trees subject to the given alignments. Let us take a closer look at what we mean by agreement between the two parse trees. Essentially, if we have two words in the English sentence denoted by i and i', aligned to words j and j' in the parallel Hindi sentence respectively, we can expect a dependency edge between i and i' in the English parse tree to correspond to an edge between j and j' in the Hindi parse tree, and vice versa.",
      "ge between i and i' in the English parse tree to correspond to an edge between j and j' in the Hindi parse tree, and vice versa. Also, in order to accommodate structural diversity in languages BIBREF1 , we can expect an edge in the parse tree in one language to correspond to more than one edge, or rather, a path, in the other language parse tree.",
      "al. The idea is to correct the PP-attachments for a sentence with the help of alignments from parallel data in another language. The novelty of our work lies in the formulation of the problem into a dual decomposition based algorithm that enforces agreement between the parse trees from two languages as a constraint.",
      "s in the inferencing step. The input to the algorithm is a parallel English-Hindi sentence pair, with its word alignments given. We first obtain the predicted parse trees for the English and Hindi sentences from the respective trained parser models as an initialsiation step. The DD algorithm then tries to enforce agreement between the two parse trees subject to the given alignments.",
      "e trees. The dual decomposition inference algorithm tries to bring the parse trees in the two languages through its constraints. The problem is formulated as below: In the above formulation, INLINEFORM0 and INLINEFORM1 represent a English and Hindi sentence respectively. INLINEFORM2 and INLINEFORM3 are the corresponding parse trees.",
      "on or projected path of the English edge on the Hindi parse tree, and similarly there are projected paths from Hindi to English. For matters of simplicity, we ignore the direction of the edges in the parse trees. The dual decomposition inference algorithm tries to bring the parse trees in the two languages through its constraints.",
      "comes from NLP applications like Machine Translation, for which, getting the correct attachment of prepositions is very crucial. The idea is to correct the PP-attachments for a sentence with the help of alignments from parallel data in another language.",
      "it often gives rise to incorrect parse trees . Statistical parsers often predict incorrect attachment for prepositional phrases. For applications like Machine Translation, incorrect PP-attachment leads to serious errors in translation. Several approaches have been proposed to solve this problem. We attempt to tackle this problem for English.",
      "S-tagged using the Stanford POS Tagger BIBREF5 for English and using the Hindi POS Tagger BIBREF6 from IIIT-Hyderabad for Hindi. It was then automatically annotated with dependency parse trees by the parsers we had trained before English and Hindi. For testing, we created a corpus of 100 parallel sentences and their word alignments from the Hindi-English Tourism parallel corpus.",
      "ence in the target language, given the parse tree in the source language, and the word alignments between the parallel sentence. Project Algorithm (tree T, sen S) A parse tree T (Hindi) and sentence S (English) Initialize INLINEFORM0 INLINEFORM1 = 0 for INLINEFORM2 to INLINEFORM3 INLINEFORM4 for INLINEFORM5 INLINEFORM6 INLINEFORM0 end for if INLINEFORM1 then return INLINEFORM2 else INLINEFORM3 INLINEFORM4 end for The lagrangian multipliers are initialized to zero."
    ],
    "answer": [
      "The algorithm enforces agreement between parse trees from two languages by formulating the problem into a dual decomposition based algorithm that enforces agreement between the parse trees from two languages as a constraint."
    ]
  },
  {
    "title": "NeuronBlocks: Building Your NLP DNN Models Like Playing Lego",
    "evidence": [
      "it encapsulating a suite of neural network modules as building blocks to construct various DNN models with complex architecture. This toolkit empowers engineers to build, train, and test various NLP models through simple configuration of JSON files. The experiments on several NLP datasets such as GLUE, WikiQA and CoNLL-2003 demonstrate the effectiveness of NeuronBlocks.",
      "y improve the productivity of engineers by saving their learning cost and guiding them to find optimal solutions to their tasks. In this paper, we introduce NeuronBlocks, a toolkit encapsulating a suite of neural network modules as building blocks to construct various DNN models with complex architecture.",
      "provides several pre-built models for NLP tasks, such as semantic role labeling, machine comprehension, textual entailment, etc. Although these toolkits reduce the development cost, they are limited to certain tasks, and thus not flexible enough to support new network architectures or new components. Design The Neuronblocks is built on PyTorch.",
      "Conclusion\nand Future Work In this paper, we introduce NeuronBlocks, a DNN toolkit for NLP tasks built on PyTorch. NeuronBlocks targets three types of engineers, and provides a two-layer solution to satisfy the requirements from all three types of users.",
      "ies a gallery of alternative layers/modules for the networks. Such design achieves a balance between generality and flexibility. Extensive experiments have verified the effectiveness of this approach. NeuronBlocks has been widely used in a product team of a commercial search engine, and significantly improved the productivity for developing NLP DNN approaches.",
      "edding, CNN/RNN, Transformer and so on. Based on the above observations, we developed NeuronBlocks, a DNN toolkit for NLP tasks. The basic idea is to provide two layers of support to the engineers. The upper layer targets common NLP tasks. For each task, the toolkit contains several end-to-end network templates, which can be immediately instantiated with simple configuration.",
      "nd thus not flexible enough to support new network architectures or new components. Design The Neuronblocks is built on PyTorch. The overall framework is illustrated in Figure FIGREF16 . It consists of two layers: the Block Zoo and the Model Zoo. In Block Zoo, the most commonly used components of deep neural networks are categorized into several groups according to their functions.",
      " components with their own modules. The technical contributions of NeuronBlocks are summarized into the following three aspects. Related Work There are several general-purpose deep learning frameworks, such as TensorFlow, PyTorch and Keras, which have gained popularity in NLP community. These frameworks offer huge flexibility in DNN model design and support various NLP tasks.",
      "face guidelines with the existing blocks. These new blocks can be further shared across all users for model architecture design. Moreover, NeuronBlocks has flexible platform support, such as GPU/CPU, GPU management platforms like PAI.",
      "istillation template to improve the inference speed of heavy DNN models like BERT/GPT. Extractive Machine Reading Comprehension. Given a pair of question and passage, predict the start and end positions of the answer spans in the passage. User Interface NeuronBlocks provides convenient user interface for users to build, train, and test DNN models.",
      "l architecture design. Moreover, NeuronBlocks has flexible platform support, such as GPU/CPU, GPU management platforms like PAI. Experiments To verify the performance of NeuronBlocks, we conducted extensive experiments for common NLP tasks on public data sets including CoNLL-2003 BIBREF14 , GLUE benchmark BIBREF13 , and WikiQA corpus BIBREF15 . The experimental",
      "te of reusable and standard components, which can be adopted as building blocks to construct networks with complex architecture. By following the interface guidelines, users can also contribute to this gallery of components with their own modules. The technical contributions of NeuronBlocks are summarized into the following three aspects.",
      "lowing major functional categories of neural network components. Each category covers as many commonly used modules as possible. The Block Zoo is an open framework, and more modules can be added in the future."
    ],
    "answer": [
      "The neural network modules included in NeuronBlocks are categorized into several groups according to their functions in the Block Zoo."
    ]
  },
  {
    "title": "Evaluating Tag Recommendations for E-Book Annotation Using a Semantic Similarity Metric",
    "evidence": [
      "results\nof the four hybrid approaches. By combining the best three popularity-based approaches, we outperform all of the initially evaluated popularity algorithms (i.e., INLINEFORM0 for INLINEFORM1 ).",
      "on approaches, i.e., (i) popularity-based, (ii) similarity-based (i.e., using content information), and (iii) hybrid approaches. Due to the lack of personalized tags (i.e., we do not know which user has assigned a tag), we do not implement other types of algorithms such as collaborative filtering BIBREF8 .",
      " approaches. We use the previously mentioned cross-source algorithm BIBREF13 to construct four hybrid recommendation approaches. In this case, tags are favored that are recommended by more than one algorithm.",
      "arity-based approaches, we outperform all of the initially evaluated popularity algorithms (i.e., INLINEFORM0 for INLINEFORM1 ). On the contrary, the combination of the two best performing similarity-based approaches INLINEFORM2 and INLINEFORM3 does not yield better accuracy.",
      "results\nof the accuracy experiment for the (i) popularity-based, (ii) similarity-based, and (iii) hybrid tag recommendation approaches. Popularity-based approaches.",
      "pproach that combines the three popularity-based methods of INLINEFORM3 and the two similarity-based approaches of INLINEFORM4 . Finally, we define INLINEFORM5 as a hybrid approach that uses the best performing popularity-based and the best performing similarity-based approach (see Figure FIGREF11 in Section SECREF4 for more details about the particular algorithm combinations).",
      " we use Kendall's Tau rank correlation BIBREF19 as suggested by BIBREF20 for automatic evaluation of information-ordering tasks. From that, we rank our recommendation approaches according to both accuracy and semantic similarity and calculate the relation between both rankings. This",
      ", editor tags and Amazon search terms) using a round-robin combination strategy, which ensures an equal weight for both sources. This gives us three additional popularity-based algorithms (= INLINEFORM0 , INLINEFORM1 and INLINEFORM2 ). Similarity-based approaches. We exploit the textual content of e-books (i.e., description or title) to recommend relevant tags BIBREF10 .",
      "ms, mostly content-based algorithms (e.g., BIBREF4 , BIBREF5 ) are used to recommend tags to annotate resources such as e-books. In our work, we incorporate both content features of e-books (i.e., title and description text) as well as Amazon search terms to account for the vocabulary of e-book readers.",
      "not know which user has assigned a tag), we do not implement other types of algorithms such as collaborative filtering BIBREF8 . In total, we evaluate 19 different algorithms to recommend tags for annotating e-books. Popularity-based approaches. We recommend the most frequently used tags in the dataset, which is a common strategy for tag recommendations BIBREF9 .",
      "tions. Approach and findings. We exploit editor tags and user-generated search terms as input for tag recommendation approaches. Our evaluation comprises of a rich set of 19 different algorithms to recommend tags for e-books, which we group into (i) popularity-based, (ii) similarity-based (i.e., using content information), and (iii) hybrid approaches.",
      "similarity-based approach (see Figure FIGREF11 in Section SECREF4 for more details about the particular algorithm combinations). Experimental Setup In this section, we describe our evaluation protocol as well as the measures we use to evaluate and compare our tag recommendation approaches.",
      "hors would like to thank Peter Langs, Jan-Philipp Wolf and Alyona Schraa from HGV GmbH for providing the e-book annotation data. This work was funded by the Know-Center GmbH (FFG COMET Program), the FFG Data Market Austria project and the AI4EU project (EU grant 825619)."
    ],
    "answer": [
      "The algorithms used are: \n1. Popularity-based approaches\n2. Similarity-based approaches\n3. Hybrid approaches\n4. Collaborative filtering\n5. Cross-source algorithm\n6. Kendall's Tau rank correlation\n7. Round-robin combination strategy\n8. Content-based algorithms (e.g., BIBREF4, BIBREF5)"
    ]
  },
  {
    "title": "Harnessing Cognitive Features for Sarcasm Detection",
    "evidence": [
      "results\nreported by the survey from DBLP:journals/corr/JoshiBC16. The following",
      "th 100%, 90%, 80% and 70% of the training data with our whole feature set, and the feature combination from joshi2015harnessing. The goodness of our system is demonstrated by improvements in F-score and Kappa statistics, shown in Figure FIGREF22 .",
      "rnessing. The goodness of our system is demonstrated by improvements in F-score and Kappa statistics, shown in Figure FIGREF22 . We further analyze the importance of features by ranking the features based on (a) Chi squared test, and (b) Information Gain test, using Weka's attribute selection module. Figure FIGREF23 shows the top 20 ranked features produced by both the tests.",
      ", we observe the performance of multiple classification techniques on our dataset through a stratified 10-fold cross validation. We also compare the classification accuracy of our system and the best available systems proposed by riloff2013sarcasm and joshi2015harnessing on our dataset. Using Weka BIBREF18 and LibSVM BIBREF19 APIs, we implement the following classifiers:",
      "results\nconsidering various feature combinations for different classifiers and other systems.",
      "ram features) as well as BIBREF3 , with the MILR classifier getting an F-score improvement of 3.7% and Kappa difference of 0.08. We also achieve an improvement of 2% over the baseline, using SVM classifier, when we employ our feature set.",
      "Results\nTable TABREF17 shows the classification",
      "ver the best available system. Using cognitive features in an NLP Processing system like ours is the first proposal of its kind. Our general approach may be useful in other NLP sub-areas like sentiment and emotion analysis, text summarization and question answering, where considering textual clues alone does not prove to be sufficient.",
      "d from readers' eye-movement data in the form of simple gaze-based features and complex features derived from a graph structure. This extended feature-set improved the success rate of the sarcasm detector by 3.7%, over the best available system. Using cognitive features in an NLP Processing system like ours is the first proposal of its kind.",
      "to detect the underlying incongruity. However, our system predicts the label successfully, possibly helped by the gaze features. Similarly, for sentence 2, the false sense of presence of incongruity (due to phrases like “Helped me” and “Can't stop”) affects the system with only linguistic features. Our system, though, performs well in this case also.",
      "ifference of 0.08. We also achieve an improvement of 2% over the baseline, using SVM classifier, when we employ our feature set. We also observe that the gaze features alone, also capture the differences between sarcasm and non-sarcasm classes with a high-precision but a low recall.",
      "ped me” and “Can't stop”) affects the system with only linguistic features. Our system, though, performs well in this case also. Sentence 3 presents a false-negative case where it was hard for even humans to get the sarcasm. This is why our gaze features (and subsequently the complete set of features) account for erroneous prediction.",
      "results\nconsidering various feature combinations for different classifiers and other systems. These are: Unigram (with principal components of unigram feature vectors), Sarcasm (the feature-set reported by joshi2015harnessing subsuming unigram features and features from other reported systems) Gaze (the simple and complex cognitive features we introduce, along with readability and word count features), and Gaze+Sarcasm (the complete set of features)."
    ],
    "answer": [
      "The best reported system is Gaze+Sarcasm, which achieved an improvement of 3.7% over the best available system and a Kappa difference of 0.08."
    ]
  },
  {
    "title": "Arabic Offensive Language on Twitter: Analysis and Experiments",
    "evidence": [
      "g/predicting bullying to measuring polarization. In this paper, we focus on building effective Arabic offensive tweet detection. We introduce a method for building an offensive dataset that is not biased by topic, dialect, or target. We produce the largest Arabic dataset to date with special tags for vulgarity and hate speech.",
      "le, the particle does not favor any specific offensive expressions and greatly improves our chances of finding offensive tweets. It is clear, the dataset is more biased toward positive class.",
      "aset that is representative of their appearance on Twitter and is hopefully not biased to specific dialects, topics, or targets. One of the main challenges is that offensive tweets constitute a very small portion of overall tweets.",
      "Conclusion\nand Future Work In this paper we presented a systematic method for building an Arabic offensive language tweet dataset that does not favor specific dialects, topics, or genres.",
      "results\non the dataset. For future work, we plan to pursue several directions. First, we want explore target specific offensive language, where attacks against an entity or a group may employ certain expressions that are only offensive within the context of that target and completely innocuous otherwise.",
      "tematic method for building an Arabic offensive language tweet dataset that does not favor specific dialects, topics, or genres. We developed detailed guidelines for tagging the tweets as clean or offensive, including special tags for vulgar tweets and hate speech.",
      "sed by topic, dialect, or target. We produce the largest Arabic dataset to date with special tags for vulgarity and hate speech. Next, we analyze the dataset to determine which topics, dialects, and gender are most associated with offensive tweets and how Arabic speakers use offensive language. Lastly, we conduct a large battery of experiments to produce strong",
      "ts in general and for vulgar and hate speech tweets separately. We believe that this is the first detailed analysis of its kind. Lastly, we conducted a large battery of experiments on the dataset, using cross-validation, to establish a strong system for Arabic offensive language detection. We showed that using static embeddings produced a competitive",
      "ons and greatly improves our chances of finding offensive tweets. It is clear, the dataset is more biased toward positive class. Using the dataset for real-life application may require de-biasing it by boosting negative class or random sampling additional data from Twitter BIBREF22.Using the Twitter API, we collected 660k Arabic tweets having this pattern between April 15, 2019 and May 6, 2019.",
      "d 10,000 tweets, which we plan to release publicly and would constitute the largest available Arabic offensive language dataset. We characterized the offensive tweets in the dataset to determine the topics that illicit such language, the dialects that are most often used, the common modes of offensiveness, and the gender distribution of their authors.",
      "olls and propaganda account detection BIBREF2, detecting the likelihood of hate crimes BIBREF3; and predicting conflict BIBREF4. In this paper, we describe our methodology for building a large dataset of Arabic offensive tweets. Given that roughly 1-2% of all Arabic tweets are offensive BIBREF5, targeted annotation is essential for efficiently building a large dataset.",
      " that span different languages and cultures, some characteristics of Arabic offensive language is language and culture specific. Thus, we conduct a thorough analysis of how Arabic users use offensive language. Next, we use the dataset to train strong Arabic offensive language classifiers using state-of-the-art representations and classification techniques.",
      "eech. To date, this is the largest available dataset, which we plan to make publicly available along with annotation guidelines. We use this dataset to characterize Arabic offensive language to ascertain the topics, dialects, and users' gender that are most associated with the use of offensive language."
    ],
    "answer": [
      "The dataset is not biased by topic, dialect, or target due to the use of a particle that does not favor specific offensive expressions and the inclusion of special tags for vulgarity and hate speech."
    ]
  },
  {
    "title": "A Bayesian Model of Multilingual Unsupervised Semantic Role Induction",
    "evidence": [
      " of individual models for each language plus additional latent variables that capture alignments between roles across languages. Because it is a generative Bayesian model, we can do evaluations in a variety of scenarios just by varying the inference procedure, without changing the model, thereby comparing the scenarios directly.",
      "separate monolingual models for each language and an extra penalty term which tries to maximize INLINEFORM0 and INLINEFORM1 i.e. for all the aligned arguments with role label INLINEFORM2 in language 1, it tries to find a role label INLINEFORM3 in language 2 such that the given proportion is maximized and vice verse.",
      "eters are integrated out). The sample counts and the priors are then used to calculate the MAP estimate of the model parameters. For the monolingual model, the role at a given position is sampled as: DISPLAYFORM0 where the subscript INLINEFORM0 refers to all the variables except at position INLINEFORM1 , INLINEFORM2 refers to the variables in all the training instances except the current one, and INLINEFORM3 refers to all the model parameters.",
      "results\n. The multilingual model obtains small improvements in both languages, which confirms the",
      "efers to the variables in all the training instances except the current one, and INLINEFORM3 refers to all the model parameters. The above integral has a closed form solution due to Dirichlet-multinomial conjugacy.",
      " principled way to incorporate additional sources of information. First, the model scales gracefully to more than two languages. If there are a total of INLINEFORM0 languages, and there is an aligned argument in INLINEFORM1 of them, the multilingual latent variable is connected to only those INLINEFORM2 aligned arguments.",
      "odel. This makes it easier to see how to extend this model in a principled way to incorporate additional sources of information. First, the model scales gracefully to more than two languages.",
      " th cluster. Closest Previous Work This work is closely related to the cross-lingual unsupervised SRL work of titovcrosslingual. Their model has separate monolingual models for each language and an extra penalty term which tries to maximize INLINEFORM0 and INLINEFORM1 i.e.",
      "odel We use the Bayesian model of garg2012unsupervised as our base monolingual model. The semantic roles are predicate-specific. To model the role ordering and repetition preferences, the role inventory for each predicate is divided into Primary and Secondary roles as follows: For example, the complete role sequence in a frame could be: INLINEFORM0 INLINEFORM1 , INLINEFORM2 , INLINEFORM3 , INLINEFORM4 , INLINEFORM5 , INLINEFORM6 , INLINEFORM7 , INLINEFORM8 INLINEFORM9 .",
      "les capture correlations between roles in different languages, and regularize the parameter estimates of the monolingual models. Because this is a joint Bayesian model of multilingual SRI, we can apply the same model to a variety of training scenarios just by changing the inference procedure appropriately.",
      " the model itself, and not an external penalty, which enables us to use the standard Bayesian learning methods such as sampling. The monolingual model we use BIBREF3 also has two main advantages over titovcrosslingual. First, the former incorporates a global role ordering probability that is missing in the latter.",
      "multilingual model uses word alignments between sentences in a parallel corpus to exploit role correspondences across languages. We make copies of the monolingual model for each language and add additional crosslingual latent variables (CLVs) to couple the monolingual models, capturing crosslingual semantic role patterns.",
      "idden variables) given the predicate, its voice, and syntactic features of all the identified arguments (the visible variables). We use a collapsed Gibbs-sampling based approach to generate samples for the hidden variables (model parameters are integrated out). The sample counts and the priors are then used to calculate the MAP estimate of the model parameters."
    ],
    "answer": [
      "The individual model consists of a monolingual model for each language and additional latent variables that capture alignments between roles across languages."
    ]
  },
  {
    "title": "Hierarchical Transformers for Long Document Classification",
    "evidence": [
      "results\nusing fine-tuned BERT predictions instead of the pooled output from final transformer block.",
      "features from the pooled output of final transformer block as these were shown to be working well for most of the tasks BIBREF1. The features extracted from a pre-trained BERT model without any fine-tuning lead to a sub-par performance. However, We also notice that ToBERT model exploited the pre-trained BERT features better than RoBERT. It also converged faster than RoBERT.",
      "ot the case in our tasks. Method ::: BERT Because our work builds heavily upon BERT, we provide a brief summary of its features. BERT is built upon the Transformer architecture BIBREF0, which uses self-attention, feed-forward layers, residual connections and layer normalization as the main building blocks.",
      "in favor of a small Transformer model (2 layers of transformer building block containing self-attention, fully connected, etc.). To investigate if preserving the information about the input sequence order is important, we also build a variant of ToBERT which learns positional embeddings at the segment-level representations (but is limited to sequences of length seen during the training).",
      "tecture. We split the input text sequence into shorter segments in order to obtain a representation for each of them using BERT. Then, we use either a recurrent LSTM BIBREF10 network, or another Transformer, to perform the actual classification. We call these techniques Recurrence over BERT (RoBERT) and Transformer over BERT (ToBERT).",
      "of our knowledge, no attempt has been done before to use the Transformer architecture for classification of such long sequences. Our novel contributions are: Two extensions - RoBERT and ToBERT - to the BERT model, which enable its application in classification of long texts by performing segmentation and using another layer on top of the segment representations. State-of-the-art",
      "results\nusing pre-trained BERT features. We extracted features from the pooled output of final transformer block as these were shown to be working well for most of the tasks BIBREF1.",
      "rns positional embeddings at the segment-level representations (but is limited to sequences of length seen during the training). ToBERT's computational complexity $O(\\frac{n^2}{k^2})$ is asymptotically inferior to RoBERT, as the top-level Transformer model again suffers from quadratic complexity in the number of segments.",
      "ing objective); Next sentence prediction - given two input sequences, decide whether the second one is the next sentence or not. BERT has been shown to beat the state-of-the-art performance on 11 tasks with no modifications to the model architecture, besides adding a task-specific output layer BIBREF1. We follow same procedure suggested in BIBREF1 for our tasks. Fig.",
      " also notice that ToBERT model exploited the pre-trained BERT features better than RoBERT. It also converged faster than RoBERT. Table TABREF26 shows",
      "ically inferior to RoBERT, as the top-level Transformer model again suffers from quadratic complexity in the number of segments. However, in practice this number is much smaller than the input sequence length (${\\frac{n}{k}} << n$), so we haven't observed performance or memory issues with our datasets.",
      "Abstract\nBERT, which stands for Bidirectional Encoder Representations from Transformers, is a recently introduced language representation model based upon the transfer learning paradigm.",
      "Introduction\nBidirectional Encoder Representations from Transformers (BERT) is a novel Transformer BIBREF0 model, which recently achieved state-of-the-art performance in several language understanding tasks, such as question answering, natural language inference, semantic similarity, sentiment analysis, and others BIBREF1."
    ],
    "answer": [
      "The RNN layer does not work better than the transformer layer on top of BERT, as the ToBERT model, which uses a transformer layer, exploited the pre-trained BERT features better and converged faster than RoBERT, which uses an RNN layer."
    ]
  },
  {
    "title": "Impact of Batch Size on Stopping Active Learning for Text Classification",
    "evidence": [
      "Acknowledgment\nThis work was supported in part by The College of New Jersey Support of Scholarly Activities (SOSA) program, by The College of New Jersey Mentored Undergraduate Summer Experience (MUSE) program, and by usage of The College of New Jersey High Performance Computing System.",
      "els. Therefore, BV2009 needs INLINEFORM1 models to be generated before it begins to check if the average is above the threshold. This does not necessarily mean it stops after INLINEFORM2 models have been generated. Rather, it represents the first point in the active learning process at which BV2009 even has a chance to stop.",
      "d only consider words that show up in the dataset more than three times. We use a stop word list to remove common English words. For analyzing the impact of batch size on stopping methods, we use a method that will stop at the first training iteration that is within a specified percentage of the maximum achievable performance.",
      "Results\nWe considered different batch sizes in our experiments, based on percentages of the entire set of training data. The",
      "results\nfrom reduced learning efficiency. We analyze this degradation and find that it can be mitigated by changing the window size parameter of how many past iterations of learning are taken into account when making the stopping decision.",
      "ging the window size parameter of how many past iterations of learning are taken into account when making the stopping decision. We find that when using larger batch sizes, stopping methods are more effective when smaller window sizes are used.",
      "Conclusion\nActive learning has the potential to significantly reduce annotation costs.",
      "rformance of iteratively trained machine learning models by selectively determining which unlabeled samples should be annotated. The number of samples that are selected for annotation at each iteration of active learning is called the batch size. An important aspect of the active learning process is when to stop the active learning process.",
      "Results\nLooking at Table~ SECREF4 , one can see that Oracle-99 needs more annotations with larger batch percents to reach approximately the same F-Measure as with smaller batch percents. These",
      "y perspective. However, in practice due to speed and human annotator considerations, the use of larger batch sizes is necessary. While past work has shown that larger batch sizes decrease learning efficiency from a learning curve perspective, it remains an open question how batch size impacts methods for stopping active learning.",
      "results\nindicate that making the window size smaller helps to mitigate the degradation in stopping method performance that occurs with larger batch sizes.",
      "are generated than when using smaller batch percents. This gives any stopping method less points to test whether or not to stop. We also note that kappa agreement scores are generally low between the first few models trained. This, combined with fewer points to stop at, causes BV2009 to stop somewhat sub-optimally when using very large batch percents.",
      "tch size to use. Previous work has shown that using smaller batch sizes leads to greater learning efficiency BIBREF2 , BIBREF7 . There is a tension between using smaller batch sizes to optimize learning efficiency and using larger batch sizes to optimize development speed and ease of annotation."
    ],
    "answer": [
      "The downstream tasks evaluated are not explicitly stated in the text, but based on the context, it can be inferred that the downstream tasks are likely related to natural language processing (NLP) tasks, such as text classification, sentiment analysis, or named entity recognition, as the text mentions machine learning models, active learning, and F-Measure, which are commonly used in NLP tasks."
    ]
  },
  {
    "title": "Generating Personalized Recipes from Historical User Preferences",
    "evidence": [
      "aining data, the average recipe length is 117 tokens with a maximum of 256. There are 13K unique ingredients across all recipes. Rare words dominate the vocabulary: 95% of words appear $$45 recipes.",
      "Results\nFor training and evaluation, we provide our model with the first 3-5 ingredients listed in each recipe. We decode recipe text via top-$k$ sampling BIBREF7, finding $k=3$ to produce satisfactory",
      "s (2000-2018) from Food.com. Here, we restrict to recipes with at least 3 steps, and at least 4 and no more than 20 ingredients. We discard users with fewer than 4 reviews, giving 180K+ recipes and 700K+ reviews, with splits as in tab:recipeixnstats.",
      " 256. There are 13K unique ingredients across all recipes. Rare words dominate the vocabulary: 95% of words appear $$45 recipes. We order reviews by timestamp, keeping the most recent review for each user as the test set, the second most recent for validation, and the remainder for training (sequential leave-one-out evaluation BIBREF27).",
      "ients. We discard users with fewer than 4 reviews, giving 180K+ recipes and 700K+ reviews, with splits as in tab:recipeixnstats. Our model must learn to generate from a diverse recipe space: in our training data, the average recipe length is 117 tokens with a maximum of 256. There are 13K unique ingredients across all recipes.",
      "abstract\nive summarization BIBREF13, among others. BIBREF14, BIBREF15 model recipes as a structured collection of ingredient entities acted upon by cooking actions.",
      "nd 1.4M user-recipe interactions (reviews) scraped from Food.com, covering a period of 18 years (January 2000 to December 2018). See tab:int-stats for dataset summary statistics, and tab:samplegk for sample information about one user-recipe interaction and the recipe involved.",
      " were given a partial recipe specification (name and 3-5 key ingredients), as well as two generated recipes labeled `A' and `B'. One recipe is generated from our baseline encoder-decoder model and one recipe is generated by one of our three personalized models (Prior Tech, Prior Name, Prior Recipe). The order of recipe presentation (A/B) is randomly selected for each question.",
      "istent long texts BIBREF8, BIBREF9, BIBREF10. Here, we generate procedurally structured recipes instead of free-form narratives. Recipe generation belongs to the field of data-to-text natural language generation BIBREF1, which sees other applications in automated journalism BIBREF11, question-answering BIBREF12, and",
      "ration, where output quality is heavily dependent on the content of the instructions—such as ingredients and cooking techniques. To summarize, our main contributions are as follows: We explore a new task of generating plausible and personalized recipes from incomplete input specifications by leveraging historical user preferences; We release a new dataset of 180K+ recipes and 700K+ user reviews for this task; We introduce new evaluation strategies for generation quality in instructional texts, centering on quantitative measures of coherence.",
      "ould be infeasible in our setting due to the potentially unconstrained number of ingredients (from a space of 10K+) in a recipe. We instead learn historical preferences to guide full recipe generation. A recent line of work has explored user- and item-dependent aspect-aware review generation BIBREF3, BIBREF4.",
      "a personalized generation of plausible, user-specific recipes using user preferences extracted from previously consumed recipes. Our work combines two important tasks from natural language processing and recommender systems: data-to-text generation BIBREF1 and personalized recommendation BIBREF2.",
      "Abstract\nExisting approaches to recipe generation are unable to create recipes for users with culinary preferences but incomplete knowledge of ingredients in specific dishes."
    ],
    "answer": [
      "They get the recipes from Food.com, where they scraped 1.4M user-recipe interactions (reviews) covering a period of 18 years (January 2000 to December 2018)."
    ]
  },
  {
    "title": "Pyramidal Recurrent Unit for Language Modeling",
    "evidence": [
      "Acknowledgment\ns This research was supported by NSF (IIS 1616112, III 1703166), Allen Distinguished Investigator Award, and gifts from Allen Institute for AI, Google, Amazon, and Bloomberg.",
      "ameters, the PRU with standard dropout outperforms most of the state-of-the-art methods by large margin on the PTB dataset (e.g. RAN BIBREF7 by 16 points with 4M less parameters, QRNN BIBREF33 by 16 points with 1M more parameters, and NAS BIBREF31 by 1.58 points with 6M less parameters). With advanced dropouts, the PRU delivers the best performance.",
      "112, III 1703166), Allen Distinguished Investigator Award, and gifts from Allen Institute for AI, Google, Amazon, and Bloomberg. We are grateful to Aaron Jaech, Hannah Rashkin, Mandar Joshi, Aniruddha Kembhavi, and anonymous reviewers for their helpful comments.",
      "ts, reaching perplexities of 56.56 and 64.53 on the Penn Treebank and WikiText2 datasets while learning 15-20% fewer parameters. Replacing an LSTM with a PRU",
      "t 1 point on both the PTB and WT-2 datasets while reducing the number of recurrent unit parameters by about 14% (see R1 and R4). We note that the performance of the PRU drops by up to 1 point when residual connections are not used (R4 and R6).",
      "points on the WT-2 dataset. This further improves with finetuning on the PTB (about 2 points) and WT-2 (about 1 point) datasets. For similar number of parameters, the PRU with standard dropout outperforms most of the state-of-the-art methods by large margin on the PTB dataset (e.g.",
      "outheast (sample 1), cost (sample 2), face (sample 4), and introduced (sample 5), which help in making more confident decisions. Furthermore, when gradients during back-propagation are visualized BIBREF37 (Table TABREF34 ), we find that PRUs have better gradient coverage than LSTMs, suggesting PRUs use more features than LSTMs that contributes to the decision.",
      "e R1 and R4). We note that the performance of the PRU drops by up to 1 point when residual connections are not used (R4 and R6). (2) Using the grouped linear transformation for context vectors reduces the total number of recurrent unit parameters by about 75% while the performance drops by about 11% (see R3 and R4).",
      "pooling for sub-sampling. These values are selected based on our ablation experiments on the validation set (Section SECREF39 ). We measure the performance of our models in terms of word-level perplexity. We follow the same training strategy as in BIBREF0 .",
      "meters, and NAS BIBREF31 by 1.58 points with 6M less parameters). With advanced dropouts, the PRU delivers the best performance. On both datasets, the PRU improves the perplexity by about 1 point while learning 15-20% fewer parameters.",
      "-of-the-art methods. Additionally, we provide a detailed examination of the PRU and its behavior on the language modeling tasks. Set-up Following recent works, we compare on two widely used datasets, the Penn Treebank (PTB) BIBREF28 as prepared by BIBREF29 and WikiText2 (WT-2) BIBREF20 . For both datasets, we follow the same training, validation, and test splits as in BIBREF0 .",
      "at PRUs have better gradient coverage than LSTMs, suggesting PRUs use more features than LSTMs that contributes to the decision. This also suggests that PRUs update more parameters at each iteration which",
      "squared norm of the gradients obtained through back-propagation. Table TABREF34 visualizes the heatmaps for different sequences. PRUs, in general, give more relevance to contextual words than LSTMs, such as southeast (sample 1), cost (sample 2), face (sample 4), and introduced (sample 5), which help in making more confident decisions."
    ],
    "answer": [
      "{\n\"NSF (IIS 1616112, III 1703166), Allen Distinguished Investigator Award, and gifts from Allen Institute for AI, Google, Amazon, and Bloomberg.\""
    ]
  },
  {
    "title": "Emotion Detection in Text: Focusing on Latent Representation",
    "evidence": [
      "e context. These methods, therefore, are not sufficient to create an applicable and generalizable emotion detection methodology. Understanding these limitations, we present a new network based on a bidirectional GRU model to show that capturing more meaningful information from text can significantly improve the performance of these models. The",
      "ecurrent Neural Networks (RNNs) have been shown to perform well for the verity of tasks in NLP, especially classification tasks. And as our goal was to capture more information about the context and sequential nature of the text, we decided to use a model based on bidirectional RNN, specifically a bidirectional GRU network to analyze the tweets.",
      "e GRU output and an average-pooling layer was used to considers all features to create a representation for the text as a whole. These partial representations are then were concatenated to create out final hidden representation.",
      "he data. For the next step, we use a concatenation of global max-pooling and average-pooling layers (with a window size of two). Then a max-pooling was used to extract the most important features form the GRU output and an average-pooling layer was used to considers all features to create a representation for the text as a whole.",
      " like is normally used in conventional machine learning approaches are unable to capture the intricacy of emotional expressions. Our architecture was designed to show that using a model that captures better information about the context and sequential nature of the text can outperform lexicon-based methods commonly used in the literature. As mentioned in the",
      " the text, we decided to use a model based on bidirectional RNN, specifically a bidirectional GRU network to analyze the tweets. For building the emotion classifier, we have decided to use 7 binary classifiers-one for each emotion- each of which uses the same architecture for detecting a specific emotion. You can see the plot diagram of the model in Figure FIGREF6 .",
      " As none of our tweets had more than 35 terms, we set the size of the embedding layer to 35 and added padding to shorter tweets. The output of this layer goes to a bidirectional GRU layer selected to capture the entirety of each tweet before passing its output forward. The goal is to create an intermediate representation for the tweets that capture the sequential nature of the data.",
      "ods, some of the most important information in the text such as the sequential nature of the data, and the context will be lost. Considering the complexity of the task, and the fact that these models lose a lot of information by using simpler models such as the bag of words model (BOW) or lexicon features, these attempts lead to methods which are not reusable and generalizable.",
      "iguity of human emotions and emotional expressions and the huge impact of context on the understanding of the expressed emotion. These complexities are what led us to believe lexicon-based features like is normally used in conventional machine learning approaches are unable to capture the intricacy of emotional expressions.",
      "learning models cannot grasp the intricacy of emotional language by ignoring the sequential nature of the text, and the context. These methods, therefore, are not sufficient to create an applicable and generalizable emotion detection methodology.",
      "beneficial when working with textual data especially faced with the hard task of detecting more complex phenomena like emotions. We accomplished that by using a recurrent network in the process of generating our hidden representation.",
      "he text as a whole and by learning the important features without any additional (and often incomplete) human-designed features. In this work, we argue that creating a model that can better capture the context and sequential nature of text , can significantly improve the performance in the hard task of emotion detection.",
      "enomena like emotions. We accomplished that by using a recurrent network in the process of generating our hidden representation. We have also used a max-pooling layer to capture the most relevant features and an average pooling layer to capture the text as a whole proving that we can achieve better performance by focusing on creating a more informative hidden representation."
    ],
    "answer": [
      "The GRU model captures the sequential nature of the text and the context, which traditional ML models do not."
    ]
  },
  {
    "title": "ReviewQA: a relational aspect-based opinion reading dataset",
    "evidence": [
      "al language questions. The reviews we used have been extracted from TripAdvisor and originally proposed in BIBREF10 , BIBREF11 . In the original data, each review comes with a set of rated aspects among the seventh available: Business service, Check in / Front Desk, Cleanliness, Location, Room, Sleep Quality, Value and for all the reviews an Overall rating.",
      "REF10 and BIBREF11 . This corpus is available at http://www.cs.virginia.edu/~hw5x/Data/LARA/TripAdvisor/TripAdvisorJson.tar.bz2. Each review comes with the name of the associated hotel, a title, an overall rating, a comment and a list of rated aspects.",
      "challenge, we propose to leverage on a hotel reviews corpus that requires reasoning skills to answer natural language questions. The reviews we used have been extracted from TripAdvisor and originally proposed in BIBREF10 , BIBREF11 .",
      "bz2. Each review comes with the name of the associated hotel, a title, an overall rating, a comment and a list of rated aspects. From 0 to 7 aspects, among value, room, location, cleanliness, check-in/front desk, service, business service, can possibly be rated in a review. Figure FIGREF8 displays a review extracted from this dataset.",
      "tasks evaluated in this dataset. Our corpus contains more than 500.000 questions in natural language over 100.000 hotel reviews. Our setup is projective, the answer of a question does not need to be extracted from a document, like in most of the recent datasets, but selected among a set of candidates that contains all the possible answers to the questions of the dataset.",
      " to get an overview of what are the strengths and the weaknesses of a given model on the set of tasks evaluated in this dataset. Our corpus contains more than 500.000 questions in natural language over 100.000 hotel reviews.",
      "s types over the relationships between aspects. We propose ReviewQA, a dataset of natural language questions over hotel reviews. These questions are divided into 8 groups, regarding the competency required to be answered. In this section, we describe each task and the process followed to generate this dataset.",
      "ess service, Check in / Front Desk, Cleanliness, Location, Room, Sleep Quality, Value and for all the reviews an Overall rating. In this articles we propose to exploit these data to create a dataset of question-answering that will challenge 8 competencies of the reader.",
      "uestions about the different aspects of the targeted venues are typical kind of questions we want to be able to ask to a system. In this context, we introduce a set of reasoning questions types over the relationships between aspects. We propose ReviewQA, a dataset of natural language questions over hotel reviews.",
      "tantly, it should be able to reason over the important pieces of the document in order to produce an answer when it is required. To motivate this purpose, we present ReviewQA, a question-answering dataset based on hotel reviews. The questions of this dataset are linked to a set of relational understanding competencies that we expect a model to master.",
      "he competency required to be answered. In this section, we describe each task and the process followed to generate this dataset. Original data We used a set of reviews extracted from the TripAdvisor website and originally proposed in BIBREF10 and BIBREF11 . This corpus is available at http://www.cs.virginia.edu/~hw5x/Data/LARA/TripAdvisor/TripAdvisorJson.tar.bz2.",
      "ited to the detection of relevant passages in a document, is only the first step in building systems that truly understand text. The second step is the ability of reasoning with the relevant information extracted from a document. To set up this challenge, we propose to leverage on a hotel reviews corpus that requires reasoning skills to answer natural language questions.",
      "average values of these ratings tend to be quite high. It could have introduced bias if it was not the case for all the aspects. For example, we do not want that the model learns that in general, the service is rated better than the location and them answer without looking at the document."
    ],
    "answer": [
      "The hotel reviews are from TripAdvisor."
    ]
  },
  {
    "title": "Fine-Grained Named Entity Recognition using ELMo and Wikidata",
    "evidence": [
      "ss these issues, in part, by combining state-of-the-art deep learning models (ELMo) with an expansive knowledge base (Wikidata). Using our framework, we cross-validate our model on the 112 fine-grained entity types based on the hierarchy given from the Wiki(gold) dataset.",
      "results\non Wiki(gold) so a direct comparison is difficult. While these knowledge bases provide semantically rich and fine-granular classes and relationship types, the task of entity classification often requires associating coarse-grained classes with discovered surface forms of entities.",
      "rms of entities. Most existing studies consider NER and entity linking as two separate tasks, whereas we try to combine the two. It has been shown that one can significantly increase the semantic information carried by a NER system when we successfully linking entities from a deep learning method to the related entities from a knowledge base BIBREF26 , BIBREF27 .",
      "Conclusion\nand Future Work In this paper, we present a deep neural network model for the task of fine-grained named entity classification using ELMo embeddings and Wikidata.",
      "of entity types. Furthermore, many named entity systems suffer when considering the categorization of fine grained entity types. Our work attempts to address these issues, in part, by combining state-of-the-art deep learning models (ELMo) with an expansive knowledge base (Wikidata).",
      "present a deep neural network model for the task of fine-grained named entity classification using ELMo embeddings and Wikidata. The proposed model learns representations for entity mentions based on its context and incorporates the rich structure of Wikidata to augment these labels into finer-grained subtypes.",
      "es and labels were embedded into a low dimensional feature space to facilitate information sharing among labels. Shimaoka et al. shimaoka2016attentive proposed an attentive neural network model which used long short-term memory (LSTMs) to encode the context of the entity, then used an attention mechanism to allow the model to focus on relevant expressions in the entity mention's context.",
      "s to all mentions of a particular entity in the corpus. For example, “Barack Obama” is a person, politician, lawyer, and author. If a knowledge base has these four matching labels, the distant supervision technique will assign all of them to every mention of “Barack Obama”.",
      "n producing adequate recognition accuracy, they often require significant human effort in carefully designing rules or features. In recent years, deep learning methods been employed in NER systems, yielding state-of-the-art performance. However, the number of types detected are still not sufficient for certain domain-specific applications.",
      "for entity classification of this type. For this study, we use Wikidata, which can be seen diagrammatically in Figure FIGREF12 . Systems such as DeepType BIBREF25 integrate symbolic information into the reasoning process of a neural network with a type system and show state-of-the-art performances for EL. They do not, however, quote",
      ", the task of entity classification often requires associating coarse-grained classes with discovered surface forms of entities. Most existing studies consider NER and entity linking as two separate tasks, whereas we try to combine the two.",
      "we successfully linking entities from a deep learning method to the related entities from a knowledge base BIBREF26 , BIBREF27 . Redirection: For the Wikidata linking element, we recognize that the lookup will be constrained by the most common lookup name for each entity.",
      "ntions based on its context and incorporates the rich structure of Wikidata to augment these labels into finer-grained subtypes. We can see comparisons of our model made on Wiki(gold) in Table TABREF20 . We note that the model performs similarly to existing systems without being trained or tuned on that particular dataset."
    ],
    "answer": [
      "The proposed model combines a deep learning model with a knowledge base by using ELMo embeddings and Wikidata to learn representations for entity mentions based on their context and incorporate the rich structure of Wikidata to augment these labels into finer-grained subtypes."
    ]
  },
  {
    "title": "Bringing Structure into Summaries: Crowdsourcing a Benchmark Corpus of Concept Maps",
    "evidence": [
      "d documents, create a concept map that represents its most important content, satisfies a specified size limit and is connected. We define a concept map as a labeled graph showing concepts as nodes and relationships between them as edges. Labels are arbitrary sequences of tokens taken from the documents, making the summarization task extractive. A concept can be an entity,",
      "Abstract\nConcept maps can be used to concisely represent important information and bring structure into large document collections. Therefore, we study a variant of multi-document summarization that produces summaries in the form of concept maps.",
      "representation that also model concepts and their relationships are knowledge bases, such as Freebase BIBREF29 , and ontologies. However, they both differ in important aspects: Whereas concept maps follow an open label paradigm and are meant to be interpretable by humans, knowledge bases and ontologies are usually more strictly typed and made to be machine-readable.",
      ", BIBREF11 . For summarization, concept maps make it possible to represent a summary concisely and clearly reveal relationships. Moreover, we see a second interesting use case that goes beyond the capabilities of textual summaries: When concepts and relations are linked to corresponding locations in the documents they have been extracted from, the graph can be used to navigate in a document collection, similar to a table of contents.",
      "ions step by step. They were instructed to reach a size of 25 concepts, the recommended maximum size for a concept map BIBREF6 . Further, they should prefer more important propositions and ensure connectedness. When connecting two propositions, they were asked to keep the concept label that was appropriate for both propositions.",
      "e if it (1) is a correct extraction, (2) is meaningful without any context and (3) has arguments that represent proper concepts. We created a guideline explaining when to label a tuple as suitable for a concept map and performed a small annotation study. Three annotators independently labeled 500 randomly sampled tuples. The agreement was 82% ( INLINEFORM0 ).",
      "s rather than crowdsource the subtasks. Annotators were given the topic description and the most important, ranked propositions. Using a simple annotation tool providing a visualization of the graph, they could connect the propositions step by step. They were instructed to reach a size of 25 concepts, the recommended maximum size for a concept map BIBREF6 .",
      "provide an evaluation protocol and baseline (§ SECREF7 ). We make these resources publicly available under a permissive license. Task Concept-map-based MDS is defined as follows: Given a set of related documents, create a concept map that represents its most important content, satisfies a specified size limit and is connected.",
      "s in education BIBREF7 , BIBREF8 , for writing assistance BIBREF9 or to structure information repositories BIBREF10 , BIBREF11 . For summarization, concept maps make it possible to represent a summary concisely and clearly reveal relationships.",
      "rresponding task that we propose is concept-map-based MDS, the summarization of a document cluster in the form of a concept map. In order to develop and evaluate methods for the task, gold-standard corpora are necessary, but no suitable corpus is available. The manual creation of such a dataset is very time-consuming, as the annotation includes many subtasks.",
      "ments they have been extracted from, the graph can be used to navigate in a document collection, similar to a table of contents. An implementation of this idea has been recently described by BIBREF12 . The corresponding task that we propose is concept-map-based MDS, the summarization of a document cluster in the form of a concept map.",
      "t concepts and relations for the summary and finally organize them in a graph satisfying the connectedness and size constraints. Related Work Some attempts have been made to automatically construct concept maps from text, working with either single documents BIBREF14 , BIBREF9 , BIBREF15 , BIBREF16 or document clusters BIBREF17 , BIBREF18 , BIBREF19 .",
      "tic structures and connect them to build a concept map. However, common task definitions and comparable evaluations are missing. In addition, only a few of them, namely Villalon.2012 and Valerio.2006, define summarization as their goal and try to compress the input to a substantially smaller size."
    ],
    "answer": [
      "A concept map is defined as a labeled graph showing concepts as nodes and relationships between them as edges, with labels being arbitrary sequences of tokens taken from the documents."
    ]
  },
  {
    "title": "CoVoST: A Diverse Multilingual Speech-To-Text Translation Corpus",
    "evidence": [
      "results\nare consistent with what we see from ASR models. For example thanks to abundant training data, French has a decent BLEU score of 29.8/25.4. German doesn't perform well, because of less richness of content (transcripts).",
      "supervised data. We encourage the community to improve upon those baselines, for example by leveraging semi-supervised training. Baseline",
      "measured the perplexity of the translations using a language model trained on a large amount of clean monolingual data BIBREF14. We manually inspected examples where the translation had a high perplexity and sent them back to translators accordingly. 4) We computed the ratio of English characters in the translations.",
      "uce the portion of short sentences. This makes the resulting evaluation set closer to real-world scenarios and more challenging. We run the same quality checks for TT as for CoVoST but we do not find poor quality translations according to our criteria. Finally, we report the overlap between CoVo transcripts and TT sentences in Table TABREF5.",
      "onding systems. Translations with a score that was too low were manually inspected and sent back to the translators when needed. 2) We manually inspected examples where the source transcript was identical to the translation. 3) We measured the perplexity of the translations using a language model trained on a large amount of clean monolingual data BIBREF14.",
      "gh perplexity and sent them back to translators accordingly. 4) We computed the ratio of English characters in the translations. We manually inspected examples with a low ratio and sent them back to translators accordingly.",
      "lingual sentence embeddings BIBREF17. Samples with low scores were manually inspected and sent back for translation when needed. We also sanity check the overlaps of train, development and test sets in terms of transcripts and voice clips (via MD5 file hashing), and confirm they are totally disjoint.",
      "ta, French has a decent BLEU score of 29.8/25.4. German doesn't perform well, because of less richness of content (transcripts). The other languages are low resource in CoVoST and it is difficult to train decent models without additional data or pre-training techniques. Baseline",
      "arity, thanks to the development of end-to-end models and the creation of new corpora, such as Augmented LibriSpeech and MuST-C. Existing datasets involve language pairs with English as a source language, involve very specific domains or are low resource.",
      "ed by a state-of-the-art system BIBREF14 (the French-English system was a Transformer big BIBREF15 separately trained on WMT14). We applied this method to these three language pairs only as we are confident about the quality of the corresponding systems. Translations with a score that was too low were manually inspected and sent back to the translators when needed.",
      "ons. We extract 80-channel log-mel filterbank features, computed with a 25ms window size and 10ms window shift using torchaudio. The features are normalized to 0 mean and 1.0 standard deviation. We remove samples having more than 3,000 frames or more than 256 characters for GPU memory efficiency (less than 25 samples are removed for all languages). Baseline",
      "CoefVar}_{MS}$ because of the lack of training data. Multilingual models are consistantly more stable on low-resource languages. Ru+Fr, Tr+Fr, Fa+Fr and Zh+Fr even have better $\\textrm {CoefVar}_{MS}$ than all individual languages.",
      " other users. Most of the sentences are covered by multiple speakers, with potentially different genders, age groups or accents. Raw CoVo data contains samples that passed validation as well as those that did not. To build CoVoST, we only use the former one and reuse the official train-development-test partition of the validated data."
    ],
    "answer": [
      "The quality of the data is empirically evaluated through various methods, including manual inspection, perplexity measurement using a language model, and computation of the ratio of English characters in the translations."
    ]
  },
  {
    "title": "Improving the Performance of Neural Machine Translation Involving Morphologically Rich Languages",
    "evidence": [
      "combination of different corpora from various domains. The major part of the corpus was made up by the EnTam v2 corpus BIBREF2 . This corpus contained sentences taken from parallel news articles, English and Tamil bible corpus and movie subtitles.",
      "results\nfor comparison. This set included a randomized selection of the translation",
      "d human evaluation metrics of adequacy, fluency and relative ranking values were used to evaluate the performance of the models. BLEU Evaluation The BLEU scores obtained using the various models used in the experiment are tabulated in Table TABREF25 .",
      "results\nto ensure the objectivity of evaluation. Fluency and adequacy",
      "hance the richness of context of the word vectors. It was also used to create the language model for the phrase-based SMT model. This corpus contained 567,772 sentences and was self-collected by combining hundreds of ancient Tamil scriptures, novels and poems by accessing the websites of popular online ebook libraries in Python using the urllib package.",
      " target language sentences used for training were taken and divided into bucketed pairs of sentences of a fixed number of sizes. This relationship was determined by examining the distribution of words in the corpus primarily to minimize the number of PAD tokens in the sentence.",
      "formances of the translations were also compared by means of human evaluation metrics of adequacy, fluency and relative ranking. Further, the use of morphological segmentation also improved the efficacy of the attention mechanism.",
      "eated from Tamil novels and short stories from AU-KBC, Anna university. The complete corpus consisted of 197,792 sentences. Fig. FIGREF20 shows the skinny shift and heatmap representations of the relativity between the sentences in terms of their sentence lengths.",
      "Results\nand",
      "results\nare tabulated. Adequacy rating was calculated on a 5-point scale of how much of the meaning is conveyed by the translation (All, Most, Much, Little, None).",
      "ulties, a trade-off between domain specificity and size of the corpus is integral in building an English–Tamil neural MT system. Corpus The corpus selected for this experiment was a combination of different corpora from various domains. The major part of the corpus was made up by the EnTam v2 corpus BIBREF2 .",
      "BIBREF2 . This corpus contained sentences taken from parallel news articles, English and Tamil bible corpus and movie subtitles. It also comprised of a tourism corpus that was obtained from TDIL (Technology Development for Indian Languages) and a corpus created from Tamil novels and short stories from AU-KBC, Anna university. The complete corpus consisted of 197,792 sentences. Fig.",
      "Discussion\nThe BLEU metric parameters (modified 1-gram, 2-gram, 3-gram and 4-gram precision values) and human evaluation metrics of adequacy, fluency and relative ranking values were used to evaluate the performance of the models."
    ],
    "answer": [
      "The human judgements were assembled using a combination of different corpora from various domains, including the EnTam v2 corpus, which contained sentences from parallel news articles, English and Tamil bible corpus, and movie subtitles, as well as a tourism corpus from TDIL and a corpus created from Tamil novels and short stories from AU-KBC, Anna university."
    ]
  },
  {
    "title": "Multi-task Pairwise Neural Ranking for Hashtag Segmentation",
    "evidence": [
      "t curated dataset, which includes all 12,594 unique English hashtags and their associated tweets from the same Stanford dataset. Experiments In this section, we present experimental",
      "-hoc conventions which frequently include multiple words joined together and can include abbreviations and unorthodox spellings. We build a dataset of 12,594 hashtags split into individual segments and propose a set of approaches for hashtag segmentation by framing it as a pairwise ranking problem between candidate segmentations.",
      " SRILM BIBREF33 to give a better performance in segmenting hashtags (§ SECREF46 ). For all our experiments, we set INLINEFORM1 . Hashtag Segmentation Data We use two datasets for experiments (Table TABREF29 ): (a) STAN INLINEFORM0 , created by BIBREF10 BansalBV15, which consists of 1,108 unique English hashtags from 1,268 randomly selected tweets in the Stanford Sentiment Analysis Dataset BIBREF36 along with their crowdsourced segmentations and our additional corrections; and (b) STAN INLINEFORM1 , our new expert curated dataset, which includes all 12,594 unique English hashtags and their associated tweets from the same Stanford dataset.",
      "results\non the STAN INLINEFORM0 and STAN INLINEFORM1 datasets, respectively. All of our pairwise neural rankers are trained on the 2,518 manually segmented hashtags in the training set of STAN INLINEFORM2 and perform favorably against other state-of-the-art approaches.",
      "ion datasets. The next section will show experiments of applying hashtag segmentation to the popular task of sentiment analysis. Existing Methods We compare our pairwise neural ranker with the following baseline and state-of-the-art approaches: The original hashtag as a single token; A rule-based segmenter, which employs a set of word-shape rules with an English dictionary BIBREF13 ; A Viterbi model which uses word frequencies from a book corpus BIBREF0 ; The specially developed GATE Hashtag Tokenizer from the open source toolkit, which combines dictionaries and gazetteers in a Viterbi-like algorithm BIBREF11 ; A maximum entropy classifier (MaxEnt) trained on the STAN INLINEFORM0 training dataset.",
      "h to segmenting hashtags by using a language model specifically trained on Twitter data (implementation details in § SECREF26 ). The performance of this method itself is competitive with state-of-the-art methods (evaluation",
      "Discussion\nTo empirically illustrate the effectiveness of different features on different types of hashtags, we show the",
      " pairwise neural ranking model for hashtag segmention and showed significant performance improvements over the state-of-the-art. We also constructed a larger and more curated dataset for analyzing and benchmarking hashtag segmentation methods. We demonstrated that hashtag segmentation helps with downstream tasks such as sentiment analysis.",
      "ir associated tweets annotated in a multi-step process for higher quality than the previous dataset of 1,108 hashtags BIBREF10 . We frame the segmentation task as a pairwise ranking problem, given a set of candidate segmentations. We build several neural architectures using this problem formulation which use corpus-based, linguistic and thesaurus based features.",
      "d metadata to a textual utterance with the goal of increasing discoverability, aiding search, or providing additional semantics. However, the semantic content of hashtags is not straightforward to infer as these represent ad-hoc conventions which frequently include multiple words joined together and can include abbreviations and unorthodox spellings.",
      "Introduction\nA hashtag is a keyphrase represented as a sequence of alphanumeric characters plus underscore, preceded by the # symbol.",
      "ding. The goal of our study is to build efficient methods for automatically splitting a hashtag into a meaningful word sequence. Our contributions are: Our new dataset includes segmentation for 12,594 unique hashtags and their associated tweets annotated in a multi-step process for higher quality than the previous dataset of 1,108 hashtags BIBREF10 .",
      "entation INLINEFORM2 and its corresponding hashtag INLINEFORM3 satisfy certain word-shapes (more details in appendix SECREF61 ). Similarly, for hashtag INLINEFORM0 , we extract the feature vector INLINEFORM1 consisting of hashtag length, ngram count of the hashtag in Google 1TB corpus BIBREF31 , and boolean features indicating if the hashtag is in an English dictionary or Urban Dictionary, is a named-entity, is in camel case, ends with a number, and has all the letters as consonants."
    ],
    "answer": [
      "The dataset of hashtags is sourced from the Stanford dataset, which includes all 12,594 unique English hashtags and their associated tweets."
    ]
  },
  {
    "title": "Extractive Summarization of EHR Discharge Notes",
    "evidence": [
      "abstract\nive EHR summarization using neural networks.",
      "R notes using Latent Dirichlet allocation (LDA) or bayesian networks BIBREF15 , and knowledge based heuristic systems BIBREF17 . To our knowledge, there is no literature to date on extractive or",
      "he 515 annotated history of present illness notes, which were split in a 70% train set, 15% development set, and a 15% test set. The model is trained using the Adam algorithm for gradient-based optimization BIBREF25 with an initial learning rate = 0.001 and decay = 0.9. A dropout rate of 0.5 was applied for regularization, and each batch size = 20.",
      "rization, and each batch size = 20. The model ran for 20 epochs and was halted early if there was no improvement after 3 epochs. We evaluated the impact of character embeddings, the choice of pretrained w2v embeddings, and the addition of learned word embeddings on model performance on the dev set. We report performance of the best performing model on the test set.",
      "ould like to thank our annotators, Andrew Goldberg, Laurie Alsentzer, Elaine Goldberg, Andy Alsentzer, Grace Lo, and Josh Donis. We would also like to acknowledge Pete Szolovits for his guidance and for providing the pretrained word embeddings and Tristan Naumann for providing the MIMIC CUIs.",
      "ere extracted using Apache cTAKES BIBREF19 and filtered by removing the CUIs that are already subsumed by a longer spanning CUI. For example, CUIs for \"head\" and \"ache\" were removed if a CUI existed for \"head ache\" in order to extract the most clinically relevant CUIs.",
      "learned word embeddings on model performance on the dev set. We report performance of the best performing model on the test set. Table TABREF16 compares dev set performance of the model using various pretrained word embeddings, with and without character embeddings, and with pretrained versus learned word embeddings.",
      "n initial learning rate = 0.001 and decay = 0.9. A dropout rate of 0.5 was applied for regularization, and each batch size = 20. The model ran for 20 epochs and was halted early if there was no improvement after 3 epochs.",
      "s. We demonstrate that our model can achieve excellent performance on a small dataset with known heterogeneity among annotators. This model can be applied to the 55,000 discharge summaries in MIMIC to create a dataset for evaluation of extractive summarization methods.",
      "otes was manually annotated by one of eight annotators using the software Multi-document Annotation Environment (MAE) BIBREF20 . MAE provides an interactive GUI for annotators and exports the",
      "e italics is used to denote scalars, lowercase bold is used to denote vectors, and uppercase italics is used to denote matrices. Token Embedding Layer: In the token embedding layer, pretrained word embeddings are combined with learned character embeddings to create a hybrid token embedding for each word in the HPI note.",
      "nitial methods did not consider word meaning or syntax at either the sentence or paragraph level, which made them crude at best. More advanced extractive heuristics like topic modeling BIBREF7 , cue word dictionary approaches BIBREF8 , and title methods BIBREF9 for scoring content in a sentence followed soon after.",
      "te, and the patient movement section uses a limited vocab (transferred, admitted, etc.), which may explain their high F1 scores. On the other hand, the vitals/labs and medication history sections had the lowest support, which may explain why they were more challenging to label. Words that belonged to the diagnosis history, patient movement, and procedure/"
    ],
    "answer": [
      "* MIMIC dataset\n* annotated history of present illness notes\n* Apache cTAKES\n* MIMIC CUIs\n* 55,000 discharge summaries in MIMIC\n* Multi-document Annotation Environment (MAE)"
    ]
  },
  {
    "title": "ZuCo 2.0: A Dataset of Physiological Recordings During Natural Reading and Annotation",
    "evidence": [
      "llowed by 50 sentences of task-specific reading. The order of blocks and sentences within blocks was identical for all subjects. Each sentence block was preceded by a practice round of three sentences.",
      "e same length range as ZuCo 1.0, and with similar Flesch reading ease scores. The dataset statistics are shown in Table TABREF2. Of the 739 sentences, the participants read 349 sentences in a normal reading paradigm, and 390 sentences in a task-specific reading paradigm, in which they had to determine whether a certain relation type occurred in the sentence or not.",
      "he first task, participants were instructed to read the sentences naturally, without any specific task other than comprehension. Participants were told to read the sentences normally without any special instructions. Figure FIGREF8 (left) shows an example sentence as it was depicted on the screen during recording.",
      "sks to avoid motor EEG artifacts. Participants were also offered snacks and water during the breaks and were encouraged to rest. All sentences were presented at the same position on the screen and could span multiple lines. The sentences were presented in black on a light grey background with font size 20-point Arial, resulting in a letter height of 0.8 mm.",
      "ng session, the participants read 739 sentences that were selected from the Wikipedia corpus provided by culotta2006integrating. This corpus was chosen because it provides annotations of semantic relations. We included seven of the originally defined relation types: political_affiliation, education, founder, wife/husband, job_title, nationality, and employer.",
      "quired to set up and calibrate the devices, and the personal reading speed of the participants. We recorded 14 blocks of approx. 50 sentences, alternating between tasks: 50 sentences of normal reading, followed by 50 sentences of task-specific reading. The order of blocks and sentences within blocks was identical for all subjects.",
      " of the data prior to the start of the experiments. The study was approved by the Ethics Commission of the University of Zurich. Corpus Construction ::: Reading materials During the recording session, the participants read 739 sentences that were selected from the Wikipedia corpus provided by culotta2006integrating.",
      "ive reading was recorded, while in the second task (TSR) the subjects had to annotate a specific relation type in each sentence. Thus, the task-specific annotation reading lead to shorter passes because the goal was merely to recognize a relation in the text, but not necessarily to process the every word in each sentence.",
      "sentences within blocks was identical for all subjects. Each sentence block was preceded by a practice round of three sentences. Corpus Construction ::: Experimental design ::: Normal reading (NR) In the first task, participants were instructed to read the sentences naturally, without any specific task other than comprehension.",
      "t include the relation type and were used as control conditions. All sentences within one block involved the same relation type. The blocks started with a practice round, which described the relation and was followed by three sample sentences, so that the participants would be familiar with the respective relation type.",
      "because the goal was merely to recognize a relation in the text, but not necessarily to process the every word in each sentence. This distinct reading behavior is shown in Figure FIGREF15, where fixations occur until the end of the sentence during normal reading, while during task-specific reading the fixations stop after the decisive words to detect a given relation type.",
      "tion of these duplicate sentences is to provide a set of sentences read twice by all participants with a different task in mind. Hence, this enables the comparison of eye-tracking and brain activity data when reading normally and when annotating specific relations (see examples in Section SECREF4). Furthermore, there is also an overlap in the sentences between ZuCo 1.0 and ZuCo 2.0.",
      " spends per sentence before switching to the next one. All 739 sentences were recorded in a single session for each participant. The duration of the recording sessions was between 100 and 180 minutes, depending on the time required to set up and calibrate the devices, and the personal reading speed of the participants. We recorded 14 blocks of approx."
    ],
    "answer": [
      "The sentences that were read were selected from the Wikipedia corpus provided by culotta2006integrating and included seven relation types: political_affiliation, education, founder, wife/husband, job_title, nationality, and employer."
    ]
  },
  {
    "title": "Data Collection for Interactive Learning through the Dialog",
    "evidence": [
      "ns and answers for questions which it is not able to answer. However, getting access to systems with real users is usually hard. Therefore, we used the crowdsourcing platform CrowdFlower (CF) for our data collection.",
      "provided by users and 702 dialogs have incorrect answer. In the remaining 847 dialogs users did not want to answer the question. The collected dialogs also contain 1828 paraphrases and 1539 explanations for 1870 questions.",
      "ble answers. Therefore lot of answers from users were labeled as incorrect even though those answers perfectly fit the question. Our annotators identified that 285 of the incorrect answers were answers for such general questions.",
      "em that the answer points to the same Freebase entity that was present in Simple questions dataset for that particular question. However, a large amount of questions from that dataset is quite general - with many possible answers. Therefore lot of answers from users were labeled as incorrect even though those answers perfectly fit the question.",
      "s during preparation runs of the dataset collection process (google it, check wikipedia, I would need... $\\rightarrow $ Negate). Dataset Properties We collected the dataset with 1900 dialogs and 8533 turns. Topics discussed in dialogs are questions randomly chosen from training examples of Simple questions BIBREF7 dataset.",
      "re the user is asked for a correct answer to the original question. The system requires the user to answer with a full sentence. In practical experiments this has shown as a useful decision because it improves system's ability to reveal cheaters. We can simply measure the connection (in terms of common words ) between question and the answer sentence.",
      "d not want to answer the question. The collected dialogs also contain 1828 paraphrases and 1539 explanations for 1870 questions. An answer for a question was labeled as correct by annotators only when it was evident to them that the answer points to the same Freebase entity that was present in Simple questions dataset for that particular question.",
      "g the collected data, we propose the following evaluation measure: $$ S_D = \\frac{n_c - w_i n_i - w_e n_e - w_a n_a}{|D|}$$ (Eq. 20) Here, $n_c$ denotes the number of correctly answered questions, $n_i$ denotes the number of incorrectly answered questions, $n_e$ denotes the number of requested explanations, $n_a$ denotes the number of requested answers and $|D|$ denotes the number of simulated dialogs in the dataset.",
      "Acknowledgment\ns This work was funded by the Ministry of Education, Youth and Sports of the Czech Republic under the grant agreement LK11221 and core research funding, SVV project 260 224, and GAUK grant 1170516 of Charles University in Prague.",
      "oes not know the correct wording. This area is extensively studied for the purposes of information retrieval BIBREF5 , BIBREF6 . The main purpose of the collected dataset is to enable interactive learning using the steps proposed above and potentially to evaluate how different systems perform on this task.",
      "e dataset. We are also planning improvements for dialog management that is used to gain explanations during the data collection. We believe that with conversation about specific aspects of the discussed question it will be possible to gain even more interesting information from users.",
      "systems with real users is usually hard. Therefore, we used the crowdsourcing platform CrowdFlower (CF) for our data collection. A CF worker gets a task instructing them to use our chat-like interface to help the system with a question which is randomly selected from training examples of Simple questions BIBREF7 dataset.",
      "formation gained from users in simulated question answering dialogs. Second, it measures accuracy on answer hints understanding. For purposes of evaluation we collected a dataset from conversational dialogs with workers on crowdsourcing platform CrowdFlower. Those dialogs were annotated with expert annotators and published under Creative Commons 4.0 BY-SA license on lindat."
    ],
    "answer": [
      "The data was collected using the crowdsourcing platform CrowdFlower (CF)."
    ]
  },
  {
    "title": "Distilling Translations with Visual Awareness",
    "evidence": [
      "results\nof our experiments, first in the original dataset without any source degradation (Section SECREF18 ) and then in the setup with various source degradation strategies (Section SECREF25 ). Standard setup Table TABREF14 shows the",
      "ing better use of the target language textual context (both left and right-side contexts) and (ii) making use of visual context. This approach leads to the state of the art",
      "te of the art performance for transformer networks (constrained models) on the English-German dataset, as compared to BIBREF30 . Second, our deliberation models lead to significant improvements over this baseline across test sets (average INLINEFORM0 , INLINEFORM1 ).",
      "hree strategies to replace words by a placeholder: random source words, ambiguous source words and gender unmarked source words. The procedure is applied to the train, validation and test sets. For the resulting dataset generated for each setting, we compare models having access to text-only context versus additional text and multimodal contexts.",
      "t of 10-best samples from the first pass decoder, with a beam search of size 10. We use these as the first-pass decoder samples. We use Adam as optimiser BIBREF29 and train the model until convergence.",
      "results\n. Additionally, we show that it has the ability to recover from erroneous or missing words in the source language. Data We build and test our MMT models on the Multi30K dataset BIBREF21 .",
      "ed by the MultiMT (H2020 ERC Starting Grant No. 678017) and MMVC (Newton Fund Institutional Links Grant, ID 352343575) projects. We also thank the annotators for their valuable help.",
      "results\nin two language-specific datasets. In this setting (PERS), we use the Flickr Entities dataset BIBREF28 to identify all the words that were annotated by humans as corresponding to the category person.",
      "results\nthan the previous state of the art.",
      "results\nthan the previous state of the art. Adding visual information, and in particular structural representations of this information, proved beneficial when input text contains noise and the language pair requires substantial restructuring from source to target.",
      "r deliberation models lead to significant improvements over this baseline across test sets (average INLINEFORM0 , INLINEFORM1 ). Transformer-based models enriched with image information (base+sum, base+att and base+obj), on the other hand, show no major improvements with respect to the base performance.",
      "is information. We propose a translate-and-refine approach to this problem where images are only used by a second stage decoder. This approach is trained jointly to generate a good first draft translation and to improve over this draft by (i) making better use of the target language textual context (both left and right-side contexts) and (ii) making use of visual context.",
      "s, we first train the standard transformer model until convergence, and use it to initialise the encoder and first-pass decoder. For each of the training samples, we follow BIBREF19 and obtain a set of 10-best samples from the first pass decoder, with a beam search of size 10. We use these as the first-pass decoder samples."
    ],
    "answer": [
      "The Multi30K dataset."
    ]
  },
  {
    "title": "English-Japanese Neural Machine Translation with Encoder-Decoder-Reconstructor",
    "evidence": [
      "We used two parallel corpora: Asian Scientific Paper Excerpt Corpus (ASPEC) BIBREF0 and NTCIR PatentMT Parallel Corpus BIBREF1 . Regarding the training data of ASPEC, we used only the first 1 million sentences sorted by sentence-alignment similarity.",
      "xperiment on two parallel corpora of English-Japanese and Japanese-English translation tasks using encode-decoder-reconstructor. Our experiments show that their method offers significant improvement in BLEU scores and alleviates the problem of repeating and missing words in the translation on English-Japanese translation task, though the difference is not significant on Japanese-English translation task.",
      "nts We evaluated the encoder-decoder-reconstructor framework for NMT on English-Japanese and Japanese-English translation tasks. Datasets We used two parallel corpora: Asian Scientific Paper Excerpt Corpus (ASPEC) BIBREF0 and NTCIR PatentMT Parallel Corpus BIBREF1 .",
      " hidden state of the decoder into the source sequence by a new decoder to enforce agreement between source and target sentences. In order to confirm the language independence of the framework, we experiment on two parallel corpora of English-Japanese and Japanese-English translation tasks using encode-decoder-reconstructor.",
      "IBREF3 and the encoder-decoder-reconstructor that jointly trained forward translation and back-translation without pre-training. The RNN used in the experiments had 512 hidden units, 512 embedding units, 30,000 vocabulary size and 64 batch size. We used Adagrad (initial learning rate 0.01) for optimizing model parameters. We trained our model on GeForce GTX TITAN X GPU.",
      "s the numbers of the sentences in each corpus. Note that sentences with more than 40 words were excluded from the training data. Models We used the attention-based NMT BIBREF2 as a baseline-NMT, the encoder-decoder-reconstructor BIBREF3 and the encoder-decoder-reconstructor that jointly trained forward translation and back-translation without pre-training.",
      "d English sentences were tokenized by tokenizer.perl of Moses. Table TABREF14 shows the numbers of the sentences in each corpus. Note that sentences with more than 40 words were excluded from the training data.",
      "REF1 . Regarding the training data of ASPEC, we used only the first 1 million sentences sorted by sentence-alignment similarity. Japanese sentences were segmented by the morphological analyzer MeCab (version 0.996, IPADIC), and English sentences were tokenized by tokenizer.perl of Moses. Table TABREF14 shows the numbers of the sentences in each corpus.",
      "der-reconstructor framework that optimizes NMT by back-translation from the output sentences into the original source sentences. In their method, after training the forward translation in a manner similar to the conventional attention-based NMT, they train a back-translation model from the hidden state of the decoder into the source sequence by a new decoder to enforce agreement between source and target sentences.",
      "that we have to train the forward translation model in a manner similar to the conventional attention-based NMT as pre-training. The main contributions of this paper are as follows:",
      "g2016 introduced a distributed version of coverage vector taken from PBSMT to consider which words have been already translated. All these methods, including ours, employ information of the source sentence to improve the quality of translation, but our method uses back-translation to ensure that there is no inconsistency.",
      "T using back-translation. In this method, they selected the best forward translation model in the same manner as Bahdanau et al. (2015), and then trained a bi-directional translation model as fine-tuning. Their experiments show that it offers significant improvement in BLEU scores in Chinese-English translation task.",
      "EF23 after training the forward translation in a manner similar to the conventional attention-based NMT using Equation EQREF13 . In addition, we experiment to jointly train a model of forward translation and back-translation without pre-training. It may learn a globally optimal model compared to locally optimal model pre-trained using the forward translation."
    ],
    "answer": [
      "Asian Scientific Paper Excerpt Corpus (ASPEC) BIBREF0 and NTCIR PatentMT Parallel Corpus BIBREF1"
    ]
  },
  {
    "title": "Stereotyping and Bias in the Flickr30K Dataset",
    "evidence": [
      "ce against this assumption, and provides a list of biases and unwarranted inferences that can be found in the Flickr30K dataset. Finally, it considers methods to find examples of these, and discusses how we should deal with stereotype-driven descriptions in future applications.",
      "t al., 2014) is that they\"focus only on the information that can be obtained from the image alone\"(Hodosh et al., 2013, p. 859). This paper presents some evidence against this assumption, and provides a list of biases and unwarranted inferences that can be found in the Flickr30K dataset.",
      "conclusion\nthat the dataset is biased. I have manually categorized each of the baby images. There are 504 white, 66 asian, and 36 black babies. 73 images do not contain a baby, and 18 images do not fall into any of the other categories.",
      "hrases. Because there are many ways in which a phrase can be biased, it is difficult to automatically detect bias from the data. So how should we deal with stereotype-driven descriptions?",
      "ace One interesting pattern is that the ethnicity/race of babies doesn't seem to be mentioned unless the baby is black or asian. In other words: white seems to be the default, and others seem to be marked. How can we tell whether or not the data is actually biased?",
      " words: white seems to be the default, and others seem to be marked. How can we tell whether or not the data is actually biased? We don't know whether or not an entity belongs to a particular social class (in this case: ethnic group) until it is marked as such.",
      "ns. This paper aims to give an overview of linguistic bias and unwarranted inferences resulting from stereotypes and prejudices. I will build on earlier work on linguistic bias in general BIBREF3 , providing examples from the Flickr30K data, and present a taxonomy of unwarranted inferences. Finally, I will discuss several methods to analyze the data in order to detect biases.",
      "ptions in the Flickr30K dataset. I have divided these descriptions into two classes: linguistic bias and unwarranted inferences. The former corresponds to the annotators' choice of words when confronted with an image that may or may not match their stereotypical expectancies.",
      "h they say hurts the accuracy and the consistency of the descriptions. But the problem is not with the lack of consistency here. Quite the contrary: the problem is that stereotypes may be pervasive enough for the data to be consistently biased.",
      "ls trained on this data may propagate harmful stereotypes, such as the idea that women are less suited for leadership positions. This paper aims to give an overview of linguistic bias and unwarranted inferences resulting from stereotypes and prejudices.",
      " the annotator is biased about the gender of basketball players; they might just be helpful by providing a detailed description. In section 3 I will discuss how to establish whether or not there is any bias in the data regarding the use of adjectives.",
      "stency here. Quite the contrary: the problem is that stereotypes may be pervasive enough for the data to be consistently biased. And so language models trained on this data may propagate harmful stereotypes, such as the idea that women are less suited for leadership positions.",
      "nd it serves as a warning: if we train and evaluate language models on this data, we are effectively teaching them to be biased. I have also looked at methods to detect stereotype-driven descriptions, but due to the richness of language it is difficult to find an automated measure."
    ],
    "answer": [
      "The biases found in the Flickr30K dataset include linguistic bias and unwarranted inferences resulting from stereotypes and prejudices, such as the assumption that white is the default ethnicity/race and others are marked, and the propagation of harmful stereotypes, such as the idea that women are less suited for leadership positions."
    ]
  },
  {
    "title": "An Evaluation Dataset for Intent Classification and Out-of-Scope Prediction",
    "evidence": [
      "edure for the out-of-scope set, except that we split the data into train/validation/test based on task prompt instead of worker. Dataset ::: Dataset Variants In addition to the full dataset, we consider three variations. First, Small, in which there are only 50 training queries per each in-scope intent, rather than 100.",
      "research and development of dialog systems. All data introduced in this paper can be found at https://github.com/clinc/oos-eval. Dataset We introduce a new crowdsourced dataset of 23,700 queries, including 22,500 in-scope queries covering 150 intents, which can be grouped into 10 general domains. The dataset also includes 1,200 out-of-scope queries.",
      " a wide range of samples per intent class (ranging from 24 to 5,981 queries per intent, and so is not constrained in all cases). BIBREF11 created datasets with constrained training data, but with very few intents, presenting a very different type of challenge.",
      "ueries covering 150 intents, which can be grouped into 10 general domains. The dataset also includes 1,200 out-of-scope queries. Table TABREF2 shows examples of the data.",
      " the 1,200 out-of-scope queries collected, 100 are used for validation and 100 are used for training, leaving 1,000 for testing. Dataset ::: Data Preprocessing and Partitioning For all queries collected, all tokens were down-cased, and all end-of-sentence punctuation was removed. Additionally, all duplicate queries were removed and replaced.",
      " new dataset with 23,700 queries that are short and unstructured, in the same style made by real users of task-oriented systems. The queries cover 150 intents, plus out-of-scope queries that do not fall within any of the 150 in-scope intents. We evaluate a range of benchmark classifiers and out-of-scope handling methods on our dataset.",
      "grouped into 10 general domains. The dataset also includes 1,200 out-of-scope queries. Table TABREF2 shows examples of the data. Dataset ::: In-Scope Data Collection We defined the intents with guidance from queries collected using a scoping crowdsourcing task, which prompted crowd workers to provide questions and commands related to topic domains in the manner they would interact with an artificially intelligent assistant.",
      "REF11 created datasets with constrained training data, but with very few intents, presenting a very different type of challenge. We also include the TREC query classification datasets BIBREF7, which have a large set of labels, but they describe the desired response type (e.g., distance, city, abbreviation) rather than the action intents we consider.",
      "Results\nwith oos-train Table TABREF14 presents",
      "ples of each. Snips does present a low-data variation, with 70 training queries per intent, in which performance drops slightly. The dataset presented in BIBREF10 has a large number of intent classes, yet also contains a wide range of samples per intent class (ranging from 24 to 5,981 queries per intent, and so is not constrained in all cases).",
      "nsive and difficult to generate. We believe our analysis and dataset will lead to developing better, more robust dialog systems. All datasets introduced in this paper can be found at https://github.com/clinc/oos-eval.",
      "edge of the capabilities of a system. Table TABREF20 compares our dataset with other short-query intent classification datasets. The Snips BIBREF0 dataset and the dataset presented in BIBREF10 are the most similar to the in-scope part of our work, with the same type of conversational agent requests. Like our work, both of these datasets were bootstrapped using crowdsourcing.",
      "ile current models work on known classes, they have difficulty on out-of-scope queries, particularly when data is not plentiful. This dataset will enable future work to address this key gap in the research and development of dialog systems. All data introduced in this paper can be found at https://github.com/clinc/oos-eval."
    ],
    "answer": [
      "The dataset contains 23,700 queries, including 22,500 in-scope queries and 1,200 out-of-scope queries."
    ]
  },
  {
    "title": "Automatic Judgment Prediction via Legal Reading Comprehension",
    "evidence": [
      "results\non this dataset demonstrate that our model achieves significant improvement over state-of-the-art models. We will publish all source codes and datasets of this work on \\urlgithub.com for further research.",
      "el the complementary inputs. (2) We construct a real-world dataset for experiments, and plan to publish it for further research. (3) Besides baselines from previous works, we also carry out comprehensive experiments comparing different existing deep neural network methods on our dataset. Supported by these experiments, improvements achieved by LRC prove to be robust.",
      "regular expressions, since the original documents have special typographical characteristics indicating the discourse structure. We also take into account law articles and their corresponding juridical interpretations. We also implement and evaluate previous methods on our dataset, which prove to be strong baselines. Our experiment",
      "is the number of training data. As all the calculation in our model is differentiable, we employ Adam BIBREF32 for optimization. Experiments To evaluate the proposed LRC framework and the AutoJudge model, we carry out a series of experiments on the divorce proceedings, a typical yet complex field of civil cases. Divorce proceedings often come with several kinds of pleas, e.g.",
      "can explore the following directions: (1) Limited by the datasets, we can only verify our proposed model on divorce proceedings. A more general and larger dataset will benefit the research on judgment prediction. (2) Judicial decisions in some civil cases are not always binary, but more diverse and flexible ones, e.g. compensation amount.",
      "results\nshow significant improvements over previous methods. Further experiments demonstrate that our model also achieves considerable improvement over other off-the-shelf state-of-the-art models under classification and question answering framework respectively.",
      "results\n.",
      "results\nalso demonstrate the effectiveness and interpretability of our proposed model. In the future, we can explore the following directions: (1) Limited by the datasets, we can only verify our proposed model on divorce proceedings.",
      "ildren, compensation, and maintenance, which focuses on different aspects and thus makes it a challenge for judgment prediction. Dataset Construction for Evaluation Since none of the datasets from previous works have been published, we decide to build a new one.",
      " reading mechanism and re-formalize judgment prediction as Legal Reading Comprehension to better model the complementary inputs. (2) We construct a real-world dataset for experiments, and plan to publish it for further research.",
      "ing deep neural network methods on our dataset. Supported by these experiments, improvements achieved by LRC prove to be robust. Judgment Prediction Automatic judgment prediction has been studied for decades. At the very first stage of judgment prediction studies, researchers focus on mathematical and statistical analysis of existing cases, without any",
      "text with regular expression. Afterwards, we select the most relevant 10 articles according to the fact descriptions as follows. We obtain sentence representation with CBOW BIBREF36 , BIBREF37 weighted by inverse document frequency, and calculate cosine distance between cases and law articles. Word embeddings are pre-trained with Chinese Wikipedia pages.",
      "NLINEFORM6 , INLINEFORM7 to INLINEFORM8 , batch size to 64. We employ precision, recall, F1 and accuracy for evaluation metrics. We repeat all the experiments for 10 times, and report the average"
    ],
    "answer": [
      "divorce proceedings, a typical yet complex field of civil cases."
    ]
  },
  {
    "title": "Improved Neural Relation Detection for Knowledge Base Question Answering",
    "evidence": [
      "Abstract\nRelation detection is a core component for many NLP applications including Knowledge Base Question Answering (KBQA).",
      "Introduction\nKnowledge Base Question Answering (KBQA) systems answer questions by obtaining information from KB tuples BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 .",
      "ion performance independently as well as evaluate on the KBQA end task. SimpleQuestions (SQ): It is a single-relation KBQA task. The KB we use consists of a Freebase subset with 2M entities (FB2M) BIBREF2 , in order to compare with previous research.",
      "IBREF7 , BIBREF2 ; and (b) a more complex case, where some constraints need to be handled for multiple entities in the question. The KBQA system in the figure performs two key tasks: (1) entity linking, which links $n$ -grams in questions to KB entities, and (2) relation detection, which identifies the KB relation(s) a question refers to.",
      "results\nto the state-of-the-art. The third block of the table details two ablation tests for the proposed components in our KBQA systems: (1) Removing the entity re-ranking step significantly decreases the scores.",
      ", multiple entities in the question). Finally the highest scored query from the above steps is used to query the KB for answers. Our main contributions include: (i) An improved relation detection model by hierarchical matching between questions and relations with residual learning; (ii) We demonstrate that the improved relation detector enables our simple KBQA system to achieve state-of-the-art",
      "detection could benefit the KBQA end task, we also propose a simple KBQA implementation composed of two-step relation detection. Given an input question and a set of candidate entities retrieved by an entity linker based on the question, our proposed relation detection model plays a key role in the KBQA process: (1) Re-ranking the entity candidates according to whether they connect to high confident relations detected from the raw question text by the relation detection model.",
      "results\nevidence that our approach achieves not only outstanding relation detection performance, but more importantly, it helps our KBQA system to achieve state-of-the-art accuracy for both single-relation (SimpleQuestions) and multi-relation (WebQSP) QA benchmarks.",
      "abstract\nion. Additionally, we propose a simple KBQA system that integrates entity linking and our proposed relation detector to enable one enhance another. Experimental",
      "sts for the proposed components in our KBQA systems: (1) Removing the entity re-ranking step significantly decreases the scores. Since the re-ranking step relies on the relation detection models, this shows that our HR-BiLSTM model contributes to the good performance in multiple ways. Appendix C gives the detailed performance of the re-ranking step. (2) In contrast to the",
      "line system. We make minimal efforts beyond the training of the relation detection model, making the whole system easy to build. Following previous work BIBREF4 , BIBREF5 , our KBQA system takes an existing entity linker to produce the top- $K$ linked entities, $EL_K(q)$ , for a question $q$ (“initial entity linking”).",
      " task. The KB we use consists of a Freebase subset with 2M entities (FB2M) BIBREF2 , in order to compare with previous research. yin2016simple also evaluated their relation extractor on this data set and released their proposed question-relation pairs, so we run our relation detection model on their data set. For the KBQA evaluation, we also start with their entity linking",
      "Conclusion\nKB relation detection is a key step in KBQA and is significantly different from general relation extraction tasks. We propose a novel KB relation detection model, HR-BiLSTM, that performs hierarchical matching between questions and KB relations."
    ],
    "answer": [
      "The core component for KBQA is relation detection."
    ]
  },
  {
    "title": "Knowledge Graph Representation with Jointly Structural and Textual Encoding",
    "evidence": [
      " is the score function. We use the standard $L_2$ regularization of all the parameters, weighted by the hyperparameter $\\eta $ . Experiment In this section, we study the empirical performance of our proposed models on two benchmark tasks: triplet classification and link prediction. Datasets We use two popular knowledge bases: WordNet BIBREF0 and Freebase BIBREF1 in this paper.",
      "elieve that our model can be further improved by adopting the score functions of other state-of-the-art methods, such as TransD. Besides, textual information largely alleviates the issue of sparsity and our model achieves substantial improvement on Mean Rank comparing with TransD.",
      "rchitecture. Experiments show that our models outperform baseline by margin on link prediction and triplet classification tasks. Source codes of this paper will be available on Github.",
      "sification and link prediction. Datasets We use two popular knowledge bases: WordNet BIBREF0 and Freebase BIBREF1 in this paper. Specifically, we use WN18 (a subset of WordNet) BIBREF24 and FB15K (a subset of Freebase) BIBREF2 since their text descriptions are easily publicly available. Table 1 lists statistics of the two datasets.",
      "oportion of valid entities or relations ranked in top $p$ predictions. Here, we set $p=10$ for entities and $p=1$ for relations. A lower Mean Rank and a higher Hits@p should be achieved by a good embedding model. We call this evaluation setting “Raw”. Since a false predicted triplet may also exist in knowledge graphs, it should be regard as a valid triplet.",
      "anking a set of candidate entities from the knowledge graph. Similar to BIBREF2 , we use two measures as our evaluation metrics. (1) Mean Rank: the averaged rank of correct entities or relations; (2) Hits@p: the proportion of valid entities or relations ranked in top $p$ predictions. Here, we set $p=10$ for entities and $p=1$ for relations.",
      ", BIBREF29 to map textual relations and knowledge base relations to the same vector space and obtained substantial improvements. While releasing the current paper we discovered a paper by BIBREF30 proposing a similar model with attention mechanism which is evaluated on link prediction and triplet classification.",
      "results\non link prediction and triplet classification show that our joint models can handle the sparsity problem well and outperform the baseline method on all metrics with a large margin. Our contributions in this paper are summarized as follows.",
      "following the same setting used in BIBREF3 . For triplets classification, we set a threshold $\\delta _r$ for each relation $r$ . $\\delta _r$ is obtained by maximizing the classification accuracies on the valid set. For a given triplet $(h, r, t)$ , if its score is larger than $\\delta _r$ , it will be classified as positive, otherwise negative. Table 4 shows the evaluation",
      "tion largely alleviates the issue of sparsity and our model achieves substantial improvement on Mean Rank comparing with TransD. However, textual information may slightly degrade the representation of frequent entities which have been well-trained. This may be another reason why our Hits@10 is worse than TransD which only utilizes structural information.",
      "2,$$ (Eq. 22) where $\\gamma > 0$ is a margin between golden triplets and negative triplets., $f(h, r, t)$ is the score function. We use the standard $L_2$ regularization of all the parameters, weighted by the hyperparameter $\\eta $ .",
      "tion to complete a triplet $(h, r, t)$ with $h$ or $t$ missing, i.e., predict $t$ given $(h, r)$ or predict $h$ given $(r, t)$ . Rather than requiring one best answer, this task emphasizes more on ranking a set of candidate entities from the knowledge graph. Similar to BIBREF2 , we use two measures as our evaluation metrics.",
      "by BIBREF30 proposing a similar model with attention mechanism which is evaluated on link prediction and triplet classification. However, our work encodes text description as a whole without explicit segmentation of sentences, which breaks the order and coherence among sentences."
    ],
    "answer": [
      "WN18 (a subset of WordNet) and FB15K (a subset of Freebase)"
    ]
  },
  {
    "title": "State-of-the-Art Vietnamese Word Segmentation",
    "evidence": [
      "Abstract\nWord segmentation is the first step of any tasks in Vietnamese language processing. This paper reviews stateof-the-art approaches and systems for word segmentation in Vietnamese.",
      "ations on building corpus and implementing machine learning techniques to improve the accuracy for Vietnamese word segmentation. According to our observation, this study also reports a few of achivements and limitations in existing Vietnamese word segmentation systems.",
      "n Vietnamese language processing. This paper reviews stateof-the-art approaches and systems for word segmentation in Vietnamese. To have an overview of all stages from building corpora to developing toolkits, we discuss building the corpus stage, approaches applied to solve the word segmentation and existing toolkits to segment words in Vietnamese sentences.",
      " corpus stage, approaches applied to solve the word segmentation and existing toolkits to segment words in Vietnamese sentences. In addition, this study shows clearly the motivations on building corpus and implementing machine learning techniques to improve the accuracy for Vietnamese word segmentation.",
      "found that is lacks of complete review approaches, datasets and toolkits which we recently used in Vietnamese word segmentation. A all sided review of word segmentation will help next studies on Vietnamese natural language processing tasks have an up-to-date guideline and choose the most suitable solution for the task. The remaining part of the paper is organized as follows.",
      "Vietnamese as well. This paper aims reviewing state-of-the-art word segmentation approaches and systems applying for Vietnamese. This study will be a foundation for studies on Vietnamese word segmentation and other following Vietnamese tasks as well, such as part-of-speech tagger, chunker, or parser systems.",
      "amese word segmentation and other following Vietnamese tasks as well, such as part-of-speech tagger, chunker, or parser systems. There are several studies about the Vietnamese word segmentation task over the last decade. Dinh et al. started this task with Weighted Finite State Transducer (WFST) approach and Neural Network approach BIBREF9 .",
      "in Section IV. Section V mainly addresses two existing toolkits, vnTokenizer and JVnSegmenter, for Vietnamese word segmentation. Several experiments based on mentioned approaches and toolkits are described in Section VI. Finally,",
      "f Vietnamese word segmentation. The review pointed out common features and methods used in Vietnamese word segmentation studies. This study also had an evaluation of the existing Vietnamese word segmentation toolkits based on a same corpus to show advantages and disadvantages as to shed some lights on system enhancement.",
      "CONCLUSION\nS AND FUTURE WORKS This study reviewed state-of-the-art approaches and systems of Vietnamese word segmentation. The review pointed out common features and methods used in Vietnamese word segmentation studies.",
      " al. started the word segmentation task for Vietnamese with Neural Network and Weighted Finite State Transducer (WFST) BIBREF9 . Nguyen et al. continued with machine learning approaches, Conditional Random Fields and Support Vector Machine BIBREF7 . Most of statistical approaches are based on the architecture as shown in Figure 2.",
      "d token-based features, studies used machine learning approaches to build word segmentation systems with accuracy about 94%-97%. According to our observation, we found that is lacks of complete review approaches, datasets and toolkits which we recently used in Vietnamese word segmentation.",
      " out several Vietnamese word segmentation inconsistencies in the corpus based on POS information and n-gram sequences BIBREF14 . Currently, there are at least three available word segmentation corpus used in Vietnamese word segmentation studies and systems. Firstly, Dinh et al. built the CADASA corpus from CADASA’s books BIBREF15 . Secondly, Nguyen et al."
    ],
    "answer": [
      "The approaches applied to solve word segmentation in Vietnamese include Weighted Finite State Transducer (WFST), Neural Network, Conditional Random Fields, and Support Vector Machine."
    ]
  },
  {
    "title": "A Multi-Task Architecture on Relevance-based Neural Query Translation",
    "evidence": [
      "espectively. It is evident that our model achieves better balance compared to baseline transformer, except for a very few cases. Given a query INLINEFORM0 , consider INLINEFORM1 as the set of terms from human translation of INLINEFORM2 and INLINEFORM3 as the set of translation terms generated by model INLINEFORM4 .",
      "lance values for transformer and our model for a random sample of 20 queries from the test set of Italian queries, respectively. It is evident that our model achieves better balance compared to baseline transformer, except for a very few cases.",
      "tion: In the decoding stage, can we restrict an NMT model so that it does not only generate terms that are highly likely in TC?. We show that training a strong baseline NMT with RAT roughly achieves 16% improvement over the baseline. Using a qualitative analysis, we further show that RAT works as a regularizer and prohibits NMT to overfit to TC vocabulary.",
      "s not work for our task because the connection between a query and ranked sentences falls rapidly after the top one (see below). We consider two data sources for learning NMT and RAT jointly. The first one is a sentence-level parallel corpus, which we refer to as translation corpus, INLINEFORM0 .",
      "Acknowledgment\ns This work was supported in part by the Center for Intelligent Information Retrieval and in part by the Air Force Research Laboratory (AFRL) and IARPA under contract #FA8650-17-C-9118 under subcontract #14775 from Raytheon BBN Technologies Corporation.",
      "values for transformer and our model for a random sample of 20 queries from the validation set of Italian queries, respectively. Figure FIGREF16 shows the balance values for transformer and our model for a random sample of 20 queries from the test set of Italian queries, respectively.",
      "he single top document retrieved against a sentence INLINEFORM0 because a sentence is a weak representation of information need. As a result, documents at lower ranks show heavy shift from the context of the sentence query. We verified this by observing that a relevance model constructed from the top INLINEFORM1 documents does not perform well in this setting.",
      "indow following BIBREF6 . In the figure, INLINEFORM0 and INLINEFORM1 represents the top document retrieved against INLINEFORM2 . The shuffler component shuffles INLINEFORM3 and INLINEFORM4 and creates (context, pivot) pairs. After that those data points are passed through a fully connected linear projection layer and eventually to the transformation function.",
      "i-task learning architecture that achieves 16% improvement over a strong NMT baseline on Italian-English query-document dataset. We show using both quantitative and qualitative analysis that our model generates balanced and precise translations with the regularization effect it achieves from multi-task learning paradigm.",
      "earning component shares two layers with the NMT model. The goal is to expose the retrieval corpus' vocabulary to the NMT model. We discuss layer sharing in the next section. We select the single top document retrieved against a sentence INLINEFORM0 because a sentence is a weak representation of information need.",
      "te the balance of INLINEFORM8 , INLINEFORM9 . If INLINEFORM10 is close to 1, the translation terms are as likely in TC as in RC. Figure FIGREF15 shows the balance values for transformer and our model for a random sample of 20 queries from the validation set of Italian queries, respectively.",
      "t pivot word Here, we use a scaling factor INLINEFORM6 , to have a balance between the gradients from the NMT loss and RAT loss. For RAT, the context is drawn from a context window following BIBREF6 . In the figure, INLINEFORM0 and INLINEFORM1 represents the top document retrieved against INLINEFORM2 .",
      "ne. Using a qualitative analysis, we further show that RAT works as a regularizer and prohibits NMT to overfit to TC vocabulary. Balanced Translation Approach We train NMT with RAT to achieve better query translations. We improve a recently proposed NMT baseline, Transformer, that achieves state-of-the-art"
    ],
    "answer": [
      "Our model achieves better balance compared to baseline transformer, except for a very few cases."
    ]
  },
  {
    "title": "Unsupervised Learning of Syntactic Structure with Invertible Neural Projections",
    "evidence": [
      "LINEFORM3 to be invertible, and show that this constraint enables tractable exact inference and marginal likelihood computation. Learning & Inference In this section, we introduce an invertibility condition for our neural projector to tackle the optimization challenge. Specifically, we constrain our neural projector with two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists.",
      "us word representations in an unsupervised fashion by cascading an invertible neural network with a structured generative prior. We show that the invertibility condition allows for efficient exact inference and marginal likelihood computation in our model so long as the prior is well-behaved.",
      "ers propose specially designed invertible networks that are easily trainable yet still powerful BIBREF16 , BIBREF17 , BIBREF19 . Inspired by these works, we use the invertible transformation proposed by BIBREF16 , which consists of a series of “coupling layers”. This architecture is specially designed to guarantee a unit Jacobian determinant (and thus the invertibility property).",
      "on challenge. Specifically, we constrain our neural projector with two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists. Invertible transformations have been explored before in independent components analysis BIBREF14 , gaussianization BIBREF15 , and deep density models BIBREF16 , BIBREF17 , BIBREF18 , for unstructured data.",
      "layers”. This architecture is specially designed to guarantee a unit Jacobian determinant (and thus the invertibility property). From Eq. ( EQREF22 ) we know that only INLINEFORM0 is required for accomplishing learning and inference; we never need to explicitly construct INLINEFORM1 . Thus, we directly define the architecture of INLINEFORM2 .",
      "nvertibility condition, we derive a learning algorithm and give another view of our approach revealed by the objective function. Then, we present the architecture of a neural projector we use in experiments: a volume-preserving invertible neural network proposed by BIBREF16 for independent components estimation.",
      "minant is equal to zero, which means the projection is non-invertible and thus information is being lost through the projection. Such “information preserving” regularization is crucial during optimization, otherwise the trivial solution of always projecting data to the same single point to maximize likelihood is viable.",
      "its Jacobian matrix is triangular with all ones on the main diagonal. Thus the Jacobian determinant is always equal to one (i.e. volume-preserving) and the invertibility condition is naturally satisfied. To be sufficiently expressive, we compose multiple coupling layers as suggested in BIBREF16 .",
      "1 the left subset INLINEFORM2 is unchanged, while from INLINEFORM3 to INLINEFORM4 the right subset INLINEFORM5 remains the same. Also note that composing multiple coupling layers does not change the volume-preserving and invertibility properties. Such a sequence of invertible transformations from the data space INLINEFORM6 to INLINEFORM7 is also called normalizing flow BIBREF20 .",
      "ior and marginal likelihood of our Markov-structured model can be computed with the forward-backward algorithm. We can apply Eq. ( EQREF14 ) and obtain : INLINEFORM3 By using the change of variable rule to the integration, which allows the integration variable INLINEFORM0 to be replaced by INLINEFORM1 , the marginal emission factor can be computed in closed-form when the invertibility condition is satisfied: DISPLAYFORM0 where INLINEFORM0 is a conditional Gaussian distribution, INLINEFORM1 is the Jacobian matrix of function INLINEFORM2 at INLINEFORM3 , and INLINEFORM4 represents the absolute value of its determinant.",
      "an be used to marginalize out INLINEFORM0 . Therefore, our joint model inherits the tractability of the underlying syntax model. Invertible Volume-Preserving Neural Net For the projection we can use an arbitrary invertible function, and given the representational power of neural networks they seem a natural choice.",
      "ere, we generalize this style of approach to structured learning, and augment it with discrete latent variables ( INLINEFORM2 ). Under the invertibility condition, we derive a learning algorithm and give another view of our approach revealed by the objective function.",
      "we use in experiments: a volume-preserving invertible neural network proposed by BIBREF16 for independent components estimation. Learning with Invertibility For ease of exposition, we explain the learning algorithm in terms of Markov structure without loss of generality. As shown in Eq."
    ],
    "answer": [
      "The invertibility condition is that the Jacobian matrix of the transformation is triangular with all ones on the main diagonal."
    ]
  },
  {
    "title": "Crowdsourcing for Beyond Polarity Sentiment Analysis A Pure Emotion Lexicon",
    "evidence": [
      " submit their answers that go through a filtering process. If the answers are considered valid, they update the lexicon entries. The crowd also evaluates existing annotations, to determine the lexicon quality. As crowd evaluation methods are new in lexicon acquisition tasks, we compare crowd evaluations to those of expert linguists.",
      " crowdsourcing method for lexicon acquisition, which is scalable, cost-effective, and doesn't require experts or gold standards. We also compare crowd and expert evaluations of the lexicon, to assess the overall lexicon quality, and the evaluation capabilities of the crowd.",
      "te their term corpora BIBREF16 , or use established lexicon such as WordNet, SentiWordNet, and various other lexicons BIBREF13 . Other studies combine manual labeling or machine learning with lexicons BIBREF17 . Manual lexicon acquisition is constrained by the number of people contributing to the task, and the number of annotations from each participant.",
      " evaluate the whole corpus. However, the uniformity of expert judgement is replaced with the diversity and mass of contributors. The corpus may be limiting the term groups in the lexicon to specific domain-specific subjects. Comparisons with existing lexicons, such as NRC BIBREF21 indicate a moderate overlap with 40% common terms.",
      "ty. As crowd evaluation methods are new in lexicon acquisition tasks, we compare crowd evaluations to those of expert linguists. Data During January 2017, we performed a keyword based crawl for articles and comments in the Europe subreddit and tweets in Twitter, which contained the word \"Brexit\".",
      "results\nfor both subjective and objective tasks. Subcomponents of the lexicon acquisition could be improved on an individual basis.",
      "sed on their rooted and checked for spelling. The resulting stems along with their stem groups are stored in a lexicon database. Crowdsourcing is using the lexicon database and the crowd to annotate each entry in the database. Participants submit their answers that go through a filtering process. If the answers are considered valid, they update the lexicon entries.",
      "gnificantly inhibits automated sentiment lexicon aqcuisition methods from achieving relevance equal to manual methods BIBREF15 . Thus a lot of researchers choose to manually annotate their term corpora BIBREF16 , or use established lexicon such as WordNet, SentiWordNet, and various other lexicons BIBREF13 .",
      "ment tracking, marketing, text correction, and text to speech systems can be improved with the use of distinct emotion lexicons. However, beyond polarity studies acquire lexicons based on a set of strict rules, and the evaluation of experts. These lexicons use only a single emotion per term BIBREF9 .",
      "ults\nfor both subjective and objective tasks. Subcomponents of the lexicon acquisition could be improved on an individual basis. Spell check can include spelling recommendations, filtering could incorporate rewarding and penalties, evaluation process can include experts and so on. Crowd diversity in the annotation and evaluation process is another limiting factor.",
      "icons based on a set of strict rules, and the evaluation of experts. These lexicons use only a single emotion per term BIBREF9 . The problems of these approaches is the lack of uniformity and contribution freedom when relying on gold standards, and high costs with low scalability when employing experts.",
      " almost identical. The number of crowd evaluations is the factor that provides a degree of freedom in the evaluation strictness. Limitations Lexicon acquisition is a complex task that includes a mixture of objective and subjective tasks.",
      " groups, of those only 22 thousand annotations for 9737 term groups are included in the final lexicon, as a result of filtering. Each term group had a mean 2.3 annotations from a total of 95 different annotators. Although the number of mean annotations in lexicon is less than half the mean annotations in the unfiltered corpus, the PEL annotations are considered of higher quality."
    ],
    "answer": [
      "Crowd evaluations are compared to those of expert linguists to assess the overall lexicon quality and the evaluation capabilities of the crowd."
    ]
  },
  {
    "title": "Explicit Utilization of General Knowledge in Machine Reading Comprehension",
    "evidence": [
      "results\n, KAR is not only comparable in performance with the state-of-the-art MRC models, but also superior to them in terms of both the hunger for data and the robustness to noise. The reasons for these achievements, we believe, are as follows:",
      "MRC model named as Knowledge Aided Reader (KAR), which explicitly uses the general knowledge to assist its attention mechanisms. Experimental",
      "method, KAR is comparable in performance with the state-of-the-art MRC models, and significantly more robust to noise than them. When only a subset ( INLINEFORM0 – INLINEFORM1 ) of the training examples are available, KAR outperforms the state-of-the-art MRC models by a large margin, and is still reasonably robust to noise.",
      "n INLINEFORM2 , where INLINEFORM3 , so that the resulting subsequence INLINEFORM4 from INLINEFORM5 is an answer to INLINEFORM6 . Overall Architecture As shown in Figure FIGREF7 , KAR is an end-to-end MRC model consisting of five layers: Lexicon Embedding Layer. This layer maps the words to the lexicon embeddings.",
      "results\nshow that KAR is not only comparable in performance with the state-of-the-art MRC models, but also superior to them in terms of both the hunger for data and the robustness to noise.",
      "method, KAR is comparable in performance with the state-of-the-art MRC models, and significantly more robust to noise than them. When only a subset (20%-80%) of the training examples are available, KAR outperforms the state-of-the-art MRC models by a large margin, and is still reasonably robust to noise.",
      " of the state-of-the-art MRC models; on the adversarial sets, KAR outperforms the state-of-the-art MRC models by a large margin. That is to say, KAR is comparable in performance with the state-of-the-art MRC models, and significantly more robust to noise than them.",
      "as Knowledge Aided Reader (KAR), which explicitly uses the above extracted general knowledge to assist its attention mechanisms. Based on the data enrichment method, KAR is comparable in performance with the state-of-the-art MRC models, and significantly more robust to noise than them.",
      "as Knowledge Aided Reader (KAR), which explicitly uses the above extracted general knowledge to assist its attention mechanisms. Based on the data enrichment method, KAR is comparable in performance with the state-of-the-art MRC models, and significantly more robust to noise than them.",
      "to say, KAR is comparable in performance with the state-of-the-art MRC models, and significantly more robust to noise than them. To verify the effectiveness of general knowledge, we first study the relationship between the amount of general knowledge and the performance of KAR.",
      "is aimed at fusing the question-aware passage representations into themselves so as to obtain the final passage representations. Although KAR is equipped with both categories, its most remarkable feature is that it explicitly uses the general knowledge extracted by the data enrichment method to assist its attention mechanisms.",
      " on the development examples). Knowledge Aided Reader In this section, we elaborate our MRC model: Knowledge Aided Reader (KAR). The key components of most existing MRC models are their attention mechanisms BIBREF13 , which are aimed at fusing the associated representations of each given passage-question pair.",
      " There are totally five such comparative objects, which can be considered as representatives of the state-of-the-art MRC models. As shown in Table TABREF12 , on the development set and the test set, the performance of KAR is on par with that of the state-of-the-art MRC models; on the adversarial sets, KAR outperforms the state-of-the-art MRC models by a large margin."
    ],
    "answer": [
      "Knowledge Aided Reader (KAR)"
    ]
  },
  {
    "title": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
    "evidence": [
      "ords are only meaningful relative to each other. Neither the vector nor its dimensions have any absolute, interpretable meaning. We introduce an additive modification to the objective function of the embedding learning algorithm that encourages the embedding vectors of words that are semantically related to a predefined concept to take larger values along a specified dimension, while leaving the original semantic learning mechanism mostly unaffected.",
      "ept to take larger values along a specified dimension, while leaving the original semantic learning mechanism mostly unaffected. In other words, we align words that are already determined to be related, along predefined concepts. Therefore, we impart interpretability to the word embedding by assigning meaning to its vector dimensions.",
      " 41, 43 and 79 in Table TABREF11 . These dimensions are aligned/imparted with the concepts that are given in the column headers. In Table TABREF11 , the words that are highlighted with green denote the words that exist in the corresponding word-group obtained from Roget's Thesaurus (and are thus explicitly forced to achieve increased dimension values), while the red words denote the words that achieve increased dimension values by virtue of their cooccurrence statistics with the thesaurus-based words (indirectly, without being explicitly forced).",
      " values by virtue of their cooccurrence statistics with the thesaurus-based words (indirectly, without being explicitly forced). This again illustrates that a semantic concept can indeed be coded to a vector dimension provided that a sensible lexical resource is used to guide semantically related words to the desired vector dimension via the proposed objective function in ( SECREF4 ).",
      " designated vector dimension above and beyond the supplied list of words belonging to the concept word-group for that dimension. We also present the list of words with the greatest dimension value for the dimensions 11, 13, 16, 31, 36, 39, 41, 43 and 79 in Table TABREF11 . These dimensions are aligned/imparted with the concepts that are given in the column headers.",
      "hm. The dimension values are denoted with blue and red/green markers for the original and the proposed algorithms, respectively. Additionally, the top-50 words that achieve the greatest 32nd dimension values among the considered 1000 words are emphasized with enlarged markers, along with text annotations.",
      "based information. Word-groups extracted from Roget's Thesaurus are directly mapped to individual dimensions of word embeddings. Specifically, the vector representations of words that belong to a particular group are encouraged to have deliberately increased values in a particular dimension that corresponds to the word-group under consideration.",
      " is used to guide semantically related words to the desired vector dimension via the proposed objective function in ( SECREF4 ). Even the words that do not appear in, but are semantically related to, the word-groups that we formed using Roget's Thesaurus, are indirectly affected by the proposed algorithm.",
      "ke sense only in relation to each other and their specific dimensions do not carry explicit information that can be interpreted. However, being able to interpret a word embedding would illuminate the semantic concepts implicitly represented along the various dimensions of the embedding, and reveal its hidden semantic structures.",
      "dding vectors of a given concept word-group to achieve deliberately increased values along an associated dimension INLINEFORM5 . The relative weight of the second term is controlled by the parameter INLINEFORM6 .",
      "results\nin an improvement in the precision scores. However, the greatest performance increase is seen for the last scenario, which underscores the extent to which the semantic features captured by embeddings can be improved with a reasonable selection of the lexical resource from which the concept word-groups were derived.",
      "achieves both of the objectives simultaneously, increased interpretability and preservation of the intrinsic semantic structure. An important point was that, while it is expected for words that are already included in the concept word-groups to be aligned together since their dimensions are directly updated with the proposed cost term, it was also observed that words not in these groups also aligned in a meaningful manner without any direct modification to their cost function.",
      "mensions even though the objective functions for their particular vectors are not modified. This point cannot be overemphasized. Although the word-groups extracted from Roget's Thesaurus impose a degree of supervision to the process, the fact that the remaining words in the entire vocabulary are also indirectly affected makes the"
    ],
    "answer": [
      "The dimension along which the semantically related words take larger values is the specified dimension that corresponds to the word-group under consideration."
    ]
  },
  {
    "title": "Stance Detection in Turkish Tweets",
    "evidence": [
      " our domain for stance detection. Considerable amounts of tweets are being published for sports-related events at every instant. Hence we have determined our targets as Galatasaray (namely Target-1) and Fenerbahçe (namely, Target-2) which are two of the most popular football clubs in Turkey.",
      "een August 18 and September 6, 2015) and firstly filtered the tweets using the possible names used to refer to the target clubs. Then, we have annotated the stance information in the tweets for these targets as Favor or Against.",
      "s as Galatasaray (namely Target-1) and Fenerbahçe (namely, Target-2) which are two of the most popular football clubs in Turkey. As is the case for the sentiment analysis tools, the outputs of the stance detection systems on a stream of tweets about these clubs can facilitate the use of the opinions of the football followers by these clubs.",
      "g 1 million random tweets) have been compiled where these sets belong to two different periods of 20 consecutive days BIBREF11 . We have decided to use one of these sets (corresponding to the period between August 18 and September 6, 2015) and firstly filtered the tweets using the possible names used to refer to the target clubs.",
      " of the stance was the management of the club while in some others a particular footballer of the club is praised or criticised. Still, we have considered the club as the target of the stance in all of the cases and carried out our annotations accordingly.",
      " a tweet data set in Turkish annotated with stance information, where the corresponding annotations are made publicly available. The domain of the tweets comprises two popular football clubs which constitute the targets of the tweets included. We also provide the evaluation",
      " Still, we have considered the club as the target of the stance in all of the cases and carried out our annotations accordingly. At the end of the annotation process, we have annotated 700 tweets, where 175 tweets are in favor of and 175 tweets are against Target-1, and similarly 175 tweets are in favor of and 175 are against Target-2.",
      "results\nfor the Against class. This outcome may be due to the common use of some terms when expressing positive stance towards sports clubs in Turkish tweets.",
      "tweets with the Neither class. This stance class and even finer-grained classes can be considered in further annotation studies. We should also note that in a few tweets, the target of the stance was the management of the club while in some others a particular footballer of the club is praised or criticised.",
      "o refer to the target clubs. Then, we have annotated the stance information in the tweets for these targets as Favor or Against. Within the course of this study, we have not considered those tweets in which the target is not explicitly mentioned, as our initial filtering process reveals. For the purposes of the current study, we have not annotated any tweets with the Neither class.",
      "iment analysis. It is the detection of stance within text towards a target which may be explicitly specified in the text or not. In this study, we present a stance-annotated tweet data set in Turkish where the targets of the annotated stances are two popular sports clubs in Turkey. The corresponding annotations are made publicly-available for research purposes.",
      " This outcome may be due to the common use of some terms when expressing positive stance towards sports clubs in Turkish tweets. The same percentage of common terms may not have been observed in tweets during the expression of negative stances towards the targets. Yet, completely the opposite pattern is observed in stance detection",
      "t instead of the sentiment of the text author, the stance expressed for a particular target is investigated in stance detection. In this paper, we present a stance detection tweet data set for Turkish comprising stance annotations of these tweets for two popular sports clubs as targets. Additionally, we provide the evaluation"
    ],
    "answer": [
      "Galatasaray and Fenerbahçe."
    ]
  },
  {
    "title": "Deep Learning the EEG Manifold for Phonological Categorization from Active Thoughts",
    "evidence": [
      "words(pat, pot, knew and gnaw). The dataset consists of 14 participants, with each prompt presented 11 times to each individual. Since our intention is to classify the phonological categories from human thoughts, we discard the facial and audio information and only consider the EEG data corresponding to imagined speech.",
      "new subject for testing, and given the high inter-subject variability of the EEG data, a reduction in the accuracy was expected. However, our network still managed to achieve an improvement of 18.91, 9.95, 67.15, 2.83 and 13.70 % over BIBREF17 .",
      "the number of electrodes. It is a major hurdle to optimally encode information from these EEG data into lower dimensional space. In fact, our investigation based on a development set (as we explain later) showed that well-known deep neural networks (e.g., fully connected networks such as convolutional neural networks, recurrent neural networks and autoencoders) fail to individually learn such complex feature representations from single-trial EEG data.",
      "rom human thoughts, we discard the facial and audio information and only consider the EEG data corresponding to imagined speech. It is noteworthy that given the mixed nature of EEG signals, it is reportedly challenging to attain a pairwise EEG-phoneme mapping BIBREF18 .",
      "al electroencephalography (EEG) data, we compute the joint variability of EEG electrodes into a channel cross-covariance matrix. We then extract the spatio-temporal information encoded within the matrix using a mixed deep neural network strategy. Our model framework is composed of a convolutional neural network (CNN), a long-short term network (LSTM), and a deep autoencoder.",
      "hallenging: these data are high dimensional, have poor SNR, and suffer from low spatial resolution and a multitude of artifacts. For these reasons, it is not particularly obvious how to decode the desired information from raw EEG signals.",
      "ultichannel EEG data is high dimensional multivariate time series data whose dimensionality depends on the number of electrodes. It is a major hurdle to optimally encode information from these EEG data into lower dimensional space.",
      " of the proposed approach with the baseline methods for PHASE-TWO of our study (cross-validation experiment) in Table TABREF15 . Since the model encounters the unseen data of a new subject for testing, and given the high inter-subject variability of the EEG data, a reduction in the accuracy was expected.",
      " we developed a novel mixed deep neural network scheme for a number of binary classification tasks from speech imagery EEG data. Unlike previous approaches which mostly deal with subject-dependent classification of EEG into discrete vowel or word labels, this work investigates a subject-invariant mapping of EEG data with different phonological categories, varying widely in terms of underlying articulator motions (eg: involvement or non-involvement of lips and velum, variation of tongue movements etc).",
      "acian filtering step since such high-pass filtering may decrease information content from the signals in the selected bandwidth. Joint variability of electrodes Multichannel EEG data is high dimensional multivariate time series data whose dimensionality depends on the number of electrodes.",
      "pendent classification of phonological categories exploiting a novel deep learning based hierarchical feature extraction scheme. To better capture the complex representation of high-dimensional electroencephalography (EEG) data, we compute the joint variability of EEG electrodes into a channel cross-covariance matrix.",
      "non-invasive, portable, low cost, and provides satisfactory temporal resolution. This makes EEG suitable to realize BCI systems. EEG data, however, is challenging: these data are high dimensional, have poor SNR, and suffer from low spatial resolution and a multitude of artifacts.",
      "nt neural networks and autoencoders) fail to individually learn such complex feature representations from single-trial EEG data. Besides, we found that instead of using the raw multi-channel high-dimensional EEG requiring large training times and resource requirements, it is advantageous to first reduce its dimensionality by capturing the information transfer among the electrodes."
    ],
    "answer": [
      "The EEG data comes from 14 subjects."
    ]
  },
  {
    "title": "Towards Understanding Neural Machine Translation with Word Importance",
    "evidence": [
      " model architectures, demonstrating its superiority on identifying input words with higher influence on translation performance. Encouragingly, the calculated importance can serve as indicators of input words that are under-translated by NMT models.",
      "several language pairs, and two representative model architectures, demonstrating its superiority on estimating word importance. We analyze the linguistic behaviors of words with the importance and show its potential to improve NMT models. First, we leverage the word importance to identify input words that are under-translated by NMT models. Experimental",
      "results\nshow that the gradient-based method is superior to several black-box methods in estimating the word importance. Further analyses show that important words are of distinct syntactic categories on different language pairs, which might support the viewpoint that essential inductive bias should be introduced into the model design BIBREF28.",
      " Specifically, we measure the word importance by attributing the NMT output to every input word through a gradient-based method. We validate the approach on a couple of perturbation operations, language pairs, and model architectures, demonstrating its superiority on identifying input words with higher influence on translation performance.",
      "results\n. All the following experiments are conducted on the test dataset, and we estimate the input word importance using the model generated hypotheses.",
      " estimating word importance. We find that word importance is useful for understanding NMT by identifying under-translated words. We provide empirical support for the design principle of NMT architectures: essential inductive bias (e.g., language characteristics) should be considered for model design.",
      "on of “word importance” is employed to quantify the contribution that a word in the input sentence makes to the NMT generations. We categorize the methods of word importance estimation into two types: black-box methods without the knowledge of the model and white-box methods that have access to the model internal information (e.g., parameters and gradients).",
      "anslation (NMT) has advanced the state-of-the-art on various language pairs, the interpretability of NMT remains unsatisfactory. In this work, we propose to address this gap by focusing on understanding the input-output behavior of NMT models. Specifically, we measure the word importance by attributing the NMT output to every input word through a gradient-based method.",
      "g experiments are conducted on the test dataset, and we estimate the input word importance using the model generated hypotheses. In the following experiments, we compare IG (Attribution) with several black-box methods (i.e., Content, Frequency, Attention) as introduced in Section SECREF8.",
      "rmance. Encouragingly, the calculated importance can serve as indicators of input words that are under-translated by NMT models. Furthermore, our analysis reveals that words of certain syntactic categories have higher importance while the categories vary across language pairs, which can inspire better design principles of NMT architectures for multi-lingual translation.",
      "on does not provide meaningful explanations since the relationship between attention scores and model output is unclear BIBREF8. In this paper, we focus on the second thread and try to open the black-box by exploiting the gradients in NMT generation, which aims to estimate the word importance better.",
      "stimate word importance, which consistently outperforms its attention counterpart across model architectures and language pairs. Related Work ::: Exploiting Gradients for Model Interpretation The intermediate gradients have proven to be useful in interpreting deep learning models, such as NLP models BIBREF14, BIBREF15 and computer vision models BIBREF16, BIBREF9.",
      "pes of synthetic perturbations (Section SECREF21), as well as different NMT architectures and language pairs (Section SECREF27). In addition, we compare with a supervised erasure method, which requires ground-truth translations for scoring word importance (Section SECREF30). Experiment :::"
    ],
    "answer": [
      "\"Transformer\" and \"Recurrent Neural Network (RNN)\""
    ]
  },
  {
    "title": "How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings",
    "evidence": [
      "ic embeddings for each word by taking the first principal component (PC) of its contextualized representations in a given layer. In Table TABREF34, we plot the performance of these PC static embeddings on several benchmark tasks.",
      "ipal component. It gives us an upper bound on how well a static embedding could replace a word's contextualized representations. The closer $\\textit {MEV}_\\ell (w)$ is to 0, the poorer a replacement a static embedding would be; if $\\textit {MEV}_\\ell (w) = 1$, then a static embedding would be a perfect replacement for the contextualized representations.",
      "oven that in theory, they both implicitly factorize a word-context matrix containing a co-occurrence statistic BIBREF7, BIBREF8. Because they create a single representation for each word, a notable problem with static word embeddings is that all senses of a polysemous word must share a single vector.",
      "y, on average, less than 5% of the variance in a word's contextualized representations could be explained by a static embedding. This means that even in the best-case scenario, in all layers of all models, static word embeddings would be a poor replacement for contextualized ones.",
      "variance across all words can be explained by a single vector than can the variance across all representations of a single word. Note that the 5% threshold represents the best-case scenario, and there is no theoretical guarantee that a word vector obtained using GloVe, for example, would be similar to the static embedding that maximizes MEV.",
      "for each word, a notable problem with static word embeddings is that all senses of a polysemous word must share a single vector. Related Work ::: Contextualized Word Representations Given the limitations of static word embeddings, recent work has tried to create context-sensitive word representations.",
      " of variance in $w$'s contextualized representations for a given layer that can be explained by their first principal component. It gives us an upper bound on how well a static embedding could replace a word's contextualized representations.",
      "ication of deep learning methods to NLP is made possible by representing words as vectors in a low-dimensional continuous space. Traditionally, these word embeddings were static: each word had a single vector, regardless of context BIBREF0, BIBREF1. This posed several problems, most notably that all senses of a polysemous word had to share the same representation.",
      "milarity can be traced back to differences in model architecture; we leave this question as future work. Findings ::: Static vs. Contextualized ::: On average, less than 5% of the variance in a word's contextualized representations can be explained by a static embedding.",
      "al component. This gives us an upper bound on how well a static embedding could replace a word's contextualized representations. Because contextualized representations are anisotropic (see section SECREF21), much of the variation across all words can be explained by a single vector.",
      "answer how contextual these representations are, and to what extent they can be replaced with static word embeddings, if at all. Our work in this paper is thus markedly different from most dissections of contextualized representations. It is more similar to BIBREF13, which studied the geometry of static word embedding spaces.",
      " representations, and even in the best possible scenario, static embeddings would be a poor replacement for contextualized ones. Still, static embeddings created by taking the first principal component of a word's contextualized representations outperform GloVe and FastText embeddings on many word vector benchmarks.",
      "r are static character-level embeddings. In all three models, the higher the layer, the lower the self-similarity is on average. In other words, the higher the layer, the more context-specific the contextualized representations. This finding makes intuitive sense."
    ],
    "answer": [
      "The static embedding for each word is calculated by taking the first principal component of its contextualized representations in a given layer."
    ]
  },
  {
    "title": "Paraphrase-Supervised Models of Compositionality",
    "evidence": [
      "ould be able to learn that translation decisions which translate it as a unit are to be favored, leading to better translations. To test this hypothesis, we built an English-Spanish MT system using the cdec decoder BIBREF27 for the entire training pipeline (word alignments, phrase extraction, feature weight tuning, and decoding).",
      "results\nfor compositional phrases by evaluating on a phrase similarity task (§ SECREF29 ); second, verify the hypothesis that compositionality is context-dependent by comparing a type-based and token-based approach on a compound noun evaluation task (§ SECREF36 ); and third, determine if the compositionality-scoring models based on learned representations improve the translations produced by a state-of-the-art phrase-based MT system (§ SECREF38 ).",
      "ces compositional functions with equivalent performance to previous approaches that have relied on hand-annotated training data. Furthermore, compositionality features consistently improve the translations produced by a strong English–Spanish translation system.",
      " more likely segmentations, as dictated by the compositionality scores of the phrases extracted from a sentence, to the decoder. A low compositionality score would ideally force the decoder to consider the entire phrase as a translation unit, due to its unique semantic characteristics.",
      "in the translation rule are unigrams), and correspondingly the second feature indicates if the translation rule has been scored. Therefore, an appropriate additional baseline would be to mark translation rules with these indicator functions but without the scores, akin to identifying rules with phrases in them (Baseline + SegOn). Table TABREF40 presents the",
      "he same language align to the same string in another language, then the strings in the original language share the same meaning. Paraphrases are ranked by their word alignment scores, and in this work we use the preselected small portion of PPDB as our training data.",
      "g under the hypothesis that sentences which express their meaning non-compositionally should also translate non-compositionally. Modern phrase-based translation systems are faced with a large number of possible segmentations of a source-language sentence during decoding, and all segmentations are considered equally likely BIBREF13 .",
      "n heuristics BIBREF29 , allowing the decoder to make the final decision on the impact of compositionality scores in translation. Thus, our work is more similar to Xiong2010, who propose maximum entropy classifiers that mark positions between words in a sentence as being a phrase boundary or not, and integrate these scores as additional features in an MT system.",
      "orrespondingly, a high score informs the decoder that it is safe to rely on word-level translations of the phrasal constituents. Thus, if we reveal to the translation system that a phrase is non-compositional, it should be able to learn that translation decisions which translate it as a unit are to be favored, leading to better translations.",
      "s based on learned representations improve the translations produced by a state-of-the-art phrase-based MT system (§ SECREF38 ). The word vectors used in all of our experiments were produced by word2vec using the skip-gram model with 20 negative samples, a context window size of 10, a minimum token count of 3, and sub-sampling of frequent words with a parameter of INLINEFORM0 .",
      "would ideally force the decoder to consider the entire phrase as a translation unit, due to its unique semantic characteristics. Correspondingly, a high score informs the decoder that it is safe to rely on word-level translations of the phrasal constituents.",
      "putationally expensive and statistically inefficient. In contrast, we obtain this information through many-to-one PPDB mappings. Most of these models also require additional syntactic BIBREF31 or semantic BIBREF15 , BIBREF14 resources; on the other hand, our proposed approach only requires a shallow syntactic parse (POS tags).",
      "istently better than the additive compositional model, indicating the benefit of learning the compositional parameters via PPDB. Machine Translation While any truly successful model of semantics must match human intuitions, understanding the applications of our models is likewise important."
    ],
    "answer": [
      "English-Spanish MT system and a state-of-the-art phrase-based MT system."
    ]
  },
  {
    "title": "Incorporating Subword Information into Matrix Factorization Word Embeddings",
    "evidence": [
      "it matrix factorization to learn subword and word representations, unlike these related models which mostly use neural networks. Finally, BIBREF26 and BIBREF27 retrofit morphological information onto pre-trained models. These differ from our work in that we incorporate morphological information at training time, and that only BIBREF26 is able to generate embeddings for OOV words.",
      "se morphological segmentation in learning word representations BIBREF20 , BIBREF21 , BIBREF22 , BIBREF23 , BIBREF24 , BIBREF25 . Our model also uses n-grams and morphological segmentation, but it performs explicit matrix factorization to learn subword and word representations, unlike these related models which mostly use neural networks.",
      "word vectors and resulting representation jointly as weighted factorization of a word-context co-occurrence matrix is performed. There are many models that use character-level subword information to form word representations BIBREF15 , BIBREF16 , BIBREF17 , BIBREF18 , BIBREF19 , as well as fastText (the model on which we base our work).",
      "main problems: 1) Learned information is not explicitly shared among the representations as each word has an independent vector. 2) There is no clear way to represent out-of-vocabulary (OOV) words.",
      " represented a word of as the sum of four-gram vectors obtained running an SVD of a four-gram to four-gram co-occurrence matrix. Our model differs by learning the subword vectors and resulting representation jointly as weighted factorization of a word-context co-occurrence matrix is performed.",
      "currence matrix using stochastic gradient descent. $$PPMI_{wc} = max(0, \\log \\frac{M_{wc} \\; M_{**}}{ M_{w*} \\; M_{*c} })$$ (Eq. 3) where $M$ is the word-context co-occurrence matrix constructed by sliding a window of fixed size centered over every target word $w$ in the subsampled BIBREF2 training corpus and incrementing cell $M_{wc}$ for every context word $c$ appearing within this window (forming a $(w,c)$ pair).",
      ", $\\textnormal {initial learning rate} = .025$ , $\\textnormal {subsampling} = 10^{-4}$ , $\\textnormal {negative samples} = 5$ ). All five models are run for 5 iterations over the training corpus and generate 300 dimensional word representations. LV-N, LV-M, and FT use 2000000 buckets when hashing subwords.",
      "name-like neighbors, although none is the expected correct spelling “louisiana”. All models stumble on the made-up prefix “tuz”. A possible fix would be to down-weigh very rare subwords in the vector summation. LV-M is less robust than LV-N and FT on this task as it is highly sensitive to incorrect segmentation, exemplified in the “hellooo” example.",
      "training corpus and generate 300 dimensional word representations. LV-N, LV-M, and FT use 2000000 buckets when hashing subwords. For word similarity evaluations, we use the WordSim-353 Similarity (WS-Sim) and Relatedness (WS-Rel) BIBREF28 and SimLex-999 (SimLex) BIBREF29 datasets, and the Rare Word (RW) BIBREF20 dataset to verify if subword information improves rare word representation.",
      "beddings are usually derived by employing the distributional hypothesis: that similar words appear in similar contexts BIBREF0 . The models that perform the word embedding can be divided into two classes: predictive, which learn a target or context word distribution, and counting, which use a raw, weighted, or factored word-context co-occurrence matrix BIBREF1 .",
      "results\n.",
      " we incorporate morphological information at training time, and that only BIBREF26 is able to generate embeddings for OOV words. Subword LexVec The LexVec BIBREF7 model factorizes the PPMI-weighted word-context co-occurrence matrix using stochastic gradient descent. $$PPMI_{wc} = max(0, \\log \\frac{M_{wc} \\; M_{**}}{ M_{w*} \\; M_{*c} })$$ (Eq.",
      "t or context word distribution, and counting, which use a raw, weighted, or factored word-context co-occurrence matrix BIBREF1 . The most well-known predictive model, which has become eponymous with word embedding, is word2vec BIBREF2 . Popular counting models include PPMI-SVD BIBREF3 , GloVe BIBREF4 , and LexVec BIBREF5 ."
    ],
    "answer": [
      "The matrix factorization methods used are SVD (singular value decomposition) and stochastic gradient descent for factorizing the PPMI-weighted word-context co-occurrence matrix."
    ]
  },
  {
    "title": "Abstractive Summarization for Low Resource Data using Domain Transfer and Data Synthesis",
    "evidence": [
      "Abstract\nive Summarization.",
      "abstract\nive summarization models typically requires large amounts of data, which can be a limitation for many domains. In this paper we explore using domain transfer and data synthesis to improve the performance of recent",
      "abstract\nive summarization using the proposed model compared to a synthesis baseline. Lastly, we combine both directions. Evaluations of neural",
      "abstract\nive summarization method across four student reflection corpora show the utility of all three methods. Related Work",
      "abstract\nive summarization methods when applied to small corpora of student reflections. First, we explored whether tuning state of the art model trained on newspaper data could boost performance on student reflection data.",
      "abstract\nive summarizers when applied to the low resource domain of student reflections using three approaches: domain transfer, data synthesis and the combination of both. For domain transfer, state of the art",
      "abstract\nive summarization methods have seen great performance strides BIBREF0, BIBREF1, BIBREF2. However, complex neural summarization models with thousands of parameters usually require a large amount of training data.",
      "abstract\nive summarization models in low resource domains using domain transfer has not been thoroughly explored on domains different than news. For example, BIBREF4 reported the",
      "abstract\nive summarization model: pointer networks with coverage mechanism (PG-net)BIBREF0.",
      "Abstract\nive summarization aims to generate coherent summaries with high readability, and has seen increasing interest and improved performance due to the emergence of seq2seq models BIBREF8 and attention mechanisms BIBREF9.",
      " adapted for summarization itself. Because the modifications introduce few parameters, the model is suitable for small datasets. Recall that for data synthesis, the input to the template method is a summary. Since for summarization the input instead is a set of reflections, we perform keyword extraction over the set of reflections.",
      "del for synthesizing new data, then investigate the performance impact of using this data when training the summarization model. The proposed model makes use of the nature of datasets such as ours, where the reference summaries tend to be close in structure: humans try to find the major points that students raise, then present the points in a way that marks their relative importance (recall the CS example in Table TABREF4).",
      "abstract\nive summarization model was pretrained using out-of-domain data (CNN/DM), then tuned using in-domain data (student reflections)."
    ],
    "answer": [
      "The recent abstractive summarization method used in this paper is pointer networks with coverage mechanism (PG-net)."
    ]
  },
  {
    "title": "Revisiting Summarization Evaluation for Scientific Articles",
    "evidence": [
      "tific summarization; we show that, contrary to the common belief, ROUGE is not much reliable in evaluating scientific summaries. We furthermore show how different variants of ROUGE result in very different correlations with the manual Pyramid scores.",
      "ive. Scientific article summarization is one such case that is different from general domain summarization (e.g. newswire data). We provide an extensive analysis of ROUGE's effectiveness as an evaluation metric for scientific summarization; we show that, contrary to the common belief, ROUGE is not much reliable in evaluating scientific summaries.",
      "Results\nreveal that, contrary to the common belief, correlations between Rouge and the Pyramid scores are weak, which challenges its effectiveness for scientific summarization.",
      "results\nconfirm our initial hypothesis that Rouge is not accurate estimator of the quality of the summary in scientific summarization. We attribute this to the differences of scientific summarization with general domain summaries.",
      "e terms and phrases in the sentences; therefore, in cases of terminology variations and paraphrasing, ROUGE is not as effective. Scientific article summarization is one such case that is different from general domain summarization (e.g. newswire data).",
      "of newswire data and are intrinsically very different than other summarization tasks such as summarization of scientific papers. We argue that Rouge is not the best metric for all summarization tasks and we propose an alternative metric for evaluation of scientific summarization.",
      "ef, correlations between Rouge and the Pyramid scores are weak, which challenges its effectiveness for scientific summarization. Furthermore, we show a large variance of correlations between different Rouge variants and the manual evaluations which further makes the reliability of Rouge for evaluating scientific summaries less clear.",
      "not the best metric for all summarization tasks and we propose an alternative metric for evaluation of scientific summarization. The proposed alternative metric shows much higher and more consistent correlations with manual judgments in comparison with the well-established Rouge.",
      "aries. We furthermore show how different variants of ROUGE result in very different correlations with the manual Pyramid scores. Finally, we propose an alternative metric for summarization evaluation which is based on the content relevance between a system generated summary and the corresponding human written summaries.",
      "domain summaries. When humans summarize a relatively long research paper, they might use different terminology and paraphrasing. Therefore, Rouge which only relies on term matching between a candidate and a gold summary, is not accurate in quantifying the quality of the candidate summary.",
      "on literature, and its high adoption has been due to its high correlation with human assessment scores in DUC datasets BIBREF1 . However, later research has casted doubts about the accuracy of Rouge against manual evaluations.",
      "important to study the validity of existing methodologies applied to the evaluation of news article summarization for this task. In particular, we raise the important question of how effective is Rouge, as an evaluation metric for scientific summarization?",
      " In particular, we raise the important question of how effective is Rouge, as an evaluation metric for scientific summarization? We answer this question by comparing Rouge scores with semi-manual evaluation score (Pyramid) in TAC 2014 scientific summarization dataset."
    ],
    "answer": [
      "The common belief that this paper refutes is that ROUGE is a reliable evaluation metric for scientific summarization."
    ]
  },
  {
    "title": "Edinburgh Neural Machine Translation Systems for WMT 16",
    "evidence": [
      "f the monolingual News corpus as additional training data BIBREF2 , pervasive dropout BIBREF3 , and target-bidirectional models. Baseline System Our systems are attentional encoder-decoder networks BIBREF0 . We base our implementation on the dl4mt-tutorial, which we enhanced with new features such as ensemble decoding and pervasive dropout.",
      "models. All reported methods give substantial improvements, and we see improvements of 4.3--11.2 BLEU over our baseline systems. In the human evaluation, our systems were the (tied) best constrained system for 7 out of 8 translation directions in which we participated.",
      "stem outperformed the ensemble of the last 4 checkpoints on dev, and we thus submitted the best single system as primary system. English↔\\leftrightarrow Russian For English INLINEFORM0 Russian, we cannot effectively learn BPE on the joint vocabulary because alphabets differ.",
      ".7 Bleu from training with a mix of parallel and synthetic data, compared to the baseline that is only trained on parallel data. Using an ensemble of the last 4 checkpoints gives further improvements (1.3–1.7 Bleu).",
      "so for Romanian INLINEFORM0 English we removed diacritics from the Romanian source side, obtaining improvements of 1.3–1.4 Bleu. Synthetic training data gives improvements of 4.1–5.1 Bleu. for English INLINEFORM0 Romanian, we found that the best single system outperformed the ensemble of the last 4 checkpoints on dev, and we thus submitted the best single system as primary system.",
      "ne that is only trained on parallel data. Using an ensemble of the last 4 checkpoints gives further improvements (1.3–1.7 Bleu). Our submitted system includes reranking of the 50-best output of the left-to-right model with a right-to-left model – again an ensemble of the last 4 checkpoints – with uniform weights. This yields an improvements of 0.6–1.1 Bleu.",
      "results\nfor English INLINEFORM0 German. We observe improvements of 3.4–5.7 Bleu from training with a mix of parallel and synthetic data, compared to the baseline that is only trained on parallel data.",
      "rst for 5 out of 8 translation directions in which we participated: EN INLINEFORM0 CS, EN INLINEFORM1 DE, and EN INLINEFORM2 RO. They are also the (tied) best constrained system for EN INLINEFORM3 RU and RO INLINEFORM4 EN, or 7 out of 8 translation directions in total.",
      "ack-translations of the monolingual News corpus as additional training data, pervasive dropout, and target-bidirectional models. All reported methods give substantial improvements, and we see improvements of 4.3--11.2 BLEU over our baseline systems.",
      "2.8 Bleu, and that the final system (+r2l reranking) improves by 0.7–1.0 Bleu on the ensemble of 4, and 4.3–4.9 on the baseline. For Czech INLINEFORM0 English the training process was similar to the above, except that we created the synthetic training data (back-translated from samples of news2015 monolingual English) in batches of 2.5M, and so were able to observe the effect of increasing the amount of synthetic data.",
      "news2015. The back-translations were, as for English INLINEFORM1 Czech, created with an earlier NMT model trained on WMT15 data. Our final Czech INLINEFORM2 English was an ensemble of 8 systems – the last 4 save-points of the 10M synthetic data run, and the last 4 save-points of the 7.5M run. We show this as ensemble8 in Table TABREF15 , and the +synthetic",
      "uage pairs: English INLINEFORM0 Czech, English INLINEFORM1 German, English INLINEFORM2 Romanian and English INLINEFORM3 Russian. Our systems are based on an attentional encoder-decoder BIBREF0 , using BPE subword segmentation for open-vocabulary translation with a fixed vocabulary BIBREF1 .",
      "o the (tied) best constrained system for EN INLINEFORM3 RU and RO INLINEFORM4 EN, or 7 out of 8 translation directions in total. Our models are also used in QT21-HimL-SysComb BIBREF10 , ranked 1–2 for EN INLINEFORM0 RO, and in AMU-UEDIN BIBREF11 , ranked 2–3 for EN INLINEFORM1 RU, and 1–2 for RU INLINEFORM2 EN."
    ],
    "answer": [
      "Our baseline systems are attentional encoder-decoder networks based on the dl4mt-tutorial, which we enhanced with new features such as ensemble decoding and pervasive dropout."
    ]
  },
  {
    "title": "Embracing data abundance: BookTest Dataset for Reading Comprehension",
    "evidence": [
      "system is still not achieving its full potential so we decided to examine the room for improvement in our own small human study. Human Study After adding more data we have the performance on the CBT validation and test datasets soaring. However is there still potential for much further growth beyond the",
      " room for improvement on the algorithmic side. The other possibility to improve performance is simply to use more training data. The importance of training data was highlighted by the frequently quoted Mercer's statement that “There is no data like more data.” The observation that having more data is often more important than having better algorithms has been frequently stressed since then BIBREF13 , BIBREF14 .",
      "rized in Table TABREF28 . They show that a majority of questions that our system could not answer so far are in fact answerable. This suggests that 1) the original human baselines might have been underestimated, however, it might also be the case that there are some examples that can be answered by machines and not by humans; 2) there is still space for improvement.",
      "e case that there are some examples that can be answered by machines and not by humans; 2) there is still space for improvement. A system that would answer correctly every time when either our ensemble or human answered correctly would achieve accuracy over 92% percent on both validation and test NE datasets and over 96% on both CN datasets.",
      "results\nwe have observed? We decided to explore the remaining space for improvement on the CBT by testing humans on a random subset of 50 named entity and 50 common noun validation questions that the psr ensemble could not answer correctly.",
      " is a practically unlimited amount of data available in this field, most research was performed on unnecessarily small datasets. As a gentle reminder to the community we have shown that simply infusing a model with more data can yield performance improvements of up to INLINEFORM0 where several attempts to improve the model architecture on the same training data have given gains of at most INLINEFORM1 compared to our best ensemble result.",
      "ed-entity version of CBT this brings the ensemble of our models to the level of human baseline as reported by Facebook BIBREF2 . However in the final section we show in our own human study that there is still room for improvement on the CBT beyond the performance of our model.",
      "Conclusion\nFew ways of improving model performance are as solidly established as using more training data. Yet we believe this principle has been somewhat neglected by recent research in text comprehension.",
      " create our data can later be used to create even larger datasets when the need arises thanks to further technological progress. We show that if we evaluate a model trained on the new dataset on the now standard Children's Book Test dataset, we see an improvement in accuracy much larger than other research groups achieved by enhancing the model architecture itself (while still using the original CBT training data).",
      "on-level system actually capable of helping humans, we want the model to use all available resources as efficiently as possible. Given that we believe that if the community is striving to bring the performance as far as possible, it should move its work to larger data. This thinking goes in line with recent developments in the area of language modelling.",
      "rding to validation performance. Then in each step we tried adding the best performing model that had not been previously tried. We kept it in the ensemble if it did improve its validation performance and discarded it otherwise. This way we gradually tried each model once. We call the resulting model a greedy ensemble.",
      " Reader model on the original CBT test data by a much larger margin than many recent attempts to improve the model architecture. On one version of the dataset our ensemble even exceeds the human baseline provided by Facebook. We then show in our own human study that there is still space for further improvement.",
      "ing data significantly help the model performance, focusing on speeding up the algorithm may be more important than ever before. This may for instance influence the decision whether to use regularization such as dropout which does seem to somewhat improve the model performance, however usually at a cost of slowing down training."
    ],
    "answer": [
      "There is still room for improvement on the CBT beyond the performance of our model."
    ]
  },
  {
    "title": "A Human-AI Loop Approach for Joint Keyword Discovery and Expectation Estimation in Micropost Event Detection",
    "evidence": [
      "keyword-specific expectations, and 2) the disagreement between the model and the crowd for discovering new informative keywords. More specifically, at each iteration after we obtain a keyword-specific expectation from the crowd, we train the model using expectation regularization and select those keyword-related microposts for which the model's prediction disagrees the most with the crowd's expectation; such microposts are then presented to the crowd to identify new keywords that best explain the disagreement.",
      "ve issues, we advocate a human-AI loop approach for discovering informative keywords and estimating their expectations reliably. Our approach iteratively leverages 1) crowd workers for estimating keyword-specific expectations, and 2) the disagreement between the model and the crowd for discovering new informative keywords.",
      "rowd's expectation; such microposts are then presented to the crowd to identify new keywords that best explain the disagreement. By doing so, our approach identifies new keywords which convey more relevant information with respect to existing ones, thus effectively boosting model performance.",
      "nd the model to discover informative keywords and leverages the joint power of the crowd and the model in expectation inference. We evaluated our approach on real-world datasets and showed that it significantly outperforms the state of the art and that it is particularly useful for detecting events where relevant microposts are semantically complex, e.g., the death of a politician.",
      " the disagreement between the crowd and the model in order to discover new keywords that are most beneficial for model training. These keywords and their expectation not only improve the resulting performance but also make the model training process more transparent.",
      " presented a new human-AI loop approach for keyword discovery and expectation estimation to better train event detection models. Our approach takes advantage of the disagreement between the crowd and the model to discover informative keywords and leverages the joint power of the crowd and the model in expectation inference.",
      "BREF10 or to debug the training data BIBREF30, BIBREF31 or components BIBREF32, BIBREF33, BIBREF34 of a machine learning system. Unlike these works, we leverage crowd workers to label sampled microposts in order to obtain keyword-specific expectations, which can then be generalized to help classify microposts containing the same keyword, thus amplifying the utility of the crowd.",
      "roduces a Human-AI loop approach to jointly discover informative keywords for model training while estimating their expectation. Our approach iteratively leverages the crowd to estimate both keyword specific expectation and the disagreement between the crowd and the model in order to discover new keywords that are most beneficial for model training.",
      "et model $\\theta ^{(t)}$. The E-step and the crowd parameter update in the M-step are similar to the Dawid-Skene model BIBREF14. The keyword expectation is inferred by taking into account both the crowd-contributed labels and the model prediction: The parameter of the target model is updated by gradient descent.",
      "(k)})$ and $Ber(\\mathbb {E}_{x\\sim \\mathcal {U}^{(k)}}(y))$, and $\\lambda $ controls the strength of expectation regularization. Unified Probabilistic Model ::: Expectation as Class Prior To learn the keyword-specific expectation $e^{(t)}$ and the crowd worker reliability $\\pi ^{(n)}$ ($1\\le n\\le N$), we model the likelihood of the crowd-contributed labels $\\mathbf {A}$ as a function of these parameters.",
      "$\\pi ^{(n)}$ ($1\\le n\\le N$), the keyword-specific expectation $e^{(t)}$, and the parameter of the target model $\\theta ^{(t)}$. The E-step and the crowd parameter update in the M-step are similar to the Dawid-Skene model BIBREF14.",
      "h the EM algorithm. For keyword discovery, we filter keywords based on the frequency of the keyword being selected by the crowd. In terms of cost-effectiveness, our approach is motivated from the fact that crowdsourced data annotation can be expensive, and is thus designed with minimal crowd involvement.",
      " to infer the keyword-specific expectation $e^{(t)}$ and train the target model by learning the model parameter $\\theta ^{(t)}$. An additional parameter of our probabilistic model is the reliability of crowd workers, which is essential when involving crowdsourcing."
    ],
    "answer": [
      "The keyword-specific expectation is elicited from the crowd through a human-AI loop approach that iteratively leverages the crowd for estimating keyword-specific expectations and the disagreement between the model and the crowd for discovering new informative keywords."
    ]
  },
  {
    "title": "ARAML: A Stable Adversarial Training Framework for Text Generation",
    "evidence": [
      "e original paper. In terms of the differences, the discriminators of GAN baselines are implemented based on the original papers. Other hyper-parameters of baselines including batch size, learning rate, and pre-training epochs, were set based on the original codes, because the convergence of baselines is sensitive to these hyper-parameters.",
      "e that the generator of LeakGAN consists of a hierarchical LSTM unit, thus we followed the implementation in the original paper. In terms of the differences, the discriminators of GAN baselines are implemented based on the original papers.",
      "pre-trained on the training set of each dataset. The codes and the datasets are available at https://github.com/kepei1106/ARAML. As for the details of the baselines, the generators of all the baselines except LeakGAN are the same as ours. Note that the generator of LeakGAN consists of a hierarchical LSTM unit, thus we followed the implementation in the original paper.",
      " conducted experiments on COCO many times and chose the best 5 trials for SeqGAN, LeakGAN, IRL, MaliGAN and ARAML, respectively. Then, we presented the forward/reverse perplexity in the training process in Figure FIGREF38 . We can see that our model with smaller standard deviation is more stable than other GAN baselines in both metrics.",
      "re FIGREF38 . We can see that our model with smaller standard deviation is more stable than other GAN baselines in both metrics. Although LeakGAN reaches the best forward perplexity, its standard deviation is extremely large and it performs badly in reverse perplexity, indicating that it generates limited expressions that are grammatical yet divergent from the data distribution.",
      "Results\nare shown in Table TABREF33 . LeakGAN performs best on forward perplexity because it can generate more fluent samples. As for reverse perplexity, our model ARAML beats other baselines, showing that our model can fit the data distribution better.",
      ". We simply removed post-response pairs containing low-frequency words and randomly selected a subset for our training/test set. The statistics of three datasets are presented in Table TABREF28 . Baselines We compared our model with MLE, RL and GAN baselines.",
      ". As for reverse perplexity, our model ARAML beats other baselines, showing that our model can fit the data distribution better. Other GANs, particularly LeakGAN, obtain high reverse perplexity due to mode collapse BIBREF12 , thus they only capture limited fluent expressions, resulting in large discrepancy between the generated distribution and data distribution.",
      "n't affect the training stability of our framework. The standard deviation of ARAML-R is still smaller than other GAN baselines. Case Study Table TABREF47 presents the examples generated by the models on COCO. We can find that other baselines suffer from grammatical errors (e.g. “in front of flying her kite\" from MLE), repetitive expressions (e.g.",
      "e statistics of three datasets are presented in Table TABREF28 . Baselines We compared our model with MLE, RL and GAN baselines. Since COCO and EMNLP2017 WMT don't have input while WeiboDial regards posts as input, we chose the following baselines respectively: MLE: a RNN model trained with MLE objective BIBREF4 . Its extension, Seq2Seq, can work on the dialogue dataset BIBREF2 .",
      "n COCO and EMNLP2017 WMT, while MLE, RAML, DialogGAN, and DPGAN on WeiboDial. The original codes are used to test the baselines. Implementation Details The implementation details of our model are shown in Table TABREF31 . For COCO / EMNLP2017, the generator is a LSTM unit BIBREF30 with 128 cells, and the discriminator is implemented based on BIBREF7 .",
      " that other baselines suffer from grammatical errors (e.g. “in front of flying her kite\" from MLE), repetitive expressions (e.g. “A group of people\" from IRL) and incoherent statements (e.g. “A group of people sitting on a cell phone” from IRL). By contrast, our model performs well in these sentences and has the ability to generate grammatical and coherent",
      "results\n. Despite the worse performance on automatic metrics, random sampling doesn't affect the training stability of our framework. The standard deviation of ARAML-R is still smaller than other GAN baselines."
    ],
    "answer": [
      "SeqGAN, LeakGAN, IRL, MaliGAN, DPGAN, DialogGAN, MLE, RL."
    ]
  },
  {
    "title": "Predicting the Industry of Users on Social Media",
    "evidence": [
      "Conclusion\nIn this paper, we examined the task of predicting a social media user's industry.",
      " public, researchers have been leveraging the newly available data to create and refine models of users' behavior and profiling. There exists a myriad research that analyzes language in order to profile social media users.",
      "uenced the expressed emotions BIBREF11 , studied mental disorders BIBREF12 , and the progression of health conditions BIBREF13 . At the same time, a number of researchers sought to predict the social media users' age and/or gender BIBREF14 , BIBREF15 , BIBREF16 , while others targeted and analyzed the ethnicity, nationality, and race of the users BIBREF17 , BIBREF18 , BIBREF19 .",
      "been verified by the Sina Weibo platform, introducing a potential bias in the resulting dataset. Finally, Preotiuc-Pietro et al. Preoctiuc15 predicted the occupational class of Twitter users using the Standard Occupational Classification (SOC) system, which groups the different jobs based on skill requirements.",
      "3 . To uncloak this information, a number of studies have utilized social media users' footprints to approximate their profiles. This paper explores the potential of predicting a user's industry –the aggregate of enterprises in a particular field– by identifying industry indicative text in social media.",
      "BIBREF16 , while others targeted and analyzed the ethnicity, nationality, and race of the users BIBREF17 , BIBREF18 , BIBREF19 . One of the profile fields that has drawn a great deal of attention is the location of a user. Among others, Hecht et al. Hecht11 predicted Twitter users' locations using machine learning on nationwide and state levels. Later, Han et al.",
      "Hecht et al. Hecht11 predicted Twitter users' locations using machine learning on nationwide and state levels. Later, Han et al. Han14 identified location indicative words to predict the location of Twitter users down to the city level. As a separate line of research, a number of studies have focused on discovering the political orientation of users BIBREF15 , BIBREF20 , BIBREF21 .",
      "of research, a number of studies have focused on discovering the political orientation of users BIBREF15 , BIBREF20 , BIBREF21 . Finally, Li et al. Li14a proposed a way to model major life events such as getting married, moving to a new place, or graduating.",
      "tating further developments of applications in advertising BIBREF0 , personalization BIBREF1 , and recommender systems BIBREF2 . However, profile information can be limited, depending on the platform, or it is often deliberately omitted BIBREF3 . To uncloak this information, a number of studies have utilized social media users' footprints to approximate their profiles.",
      " inside a user self-description, only a very small fraction of the Twitter users could be included (in their case, 5,191 users). Both of these recent studies are based on micro-blogging platforms, which inherently restrict the number of characters that a post can have, and consequently the way that users can express themselves.",
      " of users' behavior and profiling. There exists a myriad research that analyzes language in order to profile social media users. Some studies sought to characterize users' personality BIBREF9 , BIBREF10 , while others sequenced the expressed emotions BIBREF11 , studied mental disorders BIBREF12 , and the progression of health conditions BIBREF13 .",
      "Weibo users to twelve different platform-defined occupational classes highlighting the effect of homophily in user interactions. This work examined only users that have been verified by the Sina Weibo platform, introducing a potential bias in the resulting dataset. Finally, Preotiuc-Pietro et al.",
      "Abstract\nAutomatic profiling of social media users is an important task for supporting a multitude of downstream applications."
    ],
    "answer": [
      "Twitter"
    ]
  },
  {
    "title": "Open Event Extraction from Online Text using a Generative Adversarial Network",
    "evidence": [
      "e made some progress. However, these approaches typically assume that all words in a document are generated from a single event. While this may be true for short text such as tweets, such an assumption does not generally hold for long text such as news articles.",
      "Abstract\nTo extract the structured representations of open-domain events, Bayesian graphical models have made some progress. However, these approaches typically assume that all words in a document are generated from a single event.",
      "proposed the Dirichlet Process Event Mixture Model (DPEMM) in which the number of events can be learned automatically from data. However, both LEM and DPEMM have two limitations: (1) they assume that all words in a document are generated from a single event which can be represented by a quadruple INLINEFORM0 entity, location, keyword, date INLINEFORM1 .",
      "EM performs significantly better than LEM and DPEMM. It improves upon LEM by 15.5% and upon DPEMM by more than 30% in F-measure. This is because: (1) the assumption made by LEM and DPEMM that all words in a document are generated from a single event is not suitable for long text such as news articles; (2) DPEMM generates too many irrelevant events which leads to a very low precision score.",
      " this may be true for short text such as tweets, such an assumption does not generally hold for long text such as news articles. Moreover, Bayesian graphical models often rely on Gibbs sampling for parameter inference which may take long time to converge.",
      " generated from a single event which can be represented by a quadruple INLINEFORM0 entity, location, keyword, date INLINEFORM1 . However, long texts such news articles often describe multiple events which clearly violates this assumption; (2) During the inference process of both approaches, the Gibbs sampler needs to compute the conditional posterior distribution and assigns an event for each document.",
      "a discriminator network to discriminate between the reconstructed documents from latent events and the original input documents. This essentially helps the generator to construct a more realistic document from a random noise drawn from a Dirichlet distribution. Due to the flexibility of neural networks, the generator is capable of learning complicated nonlinear distributions.",
      "pared to existing Bayesian graphical modeling approaches. In future work, we will explore incorporating external knowledge (e.g. word relatedness contained in word embeddings) into the learning framework for event extraction.",
      "proaches tackles open-domain event extraction from online texts. We propose a novel GAN-based event extraction model called AEM. Compared with the previous models, AEM has the following differences: (1) Unlike most GAN-based text generation approaches, a generator network is employed in AEM to learn the projection function between an event distribution and the event-related word distributions (entity, location, keyword, date).",
      "tations, we propose an event extraction model based on Generative Adversarial Nets, called Adversarial-neural Event Model (AEM). AEM models an event with a Dirichlet prior and uses a generator network to capture the patterns underlying latent events. A discriminator is used to distinguish documents reconstructed from the latent events and the original documents.",
      "he projection function between an event distribution and the event-related word distributions (entity, location, keyword, date). The learned generator captures event-related patterns rather than generating text sequence; (2) Different from LEM and DPEMM, AEM uses a generator network to capture the event-related patterns and is able to mine events from different text sources (short and long).",
      "act structured representations of events such as who did what, when, where and why, Bayesian approaches have made some progress. Assuming that each document is assigned to a single event, which is modeled as a joint distribution over the named entities, the date and the location of the event, and the event-related keywords, Zhou et al.",
      "th approaches, the Gibbs sampler needs to compute the conditional posterior distribution and assigns an event for each document. This is time consuming and takes long time to converge. To deal with these limitations, in this paper, we propose the Adversarial-neural Event Model (AEM) based on adversarial training for open-domain event extraction."
    ],
    "answer": [
      "The proposed Adversarial-neural Event Model (AEM) overcomes the assumption that all words in a document are generated from a single event by using a generator network to capture event-related patterns and a discriminator to distinguish between reconstructed documents and original documents."
    ]
  },
  {
    "title": "Copenhagen at CoNLL--SIGMORPHON 2018: Multilingual Inflection in Context with Explicit Morphosyntactic Decoding",
    "evidence": [
      "liary task of MSD prediction, which acts as a regulariser; (3) a multilingual approach, exploiting information across languages. In future work we aim to gain better understanding of the increase in variance of the",
      "MSD prediction Figure FIGREF32 summarises the average MSD-prediction accuracy for the multi-tasking experiments discussed above. Accuracy here is generally higher than on the main task, with the multilingual finetuned setup for Spanish and the monolingual setup for French scoring best: 66.59% and 65.35%, respectively.",
      "syntactic awareness of the encoder and to regularise the learning process—the task is to predict the MSD tag of the target form. MSD tag predictions are conditioned on the context encoding, as described in UID15 . Tags are generated with an LSTM one component at a time, e.g. the tag PRO;NOM;SG;1 is predicted as a sequence of four components, INLINEFORM0 PRO, NOM, SG, 1 INLINEFORM1 .",
      "results\nwe obtain also for Track 2. Adding the auxiliary objective of MSD prediction has a variable effect: for four languages (de, en, es, and sv) the effect is positive, while for the rest it is negative.",
      "e exception to this pattern is fr where inflection gains a lot from multilingual training, while MSD prediction suffers greatly. Notice that the magnitude of change is not always the same, however, even when the general direction matches: for ru, for example, multilingual training benefits inflection much more than in benefits MSD prediction, even though the MSD decoder is the only component that is actually shared between languages.",
      "tive of MSD prediction; and (3) We train the auxiliary component in a multilingual fashion, over sets of two to three languages. In analysing the performance of our system, we found that encoding the full context improves performance considerably for all languages: 11.15 percentage points on average, although it also highly increases the variance in",
      "MSD tags (in Track 1) are embedded in their respective high-dimensional spaces as before, and their embeddings are concatenated. However, we now reduce the entire past context to a fixed-size vector by encoding it with a forward LSTM, and we similarly represent the future context by encoding it with a backwards LSTM.",
      "re than in benefits MSD prediction, even though the MSD decoder is the only component that is actually shared between languages. This observation illustrates the two-fold effect of multi-task training: an auxiliary task can either inform the main task through the parameters the two tasks share, or it can help the main task learning through its regularising effect.",
      "e vector by encoding it with a forward LSTM, and we similarly represent the future context by encoding it with a backwards LSTM. We introduce an auxiliary objective that is meant to increase the morpho-syntactic awareness of the encoder and to regularise the learning process—the task is to predict the MSD tag of the target form.",
      "nt at a time, e.g. the tag PRO;NOM;SG;1 is predicted as a sequence of four components, INLINEFORM0 PRO, NOM, SG, 1 INLINEFORM1 . For every training instance, we backpropagate the sum of the main loss and the auxiliary loss without any weighting. As MSD tags are only available in Track 1, this augmentation only applies to this track.",
      "ystem did not learn to copy the characters of lemma into inflected form, which is all it needs to do in a large number of cases. This issue could be alleviated with simple data augmentation techniques that encourage autoencoding BIBREF2 . MSD prediction Figure FIGREF32 summarises the average MSD-prediction accuracy for the multi-tasking experiments discussed above.",
      "phological Reinflection, Task 2, which achieved the best performance out of all systems submitted, an overall accuracy of 49.87. We showed in an ablation study that this is due to three core innovations, which extend a character-based encoder-decoder model: (1) a wide context window, encoding the entire available context; (2) multi-task learning with the auxiliary task of MSD prediction, which acts as a regulariser; (3) a multilingual approach, exploiting information across languages.",
      "BREF1 ), which receives information about context through an embedding of the two words immediately adjacent to the target form. We use this baseline implementation as a starting point and achieve the best overall accuracy of 49.87 on Task 2 by introducing three augmentations to the provided baseline system: (1) We use an LSTM to encode the entire available context; (2) We employ a multi-task learning approach with the auxiliary objective of MSD prediction; and (3) We train the auxiliary component in a multilingual fashion, over sets of two to three languages."
    ],
    "answer": [
      "The task of MSD prediction is to predict the Morpho-Syntactic Description (MSD) tag of the target form, conditioned on the context encoding, and is used as a regulariser to increase the morpho-syntactic awareness of the encoder."
    ]
  },
  {
    "title": "Exploiting Task-Oriented Resources to Learn Word Embeddings for Clinical Abbreviation Expansion",
    "evidence": [
      "ifferent data sets which are not publicly available. Alternatively, we use the following baselines to compare with our approach. Rating: This baseline model chooses the highest rating candidate expansion in the domain specific knowledge base.",
      ", it's not ranked as the top one due to the lower semantic similarity because there are not enough samples in the training data. Among all the incorrect expansions in our test set, such kind of errors accounted for about 54%. One possible solution may be adding more effective data to the embedding training, which means discovering more task-oriented resources.",
      ", a well-trained domain expert, and thus we use these 100 physician logs as the test set to evaluate our approach's performance. Baseline Models For our task, it's difficult to re-implement the supervised methods as in previous works mentioned since we do not have sufficient training data.",
      "h papers and 2 critical care medicine textbooks, besides our raw ICU data. We use word2vec BIBREF0 to train the word embeddings. The dimension of embeddings is empirically set to 100. Since the goal of our task is to find the correct expansion for an abbreviation, we use accuracy as a metric to evaluate the performance of our approach.",
      " our approach. Rating: This baseline model chooses the highest rating candidate expansion in the domain specific knowledge base. Raw Input embeddings: We trained word embeddings only from the 1,160 raw ICU texts and we choose the most semantically related candidate as the answer.",
      "pressions and then try to find all possible expansions of these abbreviations from domain-specific knowledge base as candidates. We train word embeddings using the clinical notes data with task-oriented resources such as Wikipedia articles of candidates and medical scientific papers and compute the semantic similarity between an abbreviation and its candidate expansions based on their embeddings (vector representations of words).",
      " rarely appear, we exploit abundant and easily-accessible task-oriented resources to enrich our dataset for training embeddings. To the best of our knowledge, we are the first to apply word embeddings to this task. Experimental",
      "k is to find the correct expansion for an abbreviation, we use accuracy as a metric to evaluate the performance of our approach. For ground-truth, we have 100 physician logs which are manually expanded and normalized by one of the authors Dr. Mathews, a well-trained domain expert, and thus we use these 100 physician logs as the test set to evaluate our approach's performance.",
      "results\nshow that the embeddings trained on the task-oriented corpus are much more useful than those trained on other corpora. By combining the embeddings with domain-specific knowledge, we achieve 82.27% accuracy, which outperforms baselines and is close to human's performance.",
      "bbreviations do not exist in the knowledge base. In this case we would not be able to populate the corresponding candidate list. Secondly, in many cases although we have the correct expansion in the candidate list, it's not ranked as the top one due to the lower semantic similarity because there are not enough samples in the training data.",
      "s difficult to re-implement the supervised methods as in previous works mentioned since we do not have sufficient training data. And a direct comparison is also impossible because all previous work used different data sets which are not publicly available. Alternatively, we use the following baselines to compare with our approach.",
      "ethods and achieves 82.27% accuracy. Figure FIGREF21 shows how our approach improves the performance of a rating-based approach. By using embeddings, we can learn that the meaning of “OD” used in our test cases should be “overdose” rather than “out-of-date” and this semantic information largely benefits the abbreviation expansion model.",
      "pproach, embeddings trained only from the ICU texts do not significantly contribute to the performance over the rating baseline. The reason is that the size of data for training the embeddings is so small that many candidate expansions of abbreviations do not appear in the corpus, which"
    ],
    "answer": [
      "The dataset used to build the model includes 1,160 raw ICU texts, Wikipedia articles of candidates, medical scientific papers, and 100 physician logs manually expanded and normalized by a well-trained domain expert."
    ]
  },
  {
    "title": "Identifying and Understanding User Reactions to Deceptive and Trusted Social News Sources",
    "evidence": [
      "Conclusion\nIn the current work, we have presented a content-based model that classifies user reactions into one of nine types, such as answer, elaboration, and question, etc., and a large-scale analysis of Twitter posts and Reddit comments in response to content from news sources of varying credibility.",
      "understand how users react to trusted and deceptive news sources across two popular, and very different, social media platforms. To that end, (1) we develop a model to classify user reactions into one of nine types, such as answer, elaboration, and question, etc, and (2) we measure the speed and the type of reaction for trusted and deceptive news sources for 10.8M Twitter posts and 6.2M Reddit comments.",
      " 10.8M Twitter posts and 6.2M Reddit comments in order to evaluate the speed and type of user reactions to various news sources. Reaction Type Classification In this section, we describe our approach to classify user reactions into one of eight types of discourse: agreement, answer, appreciation, disagreement, elaboration, humor, negative reaction, or question, or as none of the given labels, which we call “other”, using linguistically-infused neural network models.",
      " we found that the primary reaction types were answer, appreciation, elaboration, question, or “other” (no label was predicted). Figure FIGREF13 illustrates the distribution of reaction types among Reddit comments (top plot) or tweets (bottom plot) responding to each type of source, as a percentage of all comments/tweets reacting to sources of the given type (i.e., trusted, all deceptive, and deceptive excluding disinformation sources).",
      "ch tweet or comment. Using these labels, we examine how often response types occur when users react to each type of news source. For clarity, we report the five most frequently occurring reaction types (expressed in at least 5% of reactions within each source type) and compare the distributions of reaction types for each type of news source.",
      "Discussion\nFor both Twitter and Reddit datasets, we found that the primary reaction types were answer, appreciation, elaboration, question, or “other” (no label was predicted).",
      "eaction types and speed across two social media platforms — Twitter and Reddit. The first metric we report is the reaction type. Recent studies have found that 59% of bitly-URLs on Twitter are shared without ever being read BIBREF11 , and 73% of Reddit posts were voted on without reading the linked article BIBREF12 .",
      "nts/tweets reacting to sources of the given type (i.e., trusted, all deceptive, and deceptive excluding disinformation sources). For Twitter, we report clear differences in user reactions to trusted vs. deceptive sources.",
      "etweeted content from a source and then assigned a label to each tweet based on the class of the source @mentioned or retweeted. A breakdown of each dataset by source type is shown in Table TABREF10 . Figure FIGREF11 illustrates the distribution of deceptive news sources and reactions across the four sub-categories of deceptive news sources.",
      " sources. In our analysis, we consider the set of all deceptive sources and the set excluding the most extreme (disinformation). Methodology We use the linguistically-infused neural network model from Figure FIGREF5 to label the reaction type of each tweet or comment. Using these labels, we examine how often response types occur when users react to each type of news source.",
      "hat the size and shape of misinformation cascades within a social network depends heavily on the initial reactions of the users. Other work has focused on the language of misinformation in social media BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 to detect types of deceptive news.",
      "Results\nand",
      "presentations of normalized Linguistic Inquiry and Word Count (LIWC) features BIBREF16 for the text of each post and its parent. Reaction Type Classification"
    ],
    "answer": [
      "answer\nelaboration\nquestion\nappreciation\nagreement\ndisagreement\nhumor\nnegative reaction\nother"
    ]
  },
  {
    "title": "Deep Semi-Supervised Learning with Linguistically Motivated Sequence Labeling Task Hierarchies",
    "evidence": [
      " skip connections from the hidden layer to the senior supervised layers to allow layers to ignore information from junior tasks. A diagram of our network can be seen in Figure 1 . Supervision of Multiple Tasks Our model has 3 sources of error signals - one for each task.",
      "nior downstream tasks, a form of semi-supervised learning. Arguably, there is a two-way relationship between each pair of tasks. Following work such as sogaard2016deep, that exploits such hierarchies in a fully supervised setting, we represent this hierarchical relationship within the structure of a multi-task Recurrent Neural Network (RNN), where junior tasks in the hierarchy are supervised on inner layers and the parameters are jointly optimised during training.",
      "k we update both representations to maintain a coherent view of the world. By incorporating an unsupervised auxiliary task (e.g. plank2016multilingual) as the most senior layer we can use this structure for semi-supervised learning - the error on the unsupervised tasks penalises junior tasks when their representations and predictions are not consistent.",
      "ur network can be seen in Figure 1 . Supervision of Multiple Tasks Our model has 3 sources of error signals - one for each task. Since each task is categorical we use the discrete cross entropy to calculate the loss for each task: $ H(p, q) = - \\sum _{i}^{n_{labels}} p(label_i) \\ log \\ q(label_i) $ Where $n_{labels}$ is the number of labels in the task, $q(label_i)$ is the probability of label $i$ under the predicted distribution, and $p(label_i)$ is the probability of label $i$ in the true distribution (in this case, a one-hot vector).",
      "g (LM) can make use of information about the syntactic and semantic structure recovered from junior tasks in making predictions. Conversely, information about downstream tasks should also provide information that aids generalisation for junior downstream tasks, a form of semi-supervised learning. Arguably, there is a two-way relationship between each pair of tasks.",
      "rise the representations of supervised tasks by backpropagating the error of the unsupervised task through the supervised tasks. We introduce a neural network where lower layers are supervised by downstream tasks and the final layer task is an auxiliary unsupervised task.",
      "ning - the error on the unsupervised tasks penalises junior tasks when their representations and predictions are not consistent. It is the aim of this paper to demonstrate that organising a network in such a way can improve performance. To that end, although we do not achieve state of the art",
      "neural network where lower layers are supervised by downstream tasks and the final layer task is an auxiliary unsupervised task. The architecture shows improvements of up to two percentage points F β=1 for Chunking compared to a plausible baseline.",
      " with with our internal representation of a junior task we update both representations to maintain a coherent view of the world. By incorporating an unsupervised auxiliary task (e.g.",
      "ncorporating syntactic dependencies improves performance, demonstrates the benefits of incorporating junior tasks in prediction. Our neural network has one hidden layer, after which each successive task is supervised on the next layer. In addition, we add skip connections from the hidden layer to the senior supervised layers to allow layers to ignore information from junior tasks.",
      "k algorithm for conducting semisupervised learning for sequence labeling tasks arranged in a linguistically motivated hierarchy. This relationship is exploited to regularise the representations of supervised tasks by backpropagating the error of the unsupervised task through the supervised tasks.",
      "lty on the supervised junior layers for forming a representation and making predictions that are inconsistent with senior tasks. Intuitively, we can see how this can be beneficial - when humans receive new information from one task that is inconsistent with with our internal representation of a junior task we update both representations to maintain a coherent view of the world.",
      "only a fraction of the training data to see whether our semi-supervised approach makes our models more robust (Tables 3 and 4 ). Here, we find variable but consistent improvement in the performance of our tasks even at 1 % of the original training data. Label Embeddings Our model structure includes an embedding layer between each task."
    ],
    "answer": [
      "There are 3 supervised tasks."
    ]
  },
  {
    "title": "Yoga-Veganism: Correlation Mining of Twitter Health Data",
    "evidence": [
      " employed different visualizations after information integration and discovered interesting correlation (Yoga-Veganism) in data. In future, we will incorporate Local Interpretable model-agnostic Explanation (LIME) method to understand model interpretability.",
      " form the selected topic and their estimated term frequencies are shown. We observe interesting hidden correlation in data. Fig. FIGREF24 has Topic 2 as selected topic. Topic 2 contains top-4 co-occurring keywords \"vegan\", \"yoga\", \"job\", \"every_woman\" having the highest term frequency.",
      "ted topic. Topic 2 contains top-4 co-occurring keywords \"vegan\", \"yoga\", \"job\", \"every_woman\" having the highest term frequency. We can infer different things from the topic that \"women usually practice yoga more than men\", \"women teach yoga and take it as a job\", \"Yogi follow vegan diet\". We would say there are noticeable correlation in data i.e. `Yoga-Veganism', `Women-Yoga'.",
      " different application purpose. In this work, we explored Twitter health-related data, inferred topic using topic modeling (i.e. LSA, NMF, LDA), observed model behavior on new tweets, compared train/test accuracy with ground truth, employed different visualizations after information integration and discovered interesting correlation (Yoga-Veganism) in data.",
      "ia (i.e. Facebook, Twitter, Instagram etc.). It's huge amount of data and it's not possible to go through all the data manually. We need to mine the data to get overall statistics and then we will also be able to find some interesting correlation of data. Several works have been done on prediction of social media content BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 .",
      "t as a job\", \"Yogi follow vegan diet\". We would say there are noticeable correlation in data i.e. `Yoga-Veganism', `Women-Yoga'. Topic Frequency Distribution Each tweet is composed of multiple topics. But, typically only one of the topics is dominant.",
      "yoga\" has been found both in Topic 1 and Topic 4. We also notice that keyword \"eat\" is in Topic 2 and Topic 3 (Table TABREF13 ). If the same keywords being repeated in multiple topics, it is probably a sign that the `k' is large though we achieve the highest coherence score in NMF for k=4. We use LDA model for our further analysis.",
      "discussion\ns, tweets, status. It is not possible to go through all the data manually. We need to mine the data to explore hidden patterns or unknown correlations, find out the dominant topic in data and understand people's interest through the",
      "Introduction\nThe main motivation of this work has been started with a question \"What do people do to maintain their health?\"– some people do balanced diet, some do exercise.",
      "ssion\ns. In this work, we explore Twitter data related to health. We extract the popular topics under different categories (e.g. diet, exercise) discussed in Twitter via topic modeling, observe model behavior on new tweets, discover interesting correlation (i.e. Yoga-Veganism). We evaluate accuracy by comparing with ground truth using manual annotation both for train and test data.",
      " of the chart. In right hand side, the words represent the salient keywords. If we move the cursor over one of the bubbles (Fig. FIGREF21 ), the words and bars on the right-hand side have been updated and top-30 salient keywords that form the selected topic and their estimated term frequencies are shown. We observe interesting hidden correlation in data. Fig.",
      ".1, [subsec:3.2]3.2, [subsec:3.3]3.3, and [subsec:3.4]3.4 elaborately present how we can infer the meaning of unstructured data. Subsection [subsec:3.5]3.5 shows how we do manual annotation for ground truth comparison. Fig. FIGREF6 shows the overall pipeline of correlation mining.",
      "Conclusion\ns It is challenging to analyze social media data for different application purpose. In this work, we explored Twitter health-related data, inferred topic using topic modeling (i.e."
    ],
    "answer": [
      "Interesting correlations observed in the data include \"Yoga-Veganism\" and \"Women-Yoga\"."
    ]
  },
  {
    "title": "Learning Relational Dependency Networks for Relation Extraction",
    "evidence": [
      "conclusion\ns are the same across all metrics. Joint learning appears to help in about half of the relations (8/14). Particularly, in person category, joint learning with gold standard outperforms their individual learning counterparts.",
      " (8/14). Particularly, in person category, joint learning with gold standard outperforms their individual learning counterparts. This is due to the fact that some relations such as parents, spouse, siblings etc. are inter-related and learning them jointly indeed improves performance. Hence Q2 can be answered affirmatively for half the relations. word2vec Table TABREF24 shows the",
      " and joint learning being beneficial in the cases where the relations are correlated among themselves are on the expected lines. However, some surprising observations include the fact that weak supervision is not as useful as expected and word2vec features are not as predictive as the other domain-specific features.",
      "results\nof experiments that test the use of advice within the joint learning setting. The use of advice improves or matches the performance of using only joint learning.",
      "results\nsuch as human advice being useful in many relations and joint learning being beneficial in the cases where the relations are correlated among themselves are on the expected lines.",
      "dividual learning, standard features, with gold standard examples only (i.e., no weak supervision, word2vec, advice, or advice). Since our system had different components, we aimed to answer the following questions: Joint learning To address our next question, we assessed our pipeline when learning relations independently (i.e., individually) versus learning relations jointly within the RDN, displayed in Table TABREF22 .",
      "h only standard features) learned on gold standard examples only versus our system learned with weak and gold examples combined. Surprisingly, weak supervision does not seem to help learn better models for inferring relations in most cases.",
      "results\nin just 4 relations. We hypothesize that this may be due to a limitation in the depth of trees learned.",
      "of advice within the joint learning setting. The use of advice improves or matches the performance of using only joint learning. The key impact of advice can be mostly seen in the improvement of recall in several relations. This clearly shows that using human advice patterns allows us to extract more relations effectively making up for noisy or less number of training examples.",
      "vely learn the relation extraction task, performing comparably (and often better) than the state-of-art Relation Factory system. Furthermore, we demonstrated the ability of RDNs to incorporate various concepts in a relational framework, including word2vec, human advice, joint learning, and weak supervision. Some surprising",
      "standard procedure, we use three data sources – Never Ending Language Learner (NELL) BIBREF4 , Wikipedia Infoboxes and Freebase. For a given target relation, we identify relevant database(s), where the entries in the database form entity pairs (e.g., an entry of INLINEFORM0 for a parent database) that will serve as a seed for positive training examples.",
      "ing relations independently (i.e., individually) versus learning relations jointly within the RDN, displayed in Table TABREF22 . Recall and F1 are omitted for conciseness – the",
      "rd vectors from Stanford and Google along with a few specific words that, experts believe, are related to the relations learned. For example, we include words such as “father” and “mother” (inspired by the INLINEFORM0 relation) or “devout”,“convert”, and “follow” ( INLINEFORM1 relation)."
    ],
    "answer": [
      "Joint learning appears to help in about half of the relations, particularly in the person category, where it outperforms individual learning."
    ]
  },
  {
    "title": "Simultaneous Neural Machine Translation using Connectionist Temporal Classification",
    "evidence": [
      "re not from simultaneous interpretation but standard translation, so the current task does not match with the proposed approach. The use of specialized data for simultaneous translation would be important in practice, such as monotonic translations like simultaneous translation.",
      "proposed method\ninto simultaneous translation from English to Japanese and investigate its performance and remaining problems.",
      "Abstract\nSimultaneous machine translation is a variant of machine translation that starts the translation process before the end of an input. This task faces a trade-off between translation accuracy and latency.",
      "proposed method\ncan determine when to wait or translate in an adaptive manner and is useful in simultaneous translation tasks.",
      "Introduction\nSimultaneous translation is a translation task where the translation process starts before the end of an input. It helps real-time spoken language communications such as human conversations and public talks.",
      "y penalty would not be large enough to choose small latency translation at the cost of some increase in SCE- and CTC-based loss. The translation data in the experiments are not from simultaneous interpretation but standard translation, so the current task does not match with the proposed approach.",
      "determine when to translate based on two different actions: READ to take one input token and WRITE to generate one output token. While they reported some latency reduction without the loss of translation accuracy, the NMT model itself is trained independently from this incremental manner and is not fully optimized for simultaneous translation.",
      "proposed method\ndetermines when to translate or when to wait in an adaptive manner. Future work includes further analyses on translation accuracy in different latency conditions and time-based latency evaluation instead of the token-based one.",
      "n even in that case. This approach enables a simple end-to-end simultaneous NMT with implicit anticipation of unobserved inputs. It showed high translation accuracy with small latency on some common English-to-German and Chinese-to-English datasets.",
      "re we sometimes have to wait a latter part of a source language to determine the corresponding former part in a target language. Recent neural machine translation (NMT) studies tried an incremental processing for the simultaneous translation.",
      "tion, where we have a reference word sequence but do not have the corresponding segmentation or alignment in an acoustic signal. We conduct experiments in English-to-Japanese simultaneous translation with the proposed and baseline methods and show the",
      " fujita13interspeech proposed a phrase-based approach to the simultaneous translation based on phrasal reordering probabilities. oda-etal-2015-syntax proposed a syntax-based method to determine when to start translation of observed inputs.",
      "proposed method\nachieves a good translation performance with relatively small latency. The"
    ],
    "answer": [
      "The metrics used to evaluate simultaneous translation include translation accuracy, latency, SCE- and CTC-based loss, and time-based latency evaluation."
    ]
  },
  {
    "title": "Civique: Using Social Media to Detect Urban Emergencies",
    "evidence": [
      " provide labels according to the type of emergency they indicate. We use the manually labeled data for training our classifiers. We use traditional classification techniques such as Support Vector Machines(SVM), and Naive Bayes(NB) for training, and perform 10-fold cross validation to obtain f-scores.",
      " streaming APIs to get data, pre-processes it using the same modules, and detects emergencies using the classifiers built above. The tweets related to emergencies are displayed on the web interface along with the location and information for the concerned authorities.",
      "g the ongoing event on a map interface and alerting users with options to contact relevant authorities, both online and offline. We evaluate our classifiers for both the steps, i.e., emergency detection and categorization, and obtain F-scores exceeding 70% and 90%, respectively.",
      "elp Please note that, our current system performs compression, normalization and spell-checking if the language used is English. The classifier training and detection process are described below. Emergency Classification The first classifier model acts as a filter for the second stage of classification. We use both SVM and NB to compare the",
      " Our system detects whether a tweet, which contains a keyword from a pre-decided list, is related to an actual emergency or not. It also classifies the event into its appropriate category, and visualizes the possible location of the emergency event on the map.",
      "ystem detects emergency related events, and classifies them into appropriate categories like\"fire\",\"accident\",\"earthquake\", etc. We demonstrate our ideas by classifying Twitter posts in real time, visualizing the ongoing event on a map interface and alerting users with options to contact relevant authorities, both online and offline.",
      "people discuss events thousands of times across social media sites. We would like to detect such events in case of an emergency. Some previous studies BIBREF0 investigate the use of features such as keywords in the tweet, number of words, and context to devise a classifier for event detection.",
      " which indicates whether a tweet is related to an emergency or not, and if it is, then what category of emergency it belongs to. We display such positively classified tweets along with their type and location on a Google map, and notify our users to inform the concerned authorities, and possibly evacuate the area, if his location matches the affected area.",
      "requires the user to be connected to the internet. We address the above challenges and present our approach in the next section. Our Approach We propose a software architecture for Emergency detection and visualization as shown in figure FIGREF9 . We collect data using Twitter API, and perform language pre-processing before applying a classification model.",
      "such as Support Vector Machines(SVM), and Naive Bayes(NB) for training, and perform 10-fold cross validation to obtain f-scores. Later, in real time, our system uses the Twitter streaming APIs to get data, pre-processes it using the same modules, and detects emergencies using the classifiers built above.",
      "also classifies the event into its appropriate category, and visualizes the possible location of the emergency event on the map. We also support notifications to our users, containing the contacts of specifically concerned authorities, as per the category of their tweet.",
      " tweet using a filter available in the WEKA API BIBREF9 , and perform cross validation using standard classification techniques. Type Classification We employ a multi-class Naive Bayes classifier as the second stage classification mechanism, for categorizing tweets appropriately, depending on the type of emergencies they indicate.",
      "econd stage classification mechanism, for categorizing tweets appropriately, depending on the type of emergencies they indicate. This multi-class classifier is trained on data manually labeled with classes. We tokenize the training data using “NgramTokenizer” and then, apply a filter to create word vectors of strings before training."
    ],
    "answer": [
      "Support Vector Machines (SVM) and Naive Bayes (NB)"
    ]
  },
  {
    "title": "Identifying Products in Online Cybercrime Marketplaces: A Dataset for Fine-grained Domain Adaptation",
    "evidence": [
      "d the annotation standard, we annotated datasets from Darkode, Hack Forums, Blackhat, and Nulled as described in Table TABREF3 . Three people annotated every post in the Darkode training, Hack Forums training, Blackhat test, and Nulled test sets; these annotations were then merged into a final annotation by majority vote.",
      "cess trained annotators who were not security experts. The data annotated during this process is not included in Table TABREF3 . Once we had defined the annotation standard, we annotated datasets from Darkode, Hack Forums, Blackhat, and Nulled as described in Table TABREF3 .",
      "30,336 tokens; accounting for multiple annotators, our annotators considered 478,176 tokens in the process of labeling the data. Figure FIGREF2 shows two examples of posts from Darkode.",
      "notators considered 478,176 tokens in the process of labeling the data. Figure FIGREF2 shows two examples of posts from Darkode. In addition to aspects of the annotation, which we describe below, we see that the text exhibits common features of web text: abbreviations, ungrammaticality, spelling errors, and visual formatting, particularly in thread titles.",
      "s (five for Darkode, one for Hack Forums), and then every disagreement was discussed and resolved to produce a final annotation. The authors, who are researchers in either NLP or computer security, did all of the annotation. We preprocessed the data using the tokenizer and sentence-splitter from the Stanford CoreNLP toolkit BIBREF17 .",
      "rums training, Blackhat test, and Nulled test sets; these annotations were then merged into a final annotation by majority vote. The development and test sets for Darkode and Hack Forums were annotated by additional team members (five for Darkode, one for Hack Forums), and then every disagreement was discussed and resolved to produce a final annotation.",
      " time, associate users with particular activity profiles, and connect to price information to better understand the marketplace. Some of these analyses only require post-level information (what is the product being bought or sold in this post?) whereas other analyses might require token-level references; we annotate at the token level to make our annotation as general as possible.",
      "Conclusion\nWe present a new dataset of posts from cybercrime marketplaces annotated with product references, a task which blends IE and NER.",
      "ble TABREF3 gives some statistics of these forums. These are the same forums used to study product activity in PortnoffEtAl2017. We collected all available posts and annotated a subset of them. In total, we annotated 130,336 tokens; accounting for multiple annotators, our annotators considered 478,176 tokens in the process of labeling the data.",
      "Darkode with automatic annotation. Throughout the rest of this work, we focus on NP-level evaluation and post-level NP accuracy. Domain Adaptation Table TABREF30 only showed",
      "her analyses might require token-level references; we annotate at the token level to make our annotation as general as possible. Our dataset has already proven enabling for case studies on these particular forums BIBREF6 , including a study of marketplace activity on bulk hacked accounts versus users selling their own accounts.",
      "tion of every post with disagreements. We benefited from members of our team who brought extensive domain expertise to the task. As well as refining the annotation guidelines, the development process trained annotators who were not security experts. The data annotated during this process is not included in Table TABREF3 .",
      " the annotation. We preprocessed the data using the tokenizer and sentence-splitter from the Stanford CoreNLP toolkit BIBREF17 . Note that many sentences in the data are already delimited by line breaks, making the sentence-splitting task much easier."
    ],
    "answer": [
      "Three people annotated every post in the Darkode training, Hack Forums training, Blackhat test, and Nulled test sets; these annotations were then merged into a final annotation by majority vote. The development and test sets for Darkode and Hack Forums were annotated by additional team members (five for Darkode, one for Hack Forums), and then every disagreement was discussed and resolved to produce a final annotation. The authors, who are researchers in either NLP or computer security, did all of the annotation."
    ]
  },
  {
    "title": "Deep Health Care Text Classification",
    "evidence": [
      "nd it’s variant Recurrent neural network (RNN) was an enhanced model of feed forward network (FFN) introduced in 1990 BIBREF10 . The input sequences ${x_T}$ of arbitrary length are passed to RNN and a transition function $tf$ maps them into hidden state vector $h{i_{t - 1}}$ recursively.",
      "ation parameter. In task 1 the output layer contains $sigmoid$ activation function and $softmax$ activation function for task 2. Recurrent neural network (RNN) and it’s variant Recurrent neural network (RNN) was an enhanced model of feed forward network (FFN) introduced in 1990 BIBREF10 .",
      "ork (RNN) and Long short-term memory (LSTM) based embedding for automatic health text classification in the social media mining. For each task, two systems are built and that classify the tweet at the tweet level. RNN and LSTM are used for extracting features and non-linear activation function at the last layer facilitates to distinguish the tweets of different categories.",
      "rse antagonistic medicinal conditions. Mostly, the existing methods are based on machine learning with knowledge-based learning. This working note presents the Recurrent neural network (RNN) and Long short-term memory (LSTM) based embedding for automatic health text classification in the social media mining.",
      "arbitrary length are passed to RNN and a transition function $tf$ maps them into hidden state vector $h{i_{t - 1}}$ recursively. The hidden state vector $h{i_{t - 1}}$ are calculated based on the transition function $tf$ of present input sequence ${x_T}$ and previous hidden state vector $h{i_{t - 1}}$ .",
      "experiments embedding size is set to 512. The embedding layer output vector is further passed to RNN and its variant LSTM layer. RNN and LSTM obtain the optimal feature representation and those feature representation are passed to the dropout layer. Dropout layer contains 0.1 which removes the neurons and its connections randomly. This acts as a regularization parameter.",
      "size, two trails of experiments are run with embedding size 128, 256 and 512. For each experiment, learning rate is set to 0.01. An experiment with embedding size 512 performed well in both the RNN and LSTM networks. Thus for the rest of the experiments embedding size is set to 512. The embedding layer output vector is further passed to RNN and its variant LSTM layer.",
      "event detection. Though the data sets of task 1 and task 2 are limited, this paper proposes RNN and LSTM based embedding method. Background and hyper parameter selection This section discusses the concepts of tweet representation and deep learning algorithms particularly recurrent neural network (RNN) and long short-term memory (LSTM) in a mathematical way.",
      "yer. Dropout layer contains 0.1 which removes the neurons and its connections randomly. This acts as a regularization parameter. In task 1 the output layer contains $sigmoid$ activation function and $softmax$ activation function for task 2.",
      "health applications. This working note presents RNN and LSTM based embedding system for social media health text classification. Due to limited number of tweets, the performance of the",
      "nd deep learning algorithms particularly recurrent neural network (RNN) and long short-term memory (LSTM) in a mathematical way. Tweet representation Representation of tweets typically called as tweet encoding. This contains two steps. The tweets are tokenized to words during the first step. Moreover, all words are transformed to lower-case.",
      "hods have performed well BIBREF8 and used in many tasks mainly due to that it doesn't rely on any feature engineering mechanism. However, the performance of deep learning methods implicitly relies on the large amount of raw data sets. To make use of unlabeled data, BIBREF9 proposed semi-supervised approach based on Convolutional neural network for adverse drug event detection.",
      "Results\nAll experiments are trained using backpropogation through time (BPTT) BIBREF14 on Graphics processing unit (GPU) enabled TensorFlow BIBREF6 computational framework in conjunction with Keras framework in Ubuntu 14.04."
    ],
    "answer": [
      "Recurrent Neural Network (RNN) and Long Short-term Memory (LSTM)"
    ]
  },
  {
    "title": "MultiFC: A Real-World Multi-Domain Dataset for Evidence-Based Fact Checking of Claims",
    "evidence": [
      "ow to obtain claim representation, as the base model described in the previous section is agnostic to how instances are encoded. A very simple approach, which we report as a baseline, is to encode claim texts only. Such a model ignores evidence for and against a claim, and ends up guessing the veracity based on surface patterns observed in the claim texts.",
      "results\nover all domains. Core",
      "results\nfor automatic veracity prediction, both with established baselines and with a novel method for joint ranking of evidence pages and predicting veracity that outperforms all baselines.",
      "m (checker), tags, article title, publication date, claim date, as well as the full text that appears when the claim is clicked. Lastly, the above full text contains hyperlinks, so we further crawled the full text that appears when each of those hyperlinks are clicked (outlinks). There were a number of crawling issues, e.g.",
      "er to the part of the dataset crawled from a specific fact checking website as a domain, and we refer to each website as source. From each source, we crawled the ID, claim, label, URL, reason for label, categories, person making the claim (speaker), person fact checking the claim (checker), tags, article title, publication date, claim date, as well as the full text that appears when the claim is clicked.",
      "ed baselines and with a novel method for joint ranking of evidence pages and predicting veracity that outperforms all baselines. Significant performance increases are achieved by encoding evidence, and by modelling metadata. Our best-performing model achieves a Macro F1 of 49.2%, showing that this is a challenging testbed for claim veracity prediction.",
      " interest. The training dataset for task INLINEFORM3 consists of INLINEFORM4 examples INLINEFORM5 and their labels INLINEFORM6 . The base model is a classic deep neural network MTL model BIBREF31 that shares its parameters across tasks and has task-specific softmax output layers that output a probability distribution INLINEFORM7 for task INLINEFORM8 : DISPLAYFORM0 where INLINEFORM0 , INLINEFORM1 , INLINEFORM2 is the weight matrix and bias term of the output layer of task INLINEFORM3 respectively, INLINEFORM4 is the jointly learned hidden representation, INLINEFORM5 is the number of labels for task INLINEFORM6 , and INLINEFORM7 is the dimensionality of INLINEFORM8 .",
      "Results\nFor each domain, we compute the Micro as well as Macro F1, then mean average",
      "ed the full text that appears when each of those hyperlinks are clicked (outlinks). There were a number of crawling issues, e.g. security protection of websites with SSL/TLS protocols, time out, URLs that pointed to pdf files instead of HTML content, or unresolvable encoding. In all of these cases, the content could not be retrieved.",
      "by Duke Reporters' Lab and on the Fact Checking Wikipedia page. This resulted in 38 websites in total (shown in Table TABREF1 ). Ten websites could not be crawled, as further detailed in Table TABREF40 . In the later experimental descriptions, we refer to the part of the dataset crawled from a specific fact checking website as a domain, and we refer to each website as source.",
      " the instance representations, e.g. for the most elaborate model, crawled_ranked, these would be concatenated with INLINEFORM1 . Experimental Setup The base sentence embedding model is a BiLSTM over all words in the respective sequences with randomly initialised word embeddings, following BIBREF30 .",
      "extended with the number of documents, source of annotations and SoA performances can be found in the appendix (Table TABREF1 ). The datasets can be grouped into four categories (I–IV). Category I contains datasets aimed at testing how well the veracity of a claim can be predicted using the claim alone, without context or evidence documents.",
      "results\nof these manual checks on fact checking portals or websites such as politifact.com or snopes.com."
    ],
    "answer": [
      "The baselines are a simple approach to encode claim texts only, ignoring evidence for and against a claim, and guessing the veracity based on surface patterns observed in the claim texts."
    ]
  },
  {
    "title": "Retrieval-based Goal-Oriented Dialogue Generation",
    "evidence": [
      "hitecture leads to significant improvements, including responses that are rated more appropriate and fluent by human evaluators. Finally, we compare our retrieval-based model to various semantically conditioned models explicitly using past dialog act information, and find that our proposed model is competitive with the current state of the art (Chen et al., 2019), while not requiring explicit labels about past machine acts.",
      "ieved the largest improvements over the baseline when it came to the average embedding similarity and vector extrema similarity. As it is hard to interpret what the difference in performance of each model is based on standard dialogue metrics we examined the output to spot the major differences in response generation of our proposed models versus the baseline.",
      "results\ninto models using act information to condition the language generation and models that do not.",
      "results\nof a transformer modelBIBREF31 that uses dialogue context to condition the decoder as well as a LSTM that uses dialogue context and incorporates belief state and KB",
      "ue, we are interested in assessing how our simple approach compares to models explicitly using dialog acts as extra information. We compute the inform/request accuracy and compare to the state-of-the-art BIBREF1 as well as other baseline models. BIBREF1 present experiments conditioning both on predicted acts as well as ground truth past acts.",
      "erformance close to the more complex state-of-the-art model which contrary to our proposed model, makes use of annotated labels. Finally, based on our human evaluations, we show that a simple retrieval step leads to system responses that are more fluent and appropriate than the responses generated by the hierarchical encoder-decoder model.",
      "ore appropriate given the conversation context. This was confirmed by the human evaluations and also the Inform/Request metrics. When comparing the performance of the exemplar-based model to models that do not use information about past acts to condition the decoder, we observe that including a simple retrieval step leads to very large gains in the success of providing the inform/request slots..",
      " metrics we examined the output to spot the major differences in response generation of our proposed models versus the baseline. We looked at the responses generated by our proposed models that had the highest score of these metrics and compared to the response generated by the baseline for that same dialogue.",
      "is metric was more indicative of model performance than embedding similarity. We present some example outputs in table TABREF16. As mentioned earlier, from manual inspection of the outputs we observed that the the exemplar model is able to stay within the correct domain of the conversation and returns information within that domain that is more appropriate given the conversation context.",
      "the evaluated dialogs and the rest of the dialogs either both or none were picked. For appropriateness we see a similar pattern. Evaluators perceived the response produced by the exemplar model as the more appropriate one given the context, for 59 percent of the evaluated dialogs. The baseline beat the proposed model only 14 percent of the time. These",
      " it comes to open ended conversations as well as when trained on very large corpora (millions of utterances) BIBREF29, BIBREF30. In our setup however, we train the HRED model using a smaller dataset containing goal oriented dialogues in 7 different domains. We compare this model with our proposed exemplar-based model. In addition, for the BLEU metric we include the",
      "ell as other baseline models. BIBREF1 present experiments conditioning both on predicted acts as well as ground truth past acts. We include both of these as well as the performance of our baseline and proposed model in table TABREF15. We divide the",
      "odels that had the highest score of these metrics and compared to the response generated by the baseline for that same dialogue. Overall we found that the baseline models tend to generate responses containing slots and values for the wrong domain."
    ],
    "answer": [
      "Chen et al. (2019), a transformer model that uses dialogue context to condition the decoder, and a LSTM that uses dialogue context and incorporates belief state and KB."
    ]
  },
  {
    "title": "User Generated Data: Achilles' heel of BERT",
    "evidence": [
      "lication you are developing gets data from channels that are known to introduce noise in the text, then BERT will perform badly. Examples of such scenarios are applications working with twitter data, mobile based chat system, user comments on platforms like youtube, reddit to name a few. The reason for the",
      "nerated text data on the performance of BERT. We demonstrated that as the noise increases, BERT’s performance drops drastically. We further investigated the BERT system to understand the reason for this drop in performance. We show that the problem lies with how misspelt words are tokenized to create a representation of the original word.",
      "ks and benchmark datasets, industry practitioners have started to explore BERT to build applications solving industry use cases. These use cases are known to have much more noise in the data as compared to benchmark datasets. In this work we systematically show that when the data is noisy, there is a significant degradation in the performance of BERT.",
      "introduction\nof noise leads to a significant drop in performance of the BERT model for the task at hand as compared to clean dataset. We further show that as we increase the amount of noise in the data, the performance degrades sharply.",
      " In this work we systematically show that when the data is noisy, there is a significant degradation in the performance of BERT. Specifically, we performed experiments using BERT on popular tasks such sentiment analysis and textual similarity. For this we work with three well known datasets - IMDB movie reviews, SST-2 and STS-B to measure the performance.",
      "ntation in turn impacts the performance of downstream subcomponents of BERT bringing down the overall performance of BERT model. Hence, as we systematically introduce more errors, the quality of output of the tokenizer degrades further, resulting in the overall performance drop. Our",
      "results\nfor many of these tasks. People have been able to show how one can leverage BERT to improve searchBIBREF7. Owing to its success, researchers have started to focus on uncovering drawbacks in BERT, if any.",
      "plots that as we increase the percentage of error, for each of the three tasks, we see a significant drop in BERT’s performance. Also, from the plots it is evident that the reason for this drop in performance is",
      "oise (spelling mistakes). After all we get very good numbers, for each of the three tasks, when there is no error (0.0 % error). To understand the reason behind the drop in performance, first we need to understand how BERT processes input text data. BERT uses WordPiece tokenizer to tokenize the text.",
      " because that is how users prefer to write, while for mobile based chat it often suffers from ‘fat finger’ typing error problem. Depending on the amount of noise in the data, BERT can perform well below expectations. We further conducted experiments with different tokenizers other than WordPiece tokenizer.",
      "results\nand analysis shows that one cannot apply BERT blindly to solve NLP problems especially in industrial settings. If the application you are developing gets data from channels that are known to introduce noise in the text, then BERT will perform badly.",
      "of training data. In this work we highlight how BERT fails to handle Out Of Vocabulary(OOV) words, given its limited vocabulary. We show that this negatively impacts the performance of BERT when working with user generated text data and evaluate the same.",
      "Conclusion\nand Future Work In this work we systematically studied the effect of noise (spelling mistakes) in user generated text data on the performance of BERT. We demonstrated that as the noise increases, BERT’s performance drops drastically."
    ],
    "answer": [
      "The reason behind the drop in performance using BERT for some popular tasks is the presence of noise in the text data, particularly spelling mistakes, which negatively impacts the quality of the output of the tokenizer, leading to a significant drop in BERT's performance."
    ]
  },
  {
    "title": "Cohesion and Coalition Formation in the European Parliament: Roll-Call Votes and Twitter Activities",
    "evidence": [
      "on the level of cohesion of the European Parliament's groups by analyzing the co-voting through the agreement and ERGM measures. Next, we explore two important policy areas, namely Economic and monetary system and State and evolution of the Union. Finally, we analyze the cohesion of the European Parliament's groups on Twitter. Existing research by Hix et al.",
      "results\n. The main contributions of this paper are as follows: (i) We give general insights into the cohesion of political groups in the Eighth European Parliament, both overall and across different policy areas.",
      " data-analysis scenarios, and the second one is based on Exponential Random Graph Models, often used in social-network analysis. We give general insights into the cohesion of political groups in the European Parliament, explore whether coalitions are formed in the same way for different policy areas, and examine to what degree the retweeting behavior of MEPs corresponds to their co-voting patterns.",
      " data-analysis scenarios, and the second one is based on Exponential Random Graph Models, often used in social-network analysis. We give general insights into the cohesion of political groups in the European Parliament, explore whether coalitions are formed in the same way for different policy areas, and examine to what degree the retweeting behavior of MEPs corresponds to their co-voting patterns.",
      "nsights into the cohesion of political groups in the Eighth European Parliament, both overall and across different policy areas. (ii) We explore whether coalitions are formed in the same way for different policy areas. (iii) We explore to what degree the retweeting behavior of MEPs corresponds to their co-voting patterns.",
      " interested in the factors that determine the cohesion within political groups and coalition formation between political groups. Furthermore, we investigate to what extent communication in a different social context, i.e., the retweeting behavior of MEPs, can explain the co-voting of MEPs.",
      "Abstract\nWe study the cohesion within and the coalitions between political groups in the Eighth European Parliament (2014--2019) by analyzing two entirely different aspects of the behavior of the Members of the European Parliament (MEPs) in the policy-making processes.",
      "Abstract\nWe study the cohesion within and the coalitions between political groups in the Eighth European Parliament (2014–2019) by analyzing two entirely different aspects of the behavior of the Members of the European Parliament (MEPs) in the policy-making processes.",
      " In this paper we study the cohesion and coalitions exhibited by political groups in the Eighth European Parliament (2014–2019). We analyze two entirely different aspects of how the Members of the European Parliament (MEPs) behave in policy-making processes. On one hand, we analyze their co-voting patterns and, on the other, their retweeting (i.e., endorsing) behavior.",
      "Conclusion\ns In this paper we analyze (co-)voting patterns and social behavior of members of the European Parliament, as well as the interaction between these two systems.",
      "n of the Union. Finally, we analyze the cohesion of the European Parliament's groups on Twitter. Existing research by Hix et al. BIBREF10 , BIBREF13 , BIBREF11 shows that the cohesion of the European political groups has been rising since the 1990s, and the level of cohesion remained high even after the EU's enlargement in 2004, when the number of MEPs increased from 626 to 732.",
      "wed over time, increasingly so—by affiliation to a political group, as an organizational reflection of the ideological position. The authors found that the cohesion of political groups in the parliament has increased, while nationality has been less and less of a decisive factor BIBREF12 .",
      "o methodologies agree on the level of cohesion for all the political groups, except for GUE-NGL, due to a lower attendance rate. We determine the cohesion of political groups on Twitter by using the average number of retweets between MEPs within the same group. The"
    ],
    "answer": [
      "The analysis provides insights into the cohesion of political groups in the European Parliament, both overall and across different policy areas, and explores whether coalitions are formed in the same way for different policy areas."
    ]
  },
  {
    "title": "S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension",
    "evidence": [
      "anually created datasets for model training and testing of machine learning (especially deep learning) algorithms for this task. There are two main differences in existing machine reading comprehension datasets. First, the SQuAD dataset constrains the answer to be an exact sub-span in the passage, while words in the answer are not necessary in the passages in the MS-MARCO dataset.",
      "wer to be an exact sub-span in the passage, while words in the answer are not necessary in the passages in the MS-MARCO dataset. Second, the SQuAD dataset only has one passage for a question, while the MS-MARCO dataset contains multiple passages. Existing methods for the MS-MARCO dataset usually follow the extraction based approach for single passage in the SQuAD dataset.",
      "Abstract\nIn this paper, we present a novel approach to machine reading comprehension for the MS-MARCO dataset. Unlike the SQuAD dataset that aims to answer a question with exact text spans in a passage, the MS-MARCO dataset defines the task as answering a question from multiple passages and the words in the answer are not necessary in the passages.",
      " Existing methods for the MS-MARCO dataset usually follow the extraction based approach for single passage in the SQuAD dataset. It formulates the task as predicting the start and end positions of the answer in the passage.",
      "r to the SQuAD, MS-MARCO BIBREF1 is the reading comprehension dataset which aims to answer the question given a set of passages. The answer in MS-MARCO is generated by human after reading all related passages and not necessarily sub-spans of the passages. To the best of our knowledge, the existing works on the MS-MARCO dataset follow their methods on the SQuAD.",
      "ch longer phrase is more challenging. The answer in SQuAD is a segment of text, or span, from the corresponding reading passage. Similar to the SQuAD, MS-MARCO BIBREF1 is the reading comprehension dataset which aims to answer the question given a set of passages.",
      "ans of the passages. To the best of our knowledge, the existing works on the MS-MARCO dataset follow their methods on the SQuAD. BIBREF7 combine match-LSTM and pointer networks to produce the boundary of the answer. BIBREF8 and BIBREF9 employ variant co-attention mechanism to match the question and passage mutually.",
      "gle passage in the SQuAD dataset. It formulates the task as predicting the start and end positions of the answer in the passage. However, as defined in the MS-MARCO dataset, the answer may come from multiple spans, and the system needs to elaborate the answer using words in the passages and words from the questions as well as words that cannot be found in the passages or questions.",
      "Abstract\nIn this paper, we present a novel approach to machine reading comprehension for the MS-MARCO dataset.",
      ". Other works which only focus on the SQuAD dataset may also be applied on the MS-MARCO dataset BIBREF11 , BIBREF12 , BIBREF13 . The sequence-to-sequence model is widely-used in many tasks such as machine translation BIBREF14 , parsing BIBREF15 , response generation BIBREF16 , and summarization generation BIBREF17 .",
      "matching mechanism for aggregating evidence from the whole passage, which achieves the state-of-the-art result on SQuAD dataset. Other works which only focus on the SQuAD dataset may also be applied on the MS-MARCO dataset BIBREF11 , BIBREF12 , BIBREF13 .",
      "aset, which only has one passage given a question, there are several related passages for each question in the MS-MARCO dataset. In addition to annotating the answer, MS-MARCO also annotates which passage is correct. To this end, we propose improving text span prediction with passage ranking.",
      " annotate multiple evidence snippets as features in the sequence-to-sequence model for questions in this category in the future. Acknowledgement We thank the MS-MARCO organizers for help in submissions."
    ],
    "answer": [
      "The MS-MARCO dataset differs from SQuAD in that the answer may come from multiple spans, and the system needs to elaborate the answer using words in the passages and words from the questions as well as words that cannot be found in the passages or questions, whereas SQuAD constrains the answer to be an exact sub-span in the passage."
    ]
  },
  {
    "title": "Neural Machine Translation with Supervised Attention",
    "evidence": [
      "results\non word alignment task in terms of the metric, alignment error rate. We used the manually-aligned dataset as in BIBREF5 as the test set.",
      "alignment task in terms of the metric, alignment error rate. We used the manually-aligned dataset as in BIBREF5 as the test set. Following BIBREF17 , we force-decode both the bilingual sentences including source and reference sentences to obtain the alignment matrices, and then for each target word we extract one-to-one alignments by picking up the source word with the highest alignment confidence as the hard alignment.",
      "we can see that both standard neural machine translation systems NMT1 and NMT2 are much worse than Moses with a substantial gap. This result is not difficult to understand: neural network systems typically require sufficient data to boost their performance, and thus low resource translation tasks are very challenging for them.",
      "rl as its implementation. The Large Scale Translation Task We used the data from the NIST2008 Open Machine Translation Campaign. The training data consisted of 1.8M sentence pairs, the development set was nist02 (878 sentences), and the test sets are were nist05 (1082 sentences), nist06 (1664 sentences) and nist08 (1357 sentences).",
      "periments were run with the default settings. To train NMT1, NMT2 and SA-NMT, we employed the same settings for fair comparison. Specifically, except the stopping iteration which was selected using development data, we used the default settings set out in BIBREF0 for all NMT-based systems: the dimension of word embedding was 620, the dimension of hidden units was 1000, the batch size was 80, the source and target side vocabulary sizes were 30000, the maximum sequence length was 50, the beam size for decoding was 12, and the optimization was done by Adadelta with all hyper-parameters suggested by BIBREF16 .",
      "cally require sufficient data to boost their performance, and thus low resource translation tasks are very challenging for them. Secondly, the proposed SA-NMT gains much over NMT2 similar to the case in the large scale task, and the gap towards Moses is narrowed substantially.",
      "results\nfor the large scale task. We find that both standard NMT generally outperforms Moses except NMT1 on nist05.",
      "ent on the training data before training SA-NMT. In this paper, we used two different aligners, which are fast_align and GIZA++. We tuned the hyper-parameter INLINEFORM0 to be 0.3 on the development set, to balance the preference between the translation and alignment. Training was conducted on a single Tesla K40 GPU machine.",
      "sed the BTEC corpus as the training data, which consists of 30k sentence pairs with 0.27M Chinese words and 0.33M English words. As development and test sets, we used the CSTAR03 and IWSLT04 held out sets, respectively. We trained a 4-gram language model on the target side of training corpus for running Moses.",
      "arned from alignment models for SMT, and in particular there are some neural network based reordering models, such as BIBREF22 . Our work is inspired by these works in spirit, and it can be considered to be a recurrent neural network based word-level reordering model.",
      "on, since it is able to dynam- ically encode a source sentence by generating a alignment between a target word and source words. Unfortunately, it has been proved to be worse than conventional alignment models in aligment accuracy.",
      "nist02 (878 sentences), and the test sets are were nist05 (1082 sentences), nist06 (1664 sentences) and nist08 (1357 sentences). We compared the proposed approach with three strong baselines: Moses: a phrase-based machine translation system BIBREF15 ; NMT1: an attention based NMT BIBREF0 system at https://github.com/lisa-groundhog/GroundHog; NMT2: another implementation of BIBREF0 at https://github.com/nyu-dl/dl4mt-tutorial.",
      "s are typically oriented to grammar induction for conventional SMT, and they usually output `hard' alignments, such as BIBREF3 . They only indicate whether a target word is aligned to a source word or not, and this might not correspond to a distribution for each target word. For example, one target word may align to multiple source words, or no source words at all."
    ],
    "answer": [
      "The dataset used is the NIST2008 Open Machine Translation Campaign."
    ]
  },
  {
    "title": "An Unsupervised Word Sense Disambiguation System for Under-Resourced Languages",
    "evidence": [
      " averaging of dense word vectors for semantically related words. Thus, we recommend using the dense approach in further studies. Although the AdaGram approach trained on a large text corpus showed better",
      "ing of instances, according to sense labels. The system's clustering is compared to the gold-standard clustering for evaluation. Quality Measure The original SemEval 2010 Task 14 used the V-Measure external clustering measure BIBREF5 . However, this measure is maximized by clustering each sentence into his own distinct cluster, i.e., a `dummy' singleton baseline.",
      "ork, and disambiguation is performed by computing the semantic similarity between the context and the candidate senses BIBREF9 . Pelevina:16 proposed a similar approach to WSD, but based on dense vector representations (word embeddings), called SenseGram.",
      " by averaging the vectors of the words belonging to non-auxiliary parts of speech, i.e., nouns, adjectives, adverbs, verbs, etc. Then, given a word to disambiguate, we retrieve the synset that maximizes the cosine similarity between the dense sentence vector and the dense synset vector.",
      "results\nin every case (Table TABREF25 ). The primary reason for that is the implicit handling of similar words due to the averaging of dense word vectors for semantically related words. Thus, we recommend using the dense approach in further studies.",
      "r most natural languages. Due to the broad coverage of BabelNet, Babelfy offers entity linking as part of the WSD functionality. Panchenko:17:emnlp present an unsupervised WSD system that is also knowledge-free: its sense inventory is induced based on the JoBimText framework, and disambiguation is performed by computing the semantic similarity between the context and the candidate senses BIBREF9 .",
      "rd embeddings. In the vector space model approach, we follow the sparse context-based disambiguated method BIBREF12 , BIBREF13 . For estimating the sense of the word INLINEFORM0 in a sentence, we search for such a synset INLINEFORM1 that maximizes the cosine similarity to the sentence vector: DISPLAYFORM0 where INLINEFORM0 is the set of words forming the synset, INLINEFORM1 is the set of words forming the sentence.",
      "uate, we retrieve the synset that maximizes the cosine similarity between the dense sentence vector and the dense synset vector. Thus, given the number of dimensions INLINEFORM0 , disambiguation of the whole sentence INLINEFORM1 requires INLINEFORM2 operations.",
      "ining the word. Each instance is manually annotated with the single sense identifier according to a pre-defined sense inventory. Each participating system estimates the sense labels for these ambiguous words, which can be viewed as a clustering of instances, according to sense labels. The system's clustering is compared to the gold-standard clustering for evaluation.",
      "owever, this measure is maximized by clustering each sentence into his own distinct cluster, i.e., a `dummy' singleton baseline. This is achieved by the system deciding that every ambiguous word in every sentence corresponds to a different word sense.",
      "r, a part-of-speech tagger, lemmatizer, and a sense inventory, which means that low-resourced language can benefit of its usage. Acknowledgements We acknowledge the support of the Deutsche Forschungsgemeinschaft (DFG) under the project “Joining Ontologies and Semantics Induced from Text” (JOIN-T), the RFBR under the projects no. 16-37-00203 mol_a and no.",
      "arity between the given sentence and the synset constituting the sense of the target word. Watasense has two modes of operation. The sparse mode uses the traditional vector space model to estimate the most similar word sense corresponding to its context. The dense mode, instead, uses synset embeddings to cope with the sparsity problem.",
      "line. This is achieved by the system deciding that every ambiguous word in every sentence corresponds to a different word sense. To cope with this issue, we follow a similar study BIBREF1 and use instead of the adjusted Rand index (ARI) proposed by Hubert:85 as an evaluation measure. In order to provide the overall value of ARI, we follow the addition approach used in BIBREF1 ."
    ],
    "answer": [
      "The measure of semantic similarity used is cosine similarity."
    ]
  },
  {
    "title": "Self-Attentional Models Application in Task-Oriented Dialogue Generation Systems",
    "evidence": [
      "results\nusing different evaluation metrics on three different datasets.",
      "results\nare not corpus-biased. Table TABREF12 shows the statistics of these three datasets which we will use to train and evaluate the models.",
      "lation, but their usage has not been explored for the task of training end-to-end task-oriented dialogue generation systems yet. In this study, we apply these models on the three different datasets for training task-oriented chatbots.",
      "ated the models using the aforementioned metrics and also applied early stopping (with delta set to 0.1 for 600 training steps). Experiments ::: Datasets We use three different datasets for training the models. We use the Dialogue State Tracking Competition 2 (DSTC2) dataset BIBREF27 which is the most widely used dataset for research on task-oriented chatbots.",
      "te Tracking Competition 2 (DSTC2) dataset BIBREF27 which is the most widely used dataset for research on task-oriented chatbots. We also used two other datasets recently open-sourced by Google Research BIBREF28 which are M2M-sim-M (dataset in movie domain) and M2M-sim-R (dataset in restaurant domain).",
      "t corpus-biased. Table TABREF12 shows the statistics of these three datasets which we will use to train and evaluate the models. The M2M dataset has more diversity in both language and dialogue flow compared to the the commonly used DSTC2 dataset which makes it appealing for the task of creating task-oriented chatbots.",
      "results\nusing different evaluation metrics on three different datasets. We provide insight into how effective are self-attentional models for this task and benchmark the time performance of these models against the recurrence-based sequence modelling methods.",
      "e flow compared to the the commonly used DSTC2 dataset which makes it appealing for the task of creating task-oriented chatbots. This is also the reason that we decided to use M2M dataset in our experiments to see how well models can handle a more diversed dataset.",
      "wo datasets were created. In this framework, dialogues are created via dialogue self-play and later augmented via crowdsourcing. We trained on our models on different datasets in order to make sure the",
      "t and in Table TABREF18 for the M2M datasets. The bold numbers show the best performing model in each of the evaluation metrics. As discussed before, for each model we use different beam sizes (bs) in inference time and report the best one.",
      "also the reason that we decided to use M2M dataset in our experiments to see how well models can handle a more diversed dataset. Experiments ::: Datasets ::: Dataset Preparation We followed the data preparation process used for feeding the conversation history into the encoder-decoder as in BIBREF5.",
      "results\nof running the experiments for the aforementioned models is shown in Table TABREF14 for the DSTC2 dataset and in Table TABREF18 for the M2M datasets. The bold numbers show the best performing model in each of the evaluation metrics.",
      " the embedding size to 128 for all the models. We also used grid search for hyperparameter tuning for all of the trained models. Details of our training and hyperparameter tuning and the code for reproducing the"
    ],
    "answer": [
      "The three datasets used are DSTC2, M2M-sim-M, and M2M-sim-R."
    ]
  },
  {
    "title": "\"How May I Help You?\": Modeling Twitter Customer Service Conversations Using Fine-Grained Dialogue Acts",
    "evidence": [
      "success/failure of the interaction, and then to develop actionable rules to be used in automating customer service interactions. We focus on the customer service domain on Twitter, which has not previously been explored in the context of dialogue act classification. In this new domain, we can provide meaningful recommendations about good communicative practices, based on real data.",
      "he customer service domain on Twitter. Our goal is to offer useful analytics to improve outcome-oriented conversational systems. We first expand upon previous work and generic dialogue act taxonomies, developing a fine-grained set of dialogue acts for customer service, and conducting a systematic user study to identify these acts in a dataset of 800 conversations from four Twitter customer service accounts (i.e.",
      "Twitter, we find that the customer service domain presents its own interesting characteristics that are worth exploring further. The most related previous work has explored speech and dialogue act modeling in customer service, however, no previous work has focused on Twitter as a data source.",
      " our task, and tweets, although concise in nature, often consist of overlapping dialogue acts to characterize their full intent. The novelty of our work comes from the development of our fine-grained dialogue act taxonomy and multi-label approach for act prediction, as well as our analysis of the customer service domain on Twitter.",
      "y of fine-grained dialogue acts, tailored for the customer service domain, and gather annotations for 800 Twitter conversations. We show that dialogue acts are often semantically overlapping, and conduct multi-label supervised learning experiments to predict multiple appropriate dialogue act labels for each turn in real-time, under varying class sizes.",
      "cus on analysis of the dialogue acts used in customer service conversations as a first step to fully automating the interaction. We address various different challenges: dialogue act annotated data is not available for customer service on Twitter, the task of dialogue act annotation is subjective, existing taxonomies do not capture the fine-grained information we believe is valuable to our task, and tweets, although concise in nature, often consist of overlapping dialogue acts to characterize their full intent.",
      "nds in customer service conversations on Twitter to offer insight into good/bad practices with respect to conversation outcomes. We design a novel taxonomy of fine-grained dialogue acts, tailored for the customer service domain, and gather annotations for 800 Twitter conversations.",
      "eling, general conversation modeling on Twitter, and speech and dialogue act modeling of customer service in other data sources. Previous work has explored speech act modeling in different domains (as a predecessor to dialogue act modeling). Zhang et al.",
      "be good features for speech act classification and the application of such a system to detect stories on social media BIBREF11 . In this work, we are interested in the dialogic characteristics of Twitter conversations, rather than speech acts in stand-alone tweets. Different dialogue act taxonomies have been developed to characterize conversational acts.",
      "ee less frequent acts, such as Statement Sarcasm, Social Act Downplayer, Statement Promise, Greeting Closing, and Request Other. It is also interesting to note that both opening and closing greetings occur infrequently in the data – which is understandable given the nature of Twitter conversation, where formal greeting is not generally required.",
      "service domain, and focus on Twitter conversations, which are unique in their brevity and the nature of the public interactions. The most similar work to our own is that of Herzig et al. on classifying emotions in customer support dialogues on Twitter BIBREF23 .",
      "ing these acts are not very well defined, and that segmentation would not necessarily aid in clearly separating out each intent. For these reasons, and due to the overall brevity of tweets in general, we choose to avoid the overhead of requiring annotators to provide segment boundaries, and instead ask for all appropriate dialogue acts. Annotation",
      "re they focus on classifying dialogue acts in both one-on-one and multi-party live instant messaging chats BIBREF21 , BIBREF22 . These works are similar to ours in the nature of the problem addressed, but we use a much more fine-grained taxonomy to define the interactions possible in the customer service domain, and focus on Twitter conversations, which are unique in their brevity and the nature of the public interactions."
    ],
    "answer": [
      "The dialogue acts more suited to the Twitter domain are: Statement Sarcasm, Social Act Downplayer, Statement Promise, Greeting Closing, and Request Other."
    ]
  },
  {
    "title": "Sex Trafficking Detection with Ordinal Regression Neural Networks",
    "evidence": [
      "icating that they are all used in similar contexts and perhaps should all be flags for underaged victims in the updated lexicon. If we re-train the skip-gram model and update the emoji map periodically on new escort ads, when traffickers switch to new emojis, the map can link the new emojis to the old ones, assisting anti-trafficking experts in expanding the lexicon of trafficking flags.",
      "e map can link the new emojis to the old ones, assisting anti-trafficking experts in expanding the lexicon of trafficking flags. This approach also works for acronyms and deliberate typos.",
      "act that the visually similar emojis are close to one another in the map suggests that the vectors have been learned as desired. The emoji map can assist anti-trafficking experts in expanding the existing lexicon of trafficking flags.",
      "emoji analysis and showed how to use word embeddings learned from raw text data to help expand the lexicon of trafficking flags. Since our experiments, there have been considerable advancements in language representation models, such as BERT BIBREF30 .",
      "ed method\nsignificantly improves on the previous state-of-the-art on Trafficking-10K, an expert-annotated dataset of escort ads. Additionally, because traffickers use acronyms, deliberate typographical errors, and emojis to replace explicit keywords, we demonstrate how to expand the lexicon of trafficking flags through word embeddings and t-SNE.",
      " often avoid using explicit keywords when advertising victims, but instead use acronyms, intentional typos, and emojis BIBREF9 . Law enforcement maintains a lexicon of trafficking flags mapping certain emojis to their potential true meanings (e.g., the cherry emoji can indicate an underaged victim), but compiling such a lexicon manually is expensive, requires frequent updating, and relies on domain expertise that is hard to obtain (e.g., insider information from traffickers or their victims).",
      "en learned as desired. The emoji map can assist anti-trafficking experts in expanding the existing lexicon of trafficking flags. For example, according to the lexicon we obtained from Global Emancipation Network, the cherry emoji and the lollipop emoji are both flags for underaged victims.",
      "led data from escort ads, however, we go further by analyzing the emojis' embeddings and thereby expand the trafficking lexicon. Ordinal regression: We briefly review ordinal regression before introducing the",
      "ine ordinal regression models as well as improving the classification accuracy over the Human Trafficking Deep Network BIBREF9 . We also conducted an emoji analysis and showed how to use word embeddings learned from raw text data to help expand the lexicon of trafficking flags.",
      "y outperforms previously published models BIBREF9 on Trafficking-10k as well as a variety of baseline ordinal regression models. In addition, we analyze the emojis used in escort ads with Word2Vec and t-SNE BIBREF10 , and we show that the lexicon of trafficking-related emojis can be subsequently expanded.",
      "t updating, and relies on domain expertise that is hard to obtain (e.g., insider information from traffickers or their victims). To make matters worse, traffickers change their dictionaries over time and regularly switch to new emojis to replace certain keywords BIBREF9 .",
      " worse, traffickers change their dictionaries over time and regularly switch to new emojis to replace certain keywords BIBREF9 . In such a dynamic and adversarial environment, the need for a data-driven approach in updating the existing lexicon is evident.",
      " ads with Word2Vec and t-SNE BIBREF10 , and we show that the lexicon of trafficking-related emojis can be subsequently expanded. In Section SECREF2 , we discuss related work on human trafficking detection and ordinal regression. In Section SECREF3 , we present our proposed model and detail its components. In Section SECREF4 , we present the experimental"
    ],
    "answer": [
      "The lexicon of trafficking flags is expanded by re-training the skip-gram model and updating the emoji map periodically on new escort ads, linking new emojis to old ones, and using word embeddings learned from raw text data."
    ]
  },
  {
    "title": "MDE: Multi Distance Embeddings for Link Prediction in Knowledge Graphs",
    "evidence": [
      "results\nof the models are close and the reported works make an unfair comparison of the dataset construction rather than the relational learning models. This is considerable when comparing",
      "sets for link prediction evaluation miss negative samples. Therefore, models generate their own negative samples while training. In consequence, the method of negative sample generation influences the result of the models. This problem is crucial because the ranking",
      "ts\non the inverse relation excluded datasets are from BIBREF11 , Table 13 for TransE and RotatE and the rest are from BIBREF35 . Evaluation Settings: We evaluate the link prediction performance by ranking the score of each test triple against its versions with replaced head, and once for tail.",
      "omparison of MDE and TransE and other distance based models confirms the improved ability of MDE in learning different patterns. Negative Sampling and Data Augmentation Models: Currently, the training datasets for link prediction evaluation miss negative samples. Therefore, models generate their own negative samples while training.",
      "rameters, and their efficiency on large scale datasets. Specifically, their simplicity allows integrating them into many models. Previous studies have integrated them with logical rule embeddings BIBREF2 , have adopted them to encode temporal information BIBREF3 and have applied them to find equivalent entities between multi-language datasets BIBREF4 . Since the",
      "try, ComplEx cannot infer composition rules. Here, we showed a general method to override these limitations of the older models. We demonstrated and validated our contributions via both theoretical proofs and empirical",
      " learn several relational patterns, 3. It is extendable, 4. It shows competitive performance in the empirical evaluations and 5. We also develop an algorithm to find the limits for the limit-based loss function to use in embedding models.",
      "ws to the input samples is to incorporate the different formulations of samples from the different models as one learning model. In contrast to ensemble approaches that incorporate models by training independently and testing together, multi-objective optimization models (MOE) BIBREF23 join in the minimization step.",
      "xpressive. Besides MDE, BIBREF11 and BIBREF10 , most of the existing models are unable to model all the three relation patterns. Indeed, TransE cannot model the symmetry pattern, DisMult has a problem learning the antisymmetry, ComplEx cannot infer composition rules. Here, we showed a general method to override these limitations of the older models.",
      "nowledge graphs and the expansion of the web, it is crucial that the time and memory complexity of a relational mode be minimal. Despite the limitations in expressivity, TransE is one of the popular models on large datasets due to its scalability.",
      " minimal. Despite the limitations in expressivity, TransE is one of the popular models on large datasets due to its scalability. With $O(d)$ time complexity, where d is the size of embedding vectors, it is more efficient than RESCAL, NTN and the neural network models. Similar to TransE, the time complexity of MDE is $O(d)$ .",
      "We implemented MDE in PyTorch. Following BIBREF18 , we generated one negative example per positive example for all the datasets. We used Adadelta BIBREF30 as the optimizer and fine-tuned the hyperparameters on the validation dataset. The ranges of the hyperparameters are set as follows: embedding dimension 25, 50, 100, batch size 100, 150, iterations 1000, 1500, 2500, 3600.",
      "models with an experiment of TransE and RotatE on FB15k-237 that applies 256 negative samples per one positive sample BIBREF11 . Although these models perform better than the TransE on FB15K(Table1), they produce lower rankings on FB15k-237(Table2) in the more fair comparison conditions."
    ],
    "answer": [
      "BIBREF11, BIBREF35, BIBREF2, BIBREF3, BIBREF4, BIBREF23, BIBREF10, BIBREF18, BIBREF30"
    ]
  },
  {
    "title": "Gunrock: A Social Bot for Complex and Engaging Long Conversations",
    "evidence": [
      " ratings, number of turns). Additionally, users' backstory queries about Gunrock are positively correlated to user satisfaction. Finally, we found dialog flows that interleave facts and personal opinions and stories lead to better user satisfaction.",
      "ts these capabilities. Longer sentences and more questions about Gunrocks's backstory positively correlate with user experience. Additionally, we find evidence for interleaved dialog flow, where combining factual information with personal opinions and stories improve user satisfaction.",
      "depth conversations in open domains. In this paper, we introduce some innovative system designs and related validation analysis. Overall, we found that users produce longer sentences to Gunrock, which are directly related to users' engagement (e.g., ratings, number of turns). Additionally, users' backstory queries about Gunrock are positively correlated to user satisfaction.",
      " users who show greater curiosity about Gunrock will display higher overall ratings for the conversation based on her responses. Overall, the number of times users queried Gunrock's backstory was strongly related to the rating they gave at the end of the interaction (log:$\\beta $=0.10, SE=0.002, t=58.4, p$<$0.001)(see Figure 3).",
      "nd encourages users to be more active in a conversation. Analysis shows that users' speech behavior reflects these capabilities. Longer sentences and more questions about Gunrocks's backstory positively correlate with user experience.",
      "r's interest in Gunrock by tagging instances where the user triggered Gunrock's backstory (e.g., “What's your favorite color?\"). For users with at least one backstory question, we modeled overall (log) Rating with a linear regression by the (log) `Number of Backstory Questions Asked' (log transformed due to the variables' nonlinear relationship).",
      "ar regression by the (log) `Number of Backstory Questions Asked' (log transformed due to the variables' nonlinear relationship). We hypothesized that users who show greater curiosity about Gunrock will display higher overall ratings for the conversation based on her responses.",
      "ly related to the rating they gave at the end of the interaction (log:$\\beta $=0.10, SE=0.002, t=58.4, p$<$0.001)(see Figure 3). This suggests that maintaining a consistent personality — and having enough responses to questions the users are interested in — may improve user satisfaction.",
      "an 13.0), with an average of 6.98 words per utterance, and had an average conversation time of 7.33 minutes (median: 2.87 min.). We conducted three principal analyses: users' response depth (wordcount), backstory queries (backstorypersona), and interleaving of personal and factual responses (pets).",
      "results\nsuggest that users are engaging with Gunrock in similar ways to other humans: in chitchat about general topics (e.g., animals, movies, etc.), taking interest in Gunrock's backstory and persona, and even producing more information about themselves in return.",
      " more talkative may enjoy talking to the bot in general, and thus give higher ratings in tandem with higher average word counts. Analysis ::: Gunrock's Backstory and Persona We assessed the user's interest in Gunrock by tagging instances where the user triggered Gunrock's backstory (e.g., “What's your favorite color?\").",
      "at combining facts and personal questions — in this case about the user's pet — would lead to greater user satisfaction overall. We extracted conversations where Gunrock asked the user if they had ever had a pet and categorized responses as “Yes\", “No\", or “NA\" (if users did not respond with an affirmative or negative response).",
      "have you had Oliver?\"). In cases where the user does not indicate that they have a pet, the system solely provides animal facts. Therefore, the animal module can serve as a test of our interleaving strategy: we hypothesized that combining facts and personal questions — in this case about the user's pet — would lead to greater user satisfaction overall."
    ],
    "answer": [
      "0.10, SE=0.002, t=58.4, p<0.001)"
    ]
  },
  {
    "title": "Image Captioning: Transforming Objects into Words",
    "evidence": [
      "her metrics BIBREF23 , BIBREF22 , the common practice in the image captioning literature is to report all the mentioned metrics. Comparative Analysis We compare our proposed algorithm against the best",
      "Abstract\nImage captioning models typically follow an encoder-decoder architecture which uses",
      "orating spatial relationship information, most evidently when comparing the relevant sub-metrics of the SPICE captioning metric. We also present qualitative examples of how incorporating this information can yield captioning",
      "results\ndemonstrate the importance of such geometric attention for image captioning, leading to improvements on all common captioning metrics on the MS-COCO dataset.",
      "valuate our models using the CIDEr-D BIBREF22 , SPICE BIBREF23 , BLEU BIBREF24 , METEOR BIBREF25 , and ROUGE-L BIBREF26 metrics. While it has been shown experimentally that BLEU and ROUGE have lower correlation with human judgments than the other metrics BIBREF23 , BIBREF22 , the common practice in the image captioning literature is to report all the mentioned metrics.",
      " improved convolutional neural network and object detection architectures have contributed to improved image captioning systems. On the natural language processing side, more sophisticated sequential models, such as attention-based recurrent neural networks, have similarly resulted in more accurate image caption generation.",
      "al models, such as attention-based recurrent neural networks, have similarly resulted in more accurate image caption generation. Inspired by neural machine translation, most conventional image captioning systems utilize an encoder-decoder framework, in which an input image is encoded into an intermediate representation of the information contained within the image, and subsequently decoded into a descriptive text sequence.",
      "n other image captioning publications. The dataset contains 113K training images with 5 human annotated captions for each image. The Karpathy test and validation sets contain 5K images each. We evaluate our models using the CIDEr-D BIBREF22 , SPICE BIBREF23 , BLEU BIBREF24 , METEOR BIBREF25 , and ROUGE-L BIBREF26 metrics.",
      "re evaluated after training for 30 epochs, with ADAM optimization with the above learning rate schedule, and with batch size 15. Dataset and Metrics We trained and evaluated our algorithm on the Microsoft COCO (MS-COCO) 2014 Captions dataset BIBREF21 . We report",
      "ayers. In addition, skip-connections and layer-norm are applied to the outputs of the self-attention and the feed-forward layer. The decoder then uses the generated tokens from the last encoder layer as input to generate the caption text.",
      "eed-forward layer. The decoder then uses the generated tokens from the last encoder layer as input to generate the caption text. Since the dimensions of the output tokens of the Transformer encoder are identical to the tokens used in the original Transformer implementation, we make no modifications on the decoder side.",
      "results\non the Karpathy validation and test splits BIBREF8 , which are commonly used in other image captioning publications. The dataset contains 113K training images with 5 human annotated captions for each image.",
      "gorithm. We first use an object detector to extract appearance and geometry features from all the detected objects in the image. Thereafter we use the Object Relation Transformer to generate the caption text. Section SECREF7 describes how we use the Transformer architecture BIBREF4 in general for image captioning."
    ],
    "answer": [
      "CIDEr-D, SPICE, BLEU, METEOR, and ROUGE-L."
    ]
  },
  {
    "title": "RC-QED: Evaluating Natural Language Derivations in Multi-Hop Reading Comprehension",
    "evidence": [
      "odels. Related work ::: RC datasets with explanations There exists few RC datasets annotated with explanations (Table TABREF50). The most similar work to ours is Science QA dataset BIBREF21, BIBREF22, BIBREF23, which provides a small set of NLDs annotated for analysis purposes.",
      " E}$@!END@ ::: Dataset Our study uses WikiHop BIBREF0, as it is an entity-based multi-hop QA dataset and has been actively used. We randomly sampled 10,000 instances from 43,738 training instances and 2,000 instances from 5,129 validation instances (i.e. 36,000 annotation tasks were published on AMT). We manually converted structured WikiHop question-answer pairs (e.g.",
      " to the path attention: Experiments ::: Settings ::: Dataset We aggregated crowdsourced annotations obtained in Section SECREF3. As a preprocessing, we converted the NLD annotation to Unsure if the derivation contains the phrase needs to be mentioned. This is due to the fact that annotators misunderstand our instruction.",
      " by the number of tokens in its corresponding justification sentences. This indicates that annotated NLDs are indeed summarized. See Table TABREF53 in Appendix and Supplementary Material for more",
      "y to Unanswerable and discard NLD annotations. Otherwise, we employ all NLD annotations from workers as multiple reference NLDs. The statistics is shown in Table TABREF36. Regarding $\\mathcal {K}^+, \\mathcal {K}^-$, we extracted 867,936 instances from the training set of WikiHop BIBREF0. We reserve 10% of these instances as a validation set to find the best model.",
      " to ours is Science QA dataset BIBREF21, BIBREF22, BIBREF23, which provides a small set of NLDs annotated for analysis purposes. By developing the scalable crowdsourcing framework, our work provides one order-of-magnitude larger NLDs which can be used as a benchmark more reliably. In addition, it provides the community with new types of challenges not included in HotpotQA.",
      "-qed/. One immediate future work is to expand the annotation to non-entity-based multi-hop QA datasets such as HotpotQA BIBREF2. For modeling, we plan to incorporate a generative mechanism based on recent advances in conditional language modeling. Example annotations Table TABREF53 shows examples of crowdsourced annotations.",
      " corpus of NLDs. The developed crowdsourcing annotation framework can be used for annotating other QA datasets with derivations. Our experiments using two simple baseline models have demonstrated that RC-QED$^{\\rm E}$ is a non-trivial task, and that it indeed provides a challenging task of extracting and synthesizing relevant facts from supporting documents.",
      "ation sentences. As discussed earlier, we take a step further from justification explanations to provide new challenges for NLU. Several datasets are annotated with introspective explanations, ranging from textual entailments BIBREF8 to argumentative texts BIBREF26, BIBREF27, BIBREF33. All these datasets offer the classification task of single sentences or sentence pairs.",
      "Results\n::: Quality To evaluate the quality of annotation",
      "abstract\niveness of annotated NLDs ($a$), namely the number of tokens in an NLD divided by the number of tokens in its corresponding justification sentences. This indicates that annotated NLDs are indeed summarized.",
      "age derivations. The developed crowdsourcing annotation framework can be used for annotating other QA datasets with derivations. Through an experiment using two baseline models, we highlight several challenges of RC-QED. We will make the corpus of reasoning annotations and the baseline system publicly available at https://naoya-i.github.io/rc-qed/.",
      "Results\nTable TABREF17 shows the statistics of responses and example annotations. Table TABREF17 also shows the"
    ],
    "answer": [
      "The dataset was annotated using a crowdsourcing framework, where 36,000 annotation tasks were published on AMT, and 10,000 instances were randomly sampled from the training set and 2,000 instances from the validation set of WikiHop."
    ]
  },
  {
    "title": "Word Embeddings to Enhance Twitter Gang Member Profile Identification",
    "evidence": [
      "panded search of their retweet, friends and follower networks, led to identifying 400 authentic gang member profiles on Twitter. Our study discovered that the text in their tweets and profile descriptions, their emoji use, their profile images, and music interests embodied by links to YouTube music videos, can help a classifier distinguish between gang and non-gang member profiles.",
      "nterests embodied by links to YouTube music videos, can help a classifier distinguish between gang and non-gang member profiles. While a very promising INLINEFORM0 measure with low false positive rate was achieved, we hypothesize that the diverse kinds and the multitude of features employed (e.g.",
      "e gang members' Twitter profile names to be known beforehand, and data collection was localized to a single city in the country. These studies investigated a small set of manually curated gang member profiles, often from a small geographic area that may bias their findings.",
      " presented a word embeddings-based approach to address the problem of automatically identifying gang member profiles on Twitter. Using a Twitter user dataset that consist of 400 gang member and 2,865 non gang member profiles, we trained word embedding models based on users' tweets, profile descriptions, emoji, images, and videos shared on Twitter (textual features extracted from images, and videos).",
      "sets. A dataset of over 3,000 gang and non-gang member profiles that we previously curated is used to train the word embeddings. We show that pre-trained word embeddings improve the machine learning models and help us obtain an INLINEFORM0 -score of INLINEFORM1 on gang member profiles (a 6.39% improvement in INLINEFORM2 -score compared to the baseline models which were not trained using word embeddings).",
      "Conclusion\nand Future Work This paper presented a word embeddings-based approach to address the problem of automatically identifying gang member profiles on Twitter.",
      "results\nshow that pre-trained word embeddings can boost the accuracy of supervised learning algorithms trained over gang members social media posts.",
      "ember profiles to study how gang member Twitter profiles can be automatically identified based on the content they share online. A data collection process involving location neutral keywords used by gang members, with an expanded search of their retweet, friends and follower networks, led to identifying 400 authentic gang member profiles on Twitter.",
      "e of feature in the dataset. It includes a total of 821,412 tweets from gang members and 7,238,758 tweets from non-gang members. To build the classifiers we used three different learning algorithms, namely Logistic Regression (LR), Random Forest (RF), and Support Vector Machines (SVM).",
      "vestigation of gang-related crimes. This paper investigates the use of word embeddings to help identify gang members on Twitter. Building on our previous work, we generate word embeddings that translate what Twitter users post in their profile descriptions, tweets, profile images, and linked YouTube content to a real vector format amenable for machine learning classification.",
      ". Consequently, building a database of keywords, phrases, and other identifiers to find gang members nationally is not feasible. Instead, we use heterogeneous sets of features derived not only from profile and tweet text but also from the emoji usage, profile images, and links to YouTube videos reflecting their music preferences and affinity.",
      "member profiles found by searching for seed words. Specific details about our data curation procedure are discussed in BIBREF9 . Ultimately, this dataset consists of 400 gang member profiles and 2,865 non-gang member profiles.",
      "results\nto baseline Model(2) or beat it. This evaluation demonstrates the promise of using pre-trained word embeddings to boost the accuracy of supervised learning algorithms for Twitter gang member profile classification."
    ],
    "answer": [
      "The ground truth of gang membership in this dataset is established through a data collection process involving location-neutral keywords used by gang members, with an expanded search of their retweet, friends, and follower networks, leading to the identification of 400 authentic gang member profiles on Twitter."
    ]
  },
  {
    "title": "Recurrently Controlled Recurrent Networks",
    "evidence": [
      "NNs of approximately equal parameterization. There are several potential interesting directions for further investigating RCRNs. Firstly, investigating RCRNs controlling other RCRNs and secondly, investigating RCRNs in other domains where recurrent models are also prevalent for sequence modeling. The source code of our model can be found at https://github.com/vanzytay/NIPS2018_RCRN.",
      "all 26 datasets, RCRN outperforms not only standard BiLSTMs but also 3L-BiLSTMs which have approximately equal parameterization. 3L-BiLSTMs were overall better than BiLSTMs but lose out on a minority of datasets. RCRN outperforms a wide range of competitive baselines such as DiSAN, Bi-SRUs, BCN and LSTM-CNN, etc.",
      "ents constant, 3L-BiLSTM has equal parameters to RCRN while RCRN and 3L-BiLSTM are approximately three times larger than BiLSTM. Experiments This section discusses the overall empirical evaluation of our proposed RCRN model.",
      "ement of INLINEFORM0 on all four metrics. Note that our model only uses a single layered RCRN while R-NET uses 3 layered BiGRUs. This empirical evidence might suggest that RCRN is a better way to utilize multiple recurrent layers. Across all 26 datasets, RCRN outperforms not only standard BiLSTMs but also 3L-BiLSTMs which have approximately equal parameterization.",
      "Introduction\nRecurrent neural networks (RNNs) live at the heart of many sequence modeling problems.",
      "ered Quasi Recurrent Neural Networks (QRNN) BIBREF29 and the BCN model which can be considered to be very competitive baselines. RCRN also outperforms ablative baselines BiLSTM ( INLINEFORM1 ) and 3L-BiLSTM ( INLINEFORM2 ). Our",
      "ual stacked BiLSTM model which RCRN outperforms ( INLINEFORM1 ). Our model also outperforms BCN (+0.4%) and SRU ( INLINEFORM2 ). Our ablative BiLSTM baselines achieve reasonably high score, posssibly due to CoVe Embeddings. However, our RCRN can further increase the performance score.",
      "re also prevalent for sequence modeling. The source code of our model can be found at https://github.com/vanzytay/NIPS2018_RCRN. Acknowledgements We thank the anonymous reviewers and area chair from NIPS 2018 for their constructive and high quality feedback.",
      "cxt + U2ch2t-1 + b2c) h1t = o1t (c1t) and h2t = o2t (c2t) where INLINEFORM0 is the input to the model at time step INLINEFORM1 . INLINEFORM2 are the parameters of the model where INLINEFORM3 and INLINEFORM4 . INLINEFORM5 is the sigmoid function and INLINEFORM6 is the tanh nonlinearity. INLINEFORM7 is the Hadamard product.",
      "results\nreported in BIBREF45 . Overall, we observe that RCRN is much stronger than BiLSTMs and 3L-BiLSTMs on this task.",
      "tention-only models may come close in performance, some domains may still require the complex and expressive recurrent encoders. Moreover, we note that in BIBREF25 , BIBREF26 , the scores on multiple benchmarks (e.g., SST, TREC, SNLI, MultiNLI) do not outperform (or even approach) the state-of-the-art, most of which are models that still heavily rely on bidirectional LSTMs BIBREF27 , BIBREF20 , BIBREF5 , BIBREF10 .",
      "nchmark datasets. Overall findings suggest that our controller-listener architecture is more effective than stacking RNN layers. Moreover, RCRN remains equally (or slightly more) efficient compared to stacked RNNs of approximately equal parameterization. There are several potential interesting directions for further investigating RCRNs.",
      "results\ndemonstrate that RCRN not only consistently outperforms BiLSTMs but also stacked BiLSTMs, suggesting that our controller architecture might be a suitable replacement for the widely adopted stacked architecture."
    ],
    "answer": [
      "The RCRN model has more parameters than BiLSTMs and 3L-BiLSTMs, but remains equally (or slightly more) efficient compared to stacked RNNs of approximately equal parameterization."
    ]
  },
  {
    "title": "Contextual Recurrent Units for Cloze-style Reading Comprehension",
    "evidence": [
      "ling the sequence. We have tested our CRU model on the cloze-style reading comprehension task and sentiment classification task. Experimental",
      "eling. In the sentiment classification task, we build a standard neural network and replace the recurrent unit by our CRU model. To further demonstrate the effectiveness of our model, we also tested our CRU in reading comprehension tasks with a strengthened baseline system originated from Attention-over-Attention Reader (AoA Reader) BIBREF10. Experimental",
      "against other sequential models. The detailed experimental result of sentiment classification will be given in the next section. Applications ::: Reading Comprehension Besides the sentiment classification task, we also tried our CRU model in cloze-style reading comprehension, which is a much complicated task.",
      "and cloze-style reading comprehension. In the sentiment classification task, we build a simple neural model and applied our CRU. In the cloze-style reading comprehension task, we first present some modifications to a recent reading comprehension model, called AoA Reader BIBREF10, and then replace the GRU part by our CRU model to see if our model could give substantial improvements over strong baselines.",
      "lassification and reading comprehension, where the former is sentence-level modeling, and the latter is document-level modeling. In the sentiment classification task, we build a standard neural network and replace the recurrent unit by our CRU model.",
      "tasks, in this paper, we applied the CRU model to two NLP tasks: sentiment classification and cloze-style reading comprehension. In the sentiment classification task, we build a simple neural model and applied our CRU.",
      "eq(x)$ and $CoQ(x)$ are the features that introduced above. Other parts of the model remain the same as the original AoA Reader. For simplicity, we will omit this part, and the detailed illustrations can be found in BIBREF10. Experiments: Sentiment Classification ::: Experimental Setups In the sentiment classification task, we tried our model on the following public datasets.",
      "r function of CNN in all experiments. We use 10-fold cross-validation (CV) in the dataset that has no train/valid/test division. Experiments: Sentiment Classification :::",
      "the recurrent units to enhance the ability to model the local context and reducing word ambiguities even in bi-directional RNNs. We tested our CRU model on sentence-level and document-level modeling NLP tasks: sentiment classification and reading comprehension. Experimental",
      "lassification ::: Experimental Setups In the sentiment classification task, we tried our model on the following public datasets. [leftmargin=*] MR Movie reviews with one sentence each. Each review is classified into positive or negative BIBREF18. IMDB Movie reviews from IMDB website, where each movie review is labeled with binary classes, either positive or negative BIBREF19.",
      " a straightforward neural architecture to this task, as we purely want to compare our CRU model against other sequential models. The detailed experimental result of sentiment classification will be given in the next section.",
      "ntiment classification task, we also tried our CRU model in cloze-style reading comprehension, which is a much complicated task. In this paper, we strengthened the recent AoA Reader BIBREF10 and applied our CRU model to see if we could obtain substantial improvements when the baseline is strengthened.",
      "turing long-term dependencies. We propose three variants of our CRU model: shallow fusion, deep fusion and deep-enhanced fusion. To verify the effectiveness of our CRU model, we utilize it into two different NLP tasks: sentiment classification and reading comprehension, where the former is sentence-level modeling, and the latter is document-level modeling."
    ],
    "answer": [
      "MR",
      "IMDB"
    ]
  },
  {
    "title": "Saliency Maps Generation for Automatic Text Summarization",
    "evidence": [
      "ortant as the generation step, if not more. The Task and the Model We present in this section the baseline model from See et al. See2017 trained on the CNN/Daily Mail dataset. We reproduce the",
      "results\nthat are qualitatively equivalent to the",
      "results\nas well as mathematically justifying the average that we did when validating our saliency maps. Some additional work is also needed on the validation of the saliency maps with counterfactual tests.",
      "nt by themselves, and that we need to define the counter-factual case and test it to measure how truthful the saliency maps are. Future work would look into the saliency maps generated by applying LRP to pointer-generator networks and compare to our current",
      "to 400 words and generate summaries of 200 words. We pad the shorter texts using an UNKNOWN token and truncate the longer texts. We embed the texts and summaries using a vocabulary of size 50 000, thus recreating the same parameters as See et al. See2017. The Model The baseline model is a deep sequence-to-sequence encoder/decoder model with attention.",
      " to rigorously check importance attributions before trusting them, regardless of whether or not the mapping “makes sense\" to us. We finally argue that in the process of identifying the important input features, verifying the saliency maps is as important as the generation step, if not more. The Task and the Model We present in this section the baseline model from See et al.",
      "he network focused on. But we also showed that in some cases the saliency maps seem to not capture the important input features. This brought us to discuss the fact that these attributions are not sufficient by themselves, and that we need to define the counter-factual case and test it to measure how truthful the saliency maps are.",
      "y maps for the words in the output summary: they are almost all identical and seem uncorrelated with the attention distribution. We then proceeded to validate our attributions by averaging the absolute value of the relevance across the saliency maps. We obtain a ranking of the word from the most important to the least important and proceeded to delete one or another.",
      " from being valid summaries of the information in the texts but are sufficient to look at the attribution that LRP will give us. They pick up the general subject of the original text.",
      "ameters as See et al. See2017. The Model The baseline model is a deep sequence-to-sequence encoder/decoder model with attention. The encoder is a bidirectional Long-Short Term Memory(LSTM) cell BIBREF14 and the decoder a single LSTM cell with attention mechanism. The attention mechanism is computed as in BIBREF9 and we use a greedy search for decoding.",
      "conclusion\ns from the attribution generated. One interesting point is that one saliency map didn't look “better\" than the other, meaning that there is no apparent way of determining their truthfulness in regard of the computation without doing a quantitative validation.",
      "ncy maps. We obtain a ranking of the word from the most important to the least important and proceeded to delete one or another. We showed that in some cases the saliency maps are truthful to the network's computation, meaning that they do highlight the input features that the network focused on.",
      "(see Figure 4 ), but for other test examples we don't observe a significant difference between the two settings (see Figure 5 ). One might argue that the second summary in Figure 5 is better than the first one as it makes better sentences but as the model generates inaccurate summaries, we do not wish to make such a statement."
    ],
    "answer": [
      "See et al. See2017, the CNN/Daily Mail dataset, and a deep sequence-to-sequence encoder/decoder model with attention."
    ]
  },
  {
    "title": "Align, Mask and Select: A Simple Method for Incorporating Commonsense Knowledge into Language Representation Models",
    "evidence": [
      "that the multi-choice QA task works better than the masked LM task as the pre-training task for the target multi-choice QA task. We argue that, for the masked LM task, BERT_CS is required to predict each masked wordpieces (in concepts) independently and for the multi-choice QA task, BERT is required to model the whole candidate phrases.",
      "he questions based on their knowledge about the world, it is a great challenge for machines when there is limited training data. We hypothesize that exploiting knowledge graphs for commonsense in QA modeling can help model choose correct answers. For example, as shown in the part B of Table 1 , some triples from ConceptNet BIBREF11 are quite related to the questions above.",
      "ings. These more confusing candidates force BERT_CS to distinguish synonym meanings, resulting in a more powerful BERT_CS model. Comparing model 5 and model 6, we find that the multi-choice QA task works better than the masked LM task as the pre-training task for the target multi-choice QA task.",
      "RT_CS models on the WSC dataset. We transform the pronoun disambiguation problem into a multi-choice question answering problem. We mask the pronoun word with a special token [QW] to construct a question, and put the two candidate paragraphs as candidate answers. The remaining procedures are the same as QA tasks.",
      "pt $_2$ /concept $_1$ and the relation with the correct answers. We denote the resulting model from this dataset BERT_CS_random. Instead of pre-training BERT with a multi-choice QA task that chooses the correct answer from several candidate answers, we mask concept $_1$ and concept $_2$ and pre-train BERT with a masked language model (MLM) task.",
      " than human accuracy on the questions about commonsense knowledge. Some examples from CommonsenseQA are shown in Table 1 part A. As can be seen from the examples, although it is easy for humans to answer the questions based on their knowledge about the world, it is a great challenge for machines when there is limited training data.",
      "et to select the distractors instead of random selection, where $_2$2 is a wildcard character that can match any word or phrase. For each question, we reserve four distractors and one correct answer. If there are less than four matched distractors, we discard this question instead of complementing it with random selection.",
      " this paper, instead of employing the relation labels labeled by distant supervision, we focus on the aligned entities/concepts. We propose the AMS method to construct a multi-choice QA dataset that align sentences with commonsense knowledge triples, mask the aligned words (entities/concepts) in sentences and treat the masked sentences as questions, and select several entities/concepts from knowledge graphs as candidate choices.",
      " we conduct experiments on a commonsense-related multi-choice question answering benchmark, the CommonsenseQA dataset BIBREF10 . The CommonsenseQA dataset consists of 12,247 questions with one correct answer and four distractor answers. This dataset consists of two splits – the question token split and the random split.",
      "te answers share the same (concept $_1$ , relation) or (relation, concept $_2$ ), that is, these candidates have close meanings. These more confusing candidates force BERT_CS to distinguish synonym meanings, resulting in a more powerful BERT_CS model.",
      "ample in the dataset consists of a question and several candidate answers, which has the same form as the CommonsenseQA dataset. An example of constructing one training sample by masking concept $_2$ is shown in Table 2 .",
      "rom several candidate answers, we mask concept $_1$ and concept $_2$ and pre-train BERT with a masked language model (MLM) task. We denote the resulting model from this pre-training task BERT_MLM. We randomly mask 15% WordPiece tokens BIBREF27 of the question as in BIBREF4 and then conduct both multi-choice QA task and MLM task simultaneously.",
      "results\ndemonstrate that our proposed multi-choice QA pre-training task does not degrade the sentence representation capabilities of BERT models."
    ],
    "answer": [
      "The multi-choice QA task is selected as the pre-training task for the target multi-choice QA task because it allows BERT to model the whole candidate phrases, whereas the masked LM task requires BERT to predict each masked wordpiece independently."
    ]
  },
  {
    "title": "Binary and Multitask Classification Model for Dutch Anaphora Resolution: Die/Dat Prediction",
    "evidence": [
      "t a different place in the sentence. Each sentence is paired with its automatically assigned ground truth label for die and dat. The Europarl dataset, on the one hand, contains 70,057 dat-labeled and 33,814 die-labeled sentences. The resulting train and test sets consist of 103,871 (Europarl) and 1,269,091 (SoNaR) sentences.",
      "results\nfor die/dat prediction is given in Table TABREF19. The same dataset settings as for the binary classification model are used: full in which the datasets contain full sentences, windowed in which sentences are windowed around the unique prediction token without exceeding sentence boundaries (five tokens before and after the token, including token), and windowed no_boundaries in which the windows can exceed sentence boundaries.",
      "d 33,814 die-labeled sentences. The resulting train and test sets consist of 103,871 (Europarl) and 1,269,091 (SoNaR) sentences. The SoNaR dataset, on the other hand, has more than ten times the number of labeled sentences with 736,987 dat-labeled and 532,104 die-labeled.",
      "he extra bidirectional LSTM layer, the influence of the second prediction task and/or the split in sentence and context encoder. Firstly, the data is divided into batch sizes of 512 instead of 128. Table TABREF22 shows, however, that there is little consistent difference in performance when batch size is 512 or 128.",
      "results\nis given in Table TABREF11. We compare model performance when trained and tested on the two corpora individually and experiment with different settings of the two corpora in order to investigate the effect of dataset changes on model performance.",
      "weights are initialized by means of the 200-dimensional pre-trained embedding matrix. The weights are updated after every epoch. The second layer consists of two bidirectional LSTMs where the output of the first bidirectional LSTM serves as input to the second bidirectional LSTM. The layer has dropout regularization equal to 0.2.",
      "xperiment with different settings of the two corpora in order to investigate the effect of dataset changes on model performance. There are three settings: full in which the datasets contain full sentences, windowed in which sentences are windowed around the unique prediction token without exceeding sentence boundaries (five tokens before and after the token, including token), and windowed no_boundaries in which the windows can exceed sentence boundaries.",
      " higher number of dat than die instances in training, validation and test datasets extracted from the Europarl and SoNaR corpus. Secondly, when the dataset is more balanced, as in the SoNaR corpus, the difference in performance between die and dat labels decreases as expected.",
      "et is used for training and testing. The dataset is randomly divided into a training (70%), validation (15%) and test (15%) set. The data is fed into the model in batches of 516 samples and the data is reshuffled at every epoch. For die/dat prediction, the Binary Cross-Entropy loss function is minimized.",
      "he first bidirectional LSTM serves as input to the second bidirectional LSTM. The layer has dropout regularization equal to 0.2. The two-layer bidirectional LSTM concatenates the outputs at time $t$ into a 64-dimensional vector and sends it through a maxpooling layer. Until this point, the two task share the same parameters. The model than splits into two separate linear layers.",
      "nd tested on the Europarl (2) and SoNaR (3) windowed datasets is particularly noticeable in the precision, recall and F1 scores. Model performance for dat prediction is better for the Europarl dataset than for the SoNaR dataset, while model performance for die prediction is notably better for the SoNaR dataset than for the Europarl dataset.",
      " of 128. Table TABREF22 shows, however, that there is little consistent difference in performance when batch size is 512 or 128. Therefore, it can be suggested that an increased batch size has no directly positive influence on model performance. Secondly, the input data is transformed to 200-dimensional word embeddings instead of 100-dimensional word embeddings. From the",
      "ataset is more balanced, as in the SoNaR corpus, the difference in performance between die and dat labels decreases as expected. Thirdly, die/dat prediction performance increases when the window over the sentences is not limited to sentence boundaries (SoNaR windowed, no_boundaries)."
    ],
    "answer": [
      "The Europarl dataset contains 70,057 dat-labeled and 33,814 die-labeled sentences. The SoNaR dataset contains 736,987 dat-labeled and 532,104 die-labeled sentences."
    ]
  },
  {
    "title": "Seeing Things from a Different Angle: Discovering Diverse Perspectives about Claims",
    "evidence": [
      "mportant issues such as their degree of factuality BIBREF8 or trustworthiness BIBREF44 , BIBREF1 as separate aspects of problem. We hope that some of these challenges and limitations will be addressed in future work.",
      "more challenging. We make extensive use of crowdsourcing to increase the quality of the data and clean it from annotation noise. The contributions of this paper are as follows: Design Principles and Challenges In this section we provide a closer look into the challenge and propose a collection of tasks that move us closer to substantiated perspective discovery.",
      "dy a setting that would facilitate discovering diverse perspectives and their supporting evidence with respect to a given claim. Our goal is to identify and formulate the key NLP challenges underlying this task, and develop a dataset that would allow a systematic study of these challenges.",
      "late the key NLP challenges underlying this task, and develop a dataset that would allow a systematic study of these challenges. For example, for the claim in Figure FIGREF1 , multiple (non-redundant) perspectives should be retrieved from a pool of perspectives; one of them is “animals have no interest or rationality”, a perspective that should be identified as taking an opposing stance with respect to the claim.",
      "Conclusion\nThe importance of this work is three-fold; we define the problem of substantiated perspective discovery and characterize language understanding tasks necessary to address this problem.",
      "show that human baselines across multiple subtasks far outperform ma-chine baselines built upon state-of-the-art NLP techniques. This poses a challenge and opportunity for the NLP community to address.",
      " in order to drive research on this problem. Finally, we build and evaluate strong baseline supervised systems for this problem. Our hope is that this dataset would bring more attention to this important problem and would speed up the progress in this direction. There are two aspects that we defer to future work.",
      "a system is expected to recognize argumentative sentences BIBREF4 that directly address the points raised in the disputed claim. For example, while the perspectives in Figure FIGREF6 are topically related to the claims, INLINEFORM0 do not directly address the focus of claim INLINEFORM1 (i.e., “use of animals” in “entertainment”).",
      " problem of substantiated perspective discovery and characterize language understanding tasks necessary to address this problem. We combine online resources, web data and crowdsourcing and create a high-quality dataset, in order to drive research on this problem. Finally, we build and evaluate strong baseline supervised systems for this problem.",
      "fer Sheffield, Stephen Mayhew, Shyam Upadhyay, Nitish Gupta and the anonymous reviewers for insightful comments and suggestions. This work was supported in part by a gift from Google and by Contract HR0011-15-2-0025 with the US Defense Advanced Research Projects Agency (DARPA).",
      "Acknowledgment\ns The authors would like to thank Jennifer Sheffield, Stephen Mayhew, Shyam Upadhyay, Nitish Gupta and the anonymous reviewers for insightful comments and suggestions.",
      "ur dataset contains 1k claims, accompanied with pools of 10k and 8k perspective sentences and evidence paragraphs, respectively. We provide a thorough analysis of the dataset to highlight key underlying language understanding challenges, and show that human baselines across multiple subtasks far outperform ma-chine baselines built upon state-of-the-art NLP techniques.",
      "im, presenting a small but diverse set of perspectives could be an important step towards addressing the selection bias problem. Moreover, it would be impractical to develop an exhaustive pool of evidence for all perspectives, from a diverse set of credible sources. We are not attempting to do that."
    ],
    "answer": [
      "Key challenges highlighted in the dataset include:\n- selection bias problem\n- developing an exhaustive pool of evidence for all perspectives from a diverse set of credible sources\n- identifying and formulating key NLP challenges underlying the task of substantiated perspective discovery\n- recognizing argumentative sentences that directly address the points raised in the disputed claim\n- addressing the problem of substantiated perspective discovery and characterizing language understanding tasks necessary to address this problem."
    ]
  },
  {
    "title": "Unsupervised Machine Commenting with Neural Variational Topic Model",
    "evidence": [
      "s based on a retrieval-based commenting framework, which uses news to retrieve comments based on the similarity of their topics. The topic representation is obtained from a neural variational topic model, which is trained in an unsupervised manner. We evaluate our model on a news comment dataset.",
      "s is the requirement of a large parallel dataset. However, the naturally paired commenting dataset is loosely paired. Qin et al. QinEA2018 were the first to propose the article commenting task and an article-comment dataset. The dataset is crawled from a news website, and they sample 1,610 article-comment pairs to annotate the relevance score between articles and comments.",
      "trieval-based commenting framework, which uses the news as the query to retrieve the comments by the similarity of their topics. The topic is represented with a variational topic, which is trained in an unsupervised manner.",
      " present a Yahoo news comment threads dataset and annotation scheme for the new task of identifying “good” online conversations. More recently, Kolhaatkar and Taboada KolhatkarT17 propose a model to classify the comments into constructive comments and non-constructive comments.",
      " which is one of the most popular Chinese websites for news and opinion articles. The dataset consists of 198,112 news articles. Each piece of news contains a title, the content of the article, and a list of the users' comments.",
      "elect a large-scale Chinese dataset BIBREF0 with millions of real comments and a human-annotated test set to evaluate our model. The dataset is collected from Tencent News, which is one of the most popular Chinese websites for news and opinion articles. The dataset consists of 198,112 news articles.",
      "t sets, and they contain 191,502/5,000/1,610 pieces of news, respectively. The whole dataset has a vocabulary size of 1,858,452. The average lengths of the article titles and content are 15 and 554 Chinese words. The average comment length is 17 words. Implementation Details The hidden size of the model is 512, and the batch size is 64. The number of topics INLINEFORM0 is 100.",
      "gh quality comments. Napoles et al. NapolesTPRP17 propose to discriminating engaging, respectful, and informative conversations. They present a Yahoo news comment threads dataset and annotation scheme for the new task of identifying “good” online conversations.",
      "a (e.g. Twitter) are about recent news, but require redundant work to match these comments with the corresponding news articles. With the help of the unsupervised learning method, the model can also learn to generate these interesting comments.",
      "ts of 198,112 news articles. Each piece of news contains a title, the content of the article, and a list of the users' comments. Following the previous work BIBREF0 , we tokenize all text with the popular python package Jieba, and filter out short articles with less than 30 words in content and those with less than 20 comments.",
      " not directly tell what happened in the news, but talk about the underlying topics (e.g. NBA Christmas Day games, LeBron James). However, existing methods for machine commenting do not model the topics of articles, which is a potential harm to the generated comments. To this end, we propose an unsupervised neural topic model to address both problems.",
      " The ground-truth comments of the corresponding news provided by the human. Plausible: The 50 most similar comments to the news. We use the news as the query to retrieve the comments that appear in the training set based on the cosine similarity of their tf-idf values. We select the top 50 comments that are not the correct comments as the plausible comments.",
      "ne similarity of their tf-idf values. We select the top 50 comments that are not the correct comments as the plausible comments. Popular: The 50 most popular comments from the dataset. We count the frequency of each comments in the training set, and select the 50 most frequent comments to form the popular comment set."
    ],
    "answer": [
      "Qin et al. (QinEA2018), Kolhatkar and Taboada (KolhatkarT17), and Napoles et al. (NapolesTPRP17)"
    ]
  },
  {
    "title": "Learn to Code-Switch: Data Augmentation using Copy Mechanism on Language Modeling",
    "evidence": [
      "ion approach such as equivalent constraint BIBREF5 , BIBREF6 is not restrictive to languages with distinctive grammar structure. In this paper, we propose a novel language-agnostic method to learn how to generate code-switching sentences by using a pointer-generator network BIBREF7 .",
      "Conclusion\nWe introduce a new learning method for code-switching sentence generation using a parallel monolingual corpus that is applicable to any language pair.",
      "proposed method\nstill slightly outperforms the heuristic from linguistic constraint. In addition, we get a crucial gain on performance by adding syntax representation of the sequences.",
      "The data are collected from spontaneously spoken interviews and conversations in Singapore and Malaysia by bilinguals BIBREF21 . As the data preprocessing, words are tokenized using Stanford NLP toolkit BIBREF22 and all hesitations and punctuations were removed except apostrophe. The split of the dataset is identical to BIBREF9 and it is showed in Table TABREF6 .",
      "n with pointer networks to align and choose words from the monolingual sentences and form a grammatical code-switching sentence. In our experiment, we show that by training a language model using the augmented sentences we improve the perplexity score by 10% compared to the LSTM baseline.",
      " language models is challenging and very expensive. To alleviate this problem using parallel corpus has been a major workaround. However, existing solutions use linguistic constraints which may not capture the real data distribution. In this work, we propose a novel method for learning how to generate code-switching sentences from parallel corpora.",
      "source monolingual sentences. Our model leverages this property by copying input tokens, instead of generating vocabulary words. This approach has two major advantages: (1) the learning complexity decreases since it relies on copying instead of generating; (2) improvement in generalization, the copy mechanism could produce words from the input that are not present in the vocabulary.",
      " novel language-agnostic method to learn how to generate code-switching sentences by using a pointer-generator network BIBREF7 . The model is trained from concatenated sequences of parallel sentences to generate code-switching sentences, constrained by code-switching texts.",
      " of the two monolingual sentences, denoted as INLINEFORM0 , and the output is a code-switched sentence, denoted as INLINEFORM1 . The main assumption is that almost all, the token present in the code-switching sentence are also present in the source monolingual sentences. Our model leverages this property by copying input tokens, instead of generating vocabulary words.",
      "r network and language model. Our experiment, our pointer-generator model has 500-dimensional hidden states and word embeddings. We use 50k words as our vocabulary for source and target. We evaluate our pointer-generator performance using BLEU score.",
      ". A further investigation of Equivalence Constraint and Curriculum Learning showed an improvement in language modeling BIBREF6 . A multi-task learning approach was introduced to train the syntax representation of languages by constraining the language generator BIBREF9 .",
      "example in Figure FIGREF1 shows that our model is able to construct a new well-formed sentence such as “我们要去(We want to) check\". It is also shown that the pointer-generator model has the capability to learn the characteristics of the linguistic constraints from data without any word alignment between the matrix and embedded languages.",
      "ned from concatenated sequences of parallel sentences to generate code-switching sentences, constrained by code-switching texts. The pointer network copies words from both languages and pastes them into the output, generating code switching sentences in matrix language to embedded language and vice versa."
    ],
    "answer": [
      "{\n\"English\", \"Mandarin Chinese\"\n}"
    ]
  },
  {
    "title": "Rnn-transducer with language bias for end-to-end Mandarin-English code-switching speech recognition",
    "evidence": [
      "with its corresponding language ID, which is beneficial for model to learn the language identity information from transcription. In the inference process, the predicted language IDs are used to adjust the output posteriors. The experiment",
      "Abstract\nRecently, language identity information has been utilized to improve the performance of end-to-end code-switching (CS) speech recognition.",
      "n-based models trained with large speech corpus perform competitively compared to the state-of-art model in some tasks BIBREF13. However, the lack of CS training data poses serious problem to end-to-end methods. To address the problem, language identity information is utilized to improve the performance of recognition BIBREF3, BIBREF4, BIBREF5.",
      "language identity information has been utilized to improve the performance of end-to-end code-switching (CS) speech recognition. However, previous works use an additional language identification (LID) model as an auxiliary module, which causes the system complex.",
      "ress the problem, language identity information is utilized to improve the performance of recognition BIBREF3, BIBREF4, BIBREF5. They are usually based on CTC or attention-based encoder-decoder models or the combination of both. However, previous works use an additional language identification (LID) model as an auxiliary module, which causes the system complex.",
      "T to predict language IDs in CS points. So our method can model the CS distribution directly, no additional LID model is needed. Then we constrain the input word embedding with its corresponding language ID, which is beneficial for model to learn the language identity information from transcription.",
      "T) model with language bias to alleviate the problem. We use the language identities to bias the model to predict the CS points. This promotes the model to learn the language identity information directly from transcription, and no additional LID model is needed. We evaluate the approach on a Mandarin-English CS corpus SEAME. Compared to our RNN-T baseline, the",
      " which predict the current label given history labels BIBREF10. So it is effective in incorporating LID into the language model. In general, predicting language IDs only from text data is difficult. However, the joint training mechanism of RNN-T allows it to combine the language and acoustic information to model the CS distribution.",
      "ecognition and LID simultaneously. For this task, we augment the output symbols set with language IDs $$ and $$ as shown in Fig. 1, i.e., $\\hat{\\mathcal {Y}} \\in \\bar{\\mathcal {Y}} \\cup \\lbrace ,\\rbrace $. The intuition behind it is that the CS in the transcript may obey a certain probability distribution, and this distribution can be learned by neural network.",
      "r, previous works use an additional language identification (LID) model as an auxiliary module, which causes the system complex. In this paper, we propose an improved RNN-T model with language bias to alleviate the problem. The model is trained to predict language IDs as well as the subwords.",
      " the model with standard decode process. This suggests that the predicted language IDs can effectively guide the model decoding. Because the model assigns language IDs to the recognized words directly, the language IDs error rate is hard to compute. This result may imply that the prediction accuracy of our method is high enough to guide decoding.",
      "d RNN-T model with language bias to alleviate the problem. The model is trained to predict language IDs as well as the subwords. To ensure the model can learn CS information, we add language IDs in the CS point of transcription, as illustrated in Fig. 1. In the figure, we use the arrangements of different geometric icons to represent the CS distribution.",
      "t symbols such as speaker role and \"end-of-word\" symbol, which are not related to the input feature directly BIBREF14, BIBREF15. So the language IDs can also be treated as the output symbols. What's more, RNN-T can seamlessly integrate the acoustic and linguistic information."
    ],
    "answer": [
      "The language identities are obtained by constraining the input word embedding with its corresponding language ID, which is beneficial for the model to learn the language identity information from transcription."
    ]
  },
  {
    "title": "Fully Automated Fact Checking Using External Sources",
    "evidence": [
      "layer. This represents a task-specific embedding of the input example, and in our experiments it turned out to be quite helpful. Unlike in the SVM only model, this time we use the bi-LSTM embeddings as an input to the SVM. Ultimately, this yields a combination of deep learning and task-specific embeddings with RBF kernels.",
      "ord embeddings and similarities. However, the word embeddings this time are calculated by averaging rather than using a bi-LSTM. Finally, we combine the SVM with the NN by augmenting the input to the SVM with the values of the units in the hidden layer. This represents a task-specific embedding of the input example, and in our experiments it turned out to be quite helpful.",
      "of the words in the text, and also (ii) using LSTM encodings, which we train for the task as part of a deep neural network (NN). We also use a task-specific embedding of the claim together with all the above evidence about it, which comes from the last hidden layer of the NN.",
      "f related sentences from the Web pages, which we automatically train for the task using Long Short-Term Memory networks (LSTMs). We also use the final hidden layer of the neural network as a task-specific embedding of the claim, together with the Web evidence.",
      " captures the task-specific embeddings. This layer is connected to a softmax output unit to classify the claim as true or false. The bottom of Figure FIGREF7 represents the generic architecture of each of the LSTM components.",
      "eural networks to learn hidden representations that capture the variation of contextual information of relevant posts over time. Unlike this work, we do not use microblogs, but we query the Web directly in search for evidence.",
      " as features the embeddings of the claim, of the best-scoring snippet, and of the best-scoring sentence triplet from a Web page. We calculate these embeddings (i) as the average of the embeddings of the words in the text, and also (ii) using LSTM encodings, which we train for the task as part of a deep neural network (NN).",
      "ching triple of consecutive sentences from a Web page. We further feed the network with the similarity features described above. All these vectors are concatenated and fully connected to a much more compact hidden layer that captures the task-specific embeddings. This layer is connected to a softmax output unit to classify the claim as true or false.",
      "also use the final hidden layer of the neural network as a task-specific embedding of the claim, together with the Web evidence. We feed all these representations as features, together with pairwise similarities, into a Support Vector Machine (SVM) classifier using an RBF kernel to classify the claim as True or False.",
      "gs as an input to the SVM. Ultimately, this yields a combination of deep learning and task-specific embeddings with RBF kernels. Dataset We used part of the rumor detection dataset created by BIBREF3 . While they analyzed a claim based on a set of potentially related tweets, we focus on the claim itself and on the use of supporting information from the Web.",
      " few assumptions about the task, and looks for supportive information directly on the Web. Our system works fully automatically. It does not use any heavy feature engineering and can be easily used in combination with task-specific approaches as well, as a core subsystem.",
      "act checking using external sources, tapping the potential of the entire Web as a knowledge source to confirm or reject a claim. Our framework uses a deep neural network with LSTM text encoding to combine semantic kernels with task-specific embeddings that encode a claim together with pieces of potentially-relevant text fragments from the Web, taking the source reliability into account.",
      " any heavy feature engineering and can be easily used in combination with task-specific approaches as well, as a core subsystem. Finally, it combines the representational strength of recurrent neural networks with kernel-based classification. The system starts with a claim to verify."
    ],
    "answer": [
      "(i) the average of the embeddings of the words in the text, and \n(ii) using LSTM encodings, which are trained for the task as part of a deep neural network (NN)."
    ]
  },
  {
    "title": "Aspect Term Extraction with History Attention and Selective Transformation",
    "evidence": [
      "results\nshow that our framework can outperform state-of-the-art methods.",
      "results\nover four benchmark datasets clearly demonstrate that our framework can outperform all state-of-the-art methods.",
      "RS w/o THA”, we get about 1% absolute gains on each dataset, and then the performance is comparable to CMLA. By adding THA, i.e. “OURS”, the performance is further improved, and all state-of-the-art methods are surpassed.",
      "ramework achieves 5.0%, 1.6%, 1.4%, 1.3% absolute gains on INLINEFORM0 , INLINEFORM1 , INLINEFORM2 and INLINEFORM3 respectively. Our framework can outperform RNCRF, a state-of-the-art model based on dependency parsing, on all datasets. We also notice that RNCRF does not perform well on INLINEFORM0 and INLINEFORM1 (3.7% and 3.9% inferior than ours).",
      "results\nshow that each of THA and STN is helpful for improving the performance, and the contribution of STN is slightly larger than THA. “OURS w/o THA & STN” only keeps the basic bi-linear attention.",
      "results\nthan CRF-1 on all datasets. WDEmb, which is also an enhanced CRF-based method using additional dependency context embeddings, obtains superior performances than CRF-2.",
      "results\nshow that our model dominates those joint extraction works such as RNCRF and CMLA on the performance of ATE. It suggests that the joint extraction sacrifices the accuracy of aspect prediction, although the ground-truth opinion words were annotated by these authors.",
      " over the ultimate aspect/opinion features and the input word embeddings of LSTMs. The dropout rates are empirically set as 0.5. With 5-fold cross-validation on the training data of INLINEFORM0 , other hyper-parameters are set as follows: INLINEFORM1 , INLINEFORM2 ; the number of cached historical aspect representations INLINEFORM3 is 5; the learning rate of SGD is 0.07. Main",
      "results\nshow that our model dominates those joint extraction works such as RNCRF and CMLA on the performance of ATE.",
      "th the strongest baseline (i.e., CMLA), suggesting that only using attention mechanism to distill opinion summary is not enough. After inserting the STN component before the bi-linear attention, i.e. “OURS w/o THA”, we get about 1% absolute gains on each dataset, and then the performance is comparable to CMLA. By adding THA, i.e.",
      "us RNCRF's performance degradation is probably due to the errors from the dependency parser when processing such informal texts. CMLA and MIN do not rely on dependency parsing, instead, they employ attention mechanism to distill opinion information to help aspect extraction. Our framework consistently performs better than them.",
      "ormance, and the contribution of STN is slightly larger than THA. “OURS w/o THA & STN” only keeps the basic bi-linear attention. Although it performs not bad, it is still less competitive compared with the strongest baseline (i.e., CMLA), suggesting that only using attention mechanism to distill opinion summary is not enough.",
      "Results\nAs shown in Table TABREF39 , the proposed framework consistently obtains the best scores on all of the four datasets. Compared with the winning systems of SemEval ABSA, our framework achieves 5.0%, 1.6%, 1.4%, 1.3% absolute gains on INLINEFORM0 , INLINEFORM1 , INLINEFORM2 and INLINEFORM3 respectively."
    ],
    "answer": [
      "Our framework outperforms state-of-the-art methods by 5.0%, 1.6%, 1.4%, 1.3% absolute gains on INLINEFORM0, INLINEFORM1, INLINEFORM2, and INLINEFORM3 respectively."
    ]
  },
  {
    "title": "Improving Visually Grounded Sentence Representations with Self-Attention",
    "evidence": [
      "ier initialization BIBREF29 for all others. For the datasets, we use Karpathy and Fei-Fei's split for MS-COCO dataset BIBREF10 . Image features are prepared by extracting hidden representations at the final layer of ResNet-101 BIBREF30 . We evaluate sentence representation quality using SentEval BIBREF7 , BIBREF1 scripts.",
      "edict image features and the target sentence. We train the model on images and respective captions from COCO5K dataset BIBREF2 . We augment the state-of-the-art sentence representations with those produced by our model and conduct a series of experiments on transfer tasks to test the quality of sentence representations.",
      "t the final layer of ResNet-101 BIBREF30 . We evaluate sentence representation quality using SentEval BIBREF7 , BIBREF1 scripts. Mini-batch size is 128 and negative samples are prepared from remaining data samples in the same mini-batch.",
      "tion help our model produce more feature-rich visually grounded sentence representations. Related Work Sentence Representations. Since the inception of word embeddings BIBREF3 , extensive work have emerged for larger semantic units, such as sentences and paragraphs. These works range from deep neural models BIBREF4 to log-bilinear models BIBREF5 , BIBREF6 .",
      "we examine attention vectors for selected samples from MS-COCO dataset and compare them to associated images (Figure FIGREF13 ). For example, given the sentence “man in black shirt is playing guitar”, our model identifies words that have association with strong visual imagery, such as “man”, “black” and “guitar”.",
      "included in the paper due to limited space) show comparable performances to other more specialized methods BIBREF10 , BIBREF39 . Attention Mechanism at Work In order to study the effects of incorporating self-attention mechanism in joint prediction of image and language features, we examine attention vectors for selected samples from MS-COCO dataset and compare them to associated images (Figure FIGREF13 ).",
      "ures INLINEFORM3 and the target image features INLINEFORM4 higher than other pairs, which is achieved by ranking loss functions. Although margin ranking loss has been the dominant choice for training cross-modal feature matching BIBREF17 , BIBREF1 , BIBREF23 , we find that log-exp-sum pairwise ranking BIBREF24 yields better",
      "results\nin terms of evaluation performance and efficiency. Thus, the objective for ranking DISPLAYFORM0 where INLINEFORM0 is the set of negative examples and INLINEFORM1 is cosine similarity.",
      "del with those obtained from the state-of-the-art unsupervised learning model (Layer Normalized Skip-Thoughts, ST-LN) BIBREF31 . We evaluate the quality of sentence representations produced from different variants of our encoders on well-known transfer tasks: movie review sentiment (MR) BIBREF32 , customer reviews (CR) BIBREF33 , subjectivity (SUBJ) BIBREF34 , opinion polarity (MPQA) BIBREF35 , paraphrase identification (MSRP) BIBREF36 , binary sentiment classification (SST) BIBREF37 , SICK entailment and SICK relatedness BIBREF38 .",
      "ose produced by our model and conduct a series of experiments on transfer tasks to test the quality of sentence representations. Through detailed analysis, we confirm our hypothesis that self-attention help our model produce more feature-rich visually grounded sentence representations. Related Work Sentence Representations.",
      ", such as sentences and paragraphs. These works range from deep neural models BIBREF4 to log-bilinear models BIBREF5 , BIBREF6 . A recent work proposed using supervised learning of a specific task as a leverage to obtain general sentence representation BIBREF7 . Joint Learning of Language and Vision.",
      "Conclusion\nand Future Work In this paper, we proposed a novel encoder that exploits self-attention mechanism. We trained the model using MS-COCO dataset and evaluated sentence representations produced by our model (combined with universal sentence representations) on several transfer tasks.",
      "Results\non COCO5K image and caption retrieval tasks (not included in the paper due to limited space) show comparable performances to other more specialized methods BIBREF10 , BIBREF39 ."
    ],
    "answer": [
      "COCO5K dataset"
    ]
  },
  {
    "title": "Deeper Task-Specificity Improves Joint Entity and Relation Extraction",
    "evidence": [
      "results\nfrom the ablation study. These",
      "results\non the RE task while using an order of magnitude fewer trainable parameters than the current SOTA architecture. An ablation study confirms the importance of the additional task-specific layers for achieving these",
      " in which the NER and RE problems can be solved by relying on more shared information should require fewer task-specific layers. Experiments ::: Ablation Study To further demonstrate the effectiveness of the additional task-specific BiRNN layers in our architecture, we conducted an ablation study using the CoNLL04 dataset.",
      "ness of the additional task-specific BiRNN layers in our architecture, we conducted an ablation study using the CoNLL04 dataset. We trained and evaluated in the same manner described above, using the same hyperparameters, with the following exceptions: We used either (i) zero NER-specific BiRNN layers, (ii) zero RE-specific BiRNN layers, or (iii) zero task-specific BiRNN layers of any kind.",
      "results\nfor each set of hyperparameter across three trials with random weight initializations. Table TABREF26 contains the",
      "results\nfor both tasks on the ADE dataset; on the CoNLL04 dataset, we achieve SOTA",
      "ials with random weight initializations in which we trained on the combined training and dev sets and evaluated on the test set. As previous work using the CoNLL04 dataset has reported both micro- and macro-averages, we report both sets of metrics.",
      "izer with a mini-batch size of 16. For the CoNLL04 dataset, we used the Nesterov Adam optimizer with and a mini-batch size of 2. For both datasets, we used a learning rate of $5\\times 10^{-4}$, During training, dropout was applied before each BiRNN layer, other than the character BiRNN layer, and before the RE scoring layer.",
      "results\nfrom five trials with random weight initializations in which we trained on the combined training and dev sets and evaluated on the test set.",
      "results\nfor both the NER and RE tasks. On the CoNLL04 dataset, we achieve SOTA",
      "elative importance of the NER and RE tasks during training. Final training for both datasets used a value of 5 for $\\lambda ^r$. For the ADE dataset, we trained using the Adam optimizer with a mini-batch size of 16. For the CoNLL04 dataset, we used the Nesterov Adam optimizer with and a mini-batch size of 2.",
      "we remove overlapping entities. We preserve the entity with the longer span and remove any relations involving a removed entity. There are no official training, dev, and test splits for the ADE dataset, leading previous researchers to use some form of cross-validation when evaluating their models on this dataset. We split out 10% of the data to use as a held-out dev set. Final",
      "results\nare obtained by averaging"
    ],
    "answer": [
      "(i) zero NER-specific BiRNN layers, \n(ii) zero RE-specific BiRNN layers, \n(iii) zero task-specific BiRNN layers of any kind."
    ]
  },
  {
    "title": "Semi-supervised sequence tagging with bidirectional language models",
    "evidence": [
      "dding pre- trained context embeddings from bidirectional language models to NLP systems and apply it to sequence labeling tasks. We evaluate our model on two standard datasets for named entity recognition (NER) and chunking, and in both cases achieve state of the art",
      "ask BIBREF13 and the CoNLL 2000 Chunking task BIBREF14 . We report the official evaluation metric (micro-averaged INLINEFORM0 ). In both cases, we use the BIOES labeling scheme for the output tags, following previous work which showed it outperforms other options BIBREF15 .",
      "LINEFORM3 increased slightly to INLINEFORM4 indicating that the additional parameters in TagLM are slightly hurting performance. One artifact of our evaluation framework is that both the labeled data in the chunking and NER tasks and the unlabeled text in the 1 Billion Word Benchmark used to train the bidirectional LMs are derived from news articles.",
      "concatenation were encouraging so we did not explore these alternatives in this study, preferring to leave them for future work. Experiments We evaluate our approach on two well benchmarked sequence tagging tasks, the CoNLL 2003 NER task BIBREF13 and the CoNLL 2000 Chunking task BIBREF14 . We report the official evaluation metric (micro-averaged INLINEFORM0 ).",
      "ri, we expect the addition of LM embeddings to be most beneficial in cases where the task specific annotated datasets are small. To test this hypothesis, we replicated the setup from BIBREF3 that samples 1% of the CoNLL 2003 training set and compared the performance of TagLM to our baseline without LM.",
      ", are obtained as a lookup INLINEFORM4 , initialized using pre-trained word embeddings, and fine tuned during training BIBREF2 . To learn a context sensitive representation, we employ multiple layers of bidirectional RNNs.",
      "results\nwithout additional labeled data or task specific gazetteers. Tables TABREF17 and TABREF18 compare",
      "results\nin both tasks when external resources (labeled data or task specific gazetteers) are available. Furthermore, Tables TABREF17 and TABREF18 show that, in most cases, the improvements we obtain by adding LM embeddings are larger then the improvements previously obtained by adding other forms of transfer or joint learning.",
      "his result confirms that the RNNs in the baseline tagger encode essential information which is not encoded in the LM embeddings. This is unsurprising since the RNNs in the baseline tagger are trained on labeled examples, unlike the RNN in the language model which is only trained on unlabeled examples. Note that the LM weights are fixed in this experiment.",
      "he RNN in the language model which is only trained on unlabeled examples. Note that the LM weights are fixed in this experiment. A priori, we expect the addition of LM embeddings to be most beneficial in cases where the task specific annotated datasets are small.",
      "quences into a context sensitive representation before making token specific predictions BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . Although the token representation is initialized with pre-trained embeddings, the parameters of the bidirectional RNN are typically learned only on labeled data.",
      " embeddings amounts to an average absolute improvement of 1.06 and 1.37 INLINEFORM0 in the NER and Chunking tasks, respectively. Although we do not use external labeled data or gazetteers, we found that TagLM outperforms previous state of the art",
      " in the first RNN layer and general context as expressed in the LM embeddings in a way that improves overall system performance. These"
    ],
    "answer": [
      "The evaluation datasets used are the CoNLL 2003 NER task and the CoNLL 2000 Chunking task."
    ]
  },
  {
    "title": "Adapt or Get Left Behind: Domain Adaptation through BERT Language Model Finetuning for Aspect-Target Sentiment Classification",
    "evidence": [
      "Results\nAnalysis The",
      " trained in-domain, what ATSC end-task performance can be reached through fully exploitet finetuning of the BERT language model? RQ3: When trained cross-domain in the special case of domain adaptation, what ATSC end-task performance can be reached if BERT language model finetuning is fully exploitet?",
      "results\nin about 2 Mio. reviews that are finetuned for 15 epochs. The exact statistics for the three finetuning corpora are shown in the top of tab:datasets.",
      " How does the number of training iterations in the BERT language model finetuning stage influence the ATSC end-task performance? At what point does performance start to improve, when does it converge? RQ2: When trained in-domain, what ATSC end-task performance can be reached through fully exploitet finetuning of the BERT language model?",
      "results\nof our experiments are shown in fig:acc-dep-lmiterations and tab:",
      " performs strongly on the ATSC task. Here, domain-specific finetuning could probably bring significant performance improvements. Another interesting direction for future work would be to investigate cross-domain behavior for an additional domain like hotels, which is more similar to the restaurants domain.",
      "results\nrespectively. To answer RQ1, which is concerned with details on domain-specific language model finetuning, we can see in fig:acc-dep-lmiterations that first of all, language model finetuning has a substantial effect on ATSC end-task performance.",
      "results\nreported are the average of 9 runs with different random initializations. This is needed to measure significance of improvements, as the standard deviation in accuray amounts to roughly $1\\%$ for all experiments, see fig:acc-dep-lmiterations.",
      "l case of domain adaptation, what ATSC end-task performance can be reached if BERT language model finetuning is fully exploitet? Experiments ::: Datasets for Classification and Language Model Finetuning We conduct experiments using the two SemEval 2014 Task 4 Subtask 2 datasets BIBREF1 for the laptops and the restaurants domain.",
      "yzed the behavior of the number of domain-specific BERT language model finetuning steps in relation to the end-task performance. With the findings on how to best exploit BERT language model finetuning we were able to train high performing models, which one of even performs as new state-of-the-art on SemEval 2014 Task 4 restaurants dataset.",
      "n better than a BERT-base model that is trained in-domain. Overall, our findings reveal promising directions for follow-up work. The XLNet-base model performs strongly on the ATSC task. Here, domain-specific finetuning could probably bring significant performance improvements.",
      " amounts to about $1\\%$ in accuracy, which justifies averaging over 9 runs to measure differences in model performance reliably. To answer RQ2, which is concerned with in-domain ATSC performance, we see in tab:",
      "parameters. The model already converges at around 17 Mio. sentences. More finetuning does not improve performance significantly. In addition, we find that different runs have a high variance, the standard deviation amounts to about $1\\%$ in accuracy, which justifies averaging over 9 runs to measure differences in model performance reliably."
    ],
    "answer": [
      "The performance results show that the BERT language model finetuning has a substantial effect on ATSC end-task performance, with the XLNet-base model performing strongly on the ATSC task. The number of training iterations in the BERT language model finetuning stage influences the ATSC end-task performance, with the model already converging at around 17 Mio. sentences. The standard deviation in accuracy amounts to about $1\\%$, justifying averaging over 9 runs to measure differences in model performance reliably."
    ]
  },
  {
    "title": "NumNet: Machine Reading Comprehension with Numerical Reasoning",
    "evidence": [
      "dataset as compared to semantic parsing-based models, traditional MRC models and even numerical MRC models NAQANet and NAQANet+. The reason is that our NumNet model can make full use of the numerical comparison information over numbers in both question and passage via the proposed NumGNN module.",
      "results\non both the development and testing sets on DROP dataset as compared to semantic parsing-based models, traditional MRC models and even numerical MRC models NAQANet and NAQANet+.",
      " make full use of the numerical comparison information over numbers in both question and passage via the proposed NumGNN module. (2) Our implemented NAQANet+ has a much better performance compared to the original version of NAQANet. It verifies the effectiveness of our proposed enhancements for baseline.",
      "results\nin various public benchmarks such as SQuAD BIBREF4 and RACE BIBREF5.",
      "coding module and prediction module to explicitly consider the numerical comparison information and perform numerical reasoning. As NAQANet has been shown effective for handling numerical MRC problem BIBREF6, we leverage it as our base model and mainly focus on the design and integration of the NumGNN in this work.",
      "re FIGREF52, we could observe that: (1) The 2-layer version of NumNet achieves the best performance for the comparing questions. From careful analysis, we find that most comparing questions only require at most 2-step reasoning (e.g., “Who was the second oldest player in the MLB, Clemens or Franco?”), and therefore the 3-layer version of NumNet is more complex but brings no gains for these questions.",
      "s: (1) Exact matching/Paraphrasing; (2) Summary; (3) Logic reasoning; (4) Utilizing external knowledge; (5) Numerical reasoning. They found that most existing MRC models are focusing on dealing with the first three types of questions. However, all these models suffer from problems when answering the questions requiring numerical reasoning.",
      "results\n, we can observe that: (1) Our NumNet model achieves better",
      "ement as compared to all baseline methods by explicitly performing numerical reasoning over numbers in the question and passage. In particular, we show that our model could effectively deal with questions requiring sorting with multi-layer NumGNN. The source code of our paper is available at https://github.com/ranqiu92/NumNet.",
      " effectiveness and efficiency. However, all the existing AWP systems are only trained and validated on small benchmark datasets. BIBREF21 found that the performance of these AWP systems sharply degrades on larger datasets.",
      "results\nin most existing public MRC datasets. Despite the success of neural network-based MRC models, researchers began to analyze the data and rethink to what extent we have solved the problem of MRC.",
      "ly-aware graph neural network (NumGNN) to further integrate the comparison information among numbers into their representations. Finally, we utilize the numerically-aware representation of passages to infer the answer to the question. The experimental",
      " statistically significant improvements compared to traditional GNN on both EM and F1 scores especially for comparing questions. It indicates that considering the comparing information over numbers could effectively help the numerical reasoning for comparing questions."
    ],
    "answer": [
      "Semantic parsing-based models, traditional MRC models, numerical MRC models NAQANet and NAQANet+, and traditional GNN."
    ]
  },
  {
    "title": "Classifying topics in speech when all you have is crummy translations.",
    "evidence": [
      "een some limited work on cross-lingual keyword spotting BIBREF23, where ASR is cascaded with text-based cross-lingual retrieval. Some recent studies have attempted to use vision as a complementary modality to do cross-lingual retrieval BIBREF24, BIBREF25. But cross-lingual topic classification for speech has not been considered elsewhere, as far as we know.",
      "Abstract\nGiven a large amount of unannotated speech in a language with few resources, can we classify the speech utterances by topic?",
      "ckly filter social media streams for messages about the safety of individuals, and to populate a person finder database BIBREF0. Japanese text is high-resource, but there are many cases where it would be useful to make sense of speech in low-resource languages.",
      "nstead focus only on the high importance terms. We lowercase the English translations and remove all punctuation, and stopwords. We further remove the terms occurring in more than 10% of the documents and those which occur in less than 2 documents, keeping only the 1000 most frequent out of the remaining.",
      "EF19 and to semantic keyword retrieval, where non-exact but relevant keyword matches are retrieved BIBREF20, BIBREF21, BIBREF22. In all these studies, the query and search languages are the same, while we consider the cross-lingual case. There has been some limited work on cross-lingual keyword spotting BIBREF23, where ASR is cascaded with text-based cross-lingual retrieval.",
      "nese text is high-resource, but there are many cases where it would be useful to make sense of speech in low-resource languages. For example, in Uganda, as in many parts of the world, the primary source of news is local radio stations, which broadcast in many languages.",
      "t by generating a tf-idf representation for each of these. The English text contains 170K tokens and 6K terms (vocabulary size). As we are looking for topics which are coarse-level categories, we do not use the entire vocabulary, but instead focus only on the high importance terms. We lowercase the English translations and remove all punctuation, and stopwords.",
      "Introduction\nQuickly making sense of large amounts of linguistic data is an important application of language technology.",
      "data from the Switchboard Telephone speech corpus BIBREF7, which consists of around 300 hours of English speech and transcripts. This was reported to substantially improve translation quality when the training set for ST was only tens of hours. Methods ::: Topic modeling and classification.",
      "n task of classification, we first review the set of topics discovered from the human translations of train20h (Table TABREF13). We explored different numbers of topics, and chose 10 after reviewing the",
      "Introduction\nQuickly making sense of large amounts of linguistic data is an important application of language technology. For example, after the 2011 Japanese tsunami, natural language processing was used to quickly filter social media streams for messages about the safety of individuals, and to populate a person finder database BIBREF0.",
      "speech using supervision, but what is important about our result is it shows that a small amount of supervision goes a long way. A slightly different approach to quickly analysing speech is the established task of Keyword spotting BIBREF16, BIBREF17, which simply asks whether any of a specific set of keywords appears in each segment.",
      "ate what an automatic topic classifier would predict given correct translations and they capture finer-grained changes in topic. Table TABREF14 shows a few examples where the silver labels differ from the assigned call topic prompts."
    ],
    "answer": [
      "The language they look at is not specified, but based on the context, it appears to be a low-resource language, such as Japanese or a language spoken in Uganda."
    ]
  },
  {
    "title": "LAXARY: A Trustworthy Explainable Twitter Analysis Model for Post-Traumatic Stress Disorder Assessment",
    "evidence": [
      "mate survey scores (similar to sentiment category scores of naive LIWC) which is both explainable and trustworthy to clinicians. Demographics of Clinically Validated PTSD Assessment Tools There are many clinically validated PTSD assessment tools that are being used both to detect the prevalence of PTSD and its intensity among sufferers.",
      "nically validated PTSD assessment surveys, can we fill out PTSD assessment surveys using twitter posts analysis of war-veterans? If possible, what sort of analysis and approach are needed to develop such XAI model to detect the prevalence and intensity of PTSD among war-veterans only using the social media (twitter) analysis where users are free to share their everyday mental and social conditions?",
      "ox machine learning techniques, these frameworks cannot be trusted by the clinicians due to the lack of clinical explainability. To obtain the trust of clinicians, we explore the big question, can twitter posts provide enough information to fill up clinical PTSD assessment surveys that have been traditionally trusted by clinicians?",
      "extraction of posted texts which failed to obtain acceptability and trust of clinicians due to the lack of their explainability. In the context of the above research problem, we aim to answer the following research questions Given clinicians have trust on clinically validated PTSD assessment surveys, can we fill out PTSD assessment surveys using twitter posts analysis of war-veterans?",
      "lly validated PTSD assessment tools that are being used both to detect the prevalence of PTSD and its intensity among sufferers. Among all of the tools, the most popular and well accepted one is Domain-Specific Risk-Taking (DOSPERT) Scale BIBREF15.",
      "in time before going out of control that may result catastrophic impacts on society, people around or even sufferers themselves. Although, psychiatrists invented several clinical diagnosis tools (i.e., surveys) by assessing symptoms, signs and impairment associated with PTSD, most of the times, the process of diagnosis happens at the severe stage of illness which may have already caused some irreversible damages of mental health of the sufferers.",
      "clinicians. In this paper, we proposed, LAXARY, a novel method of filling up PTSD assessment surveys using weekly twitter posts. As the clinical surveys are trusted and understandable method, we believe that this method will be able to gain trust of clinicians towards early detection of PTSD.",
      "constructs to show convergent validity, and not strongly correlated with positive PTSD constructs to show discriminant validity. To test these two types of validity, we use the same 210 users' tweets used for the reliability assessment. The",
      "ssification using the BIBREF14 which fails to provide trust of clinicians due to its lack of interpretability in clinical terms. In this paper, we develop LAXARY model where first we start investigating clinically validated survey tools which are trustworthy methods of PTSD assessment among clinicians, build our category sets based on the survey questions and use these as dictionary words in terms of first person singular number pronouns aspect for next level LIWC algorithm.",
      "r selection and inclusion criteria, it was extremely difficult to manually find the evidence of the self-claimed PTSD sufferers. Although, we have shown extremely promising initial findings about the representation of a blackbox model into clinically trusted tools, using only 210 users' data is not enough to come up with a trustworthy model.",
      " (vii) estimate the quality of selected features-based classification for filling up surveys based on classified categories i.e. PTSD assessment which is trustworthy among the psychiatry community.",
      "of a blackbox model into clinically trusted tools, using only 210 users' data is not enough to come up with a trustworthy model. Moreover, more clinical validation must be done in future with real clinicians to firmly validate LAXARY model provided PTSD assessment outcomes.",
      " .96 based on the binary method, which confirm the high reliability of our PTSD Dictionary created PTSD survey based categories. After assessing the reliability of the PTSD Linguistic dictionary, we focus on the two most common forms of construct validity: convergent validity and discriminant validity BIBREF25."
    ],
    "answer": [
      "The clinically validated survey tools used for PTSD assessment include the Domain-Specific Risk-Taking (DOSPERT) Scale."
    ]
  },
  {
    "title": "Reference-less Quality Estimation of Text Simplification Systems",
    "evidence": [
      "tend their work in two directions. Firstly, we extend the comparison to include the degree of simplicity achieved by the system. Secondly, we compare additional features, including those used by BIBREF18 , both individually, as elementary metrics, and within multi-feature metrics.",
      "f being a monolingual task, which allows for direct comparisons to be made between the simplified text and its original version. In this paper, we compare multiple approaches to reference-less quality estimation of sentence-level text simplification systems, based on the dataset used for the QATS 2016 shared task.",
      " human judgments. These approaches even outperform metrics that make an extensive use of external data, such as language models. This shows that a lot of useful information can be obtained from the source sentence itself. Regarding simplicity, we observe that counting the number of characters, syllables and words provides the best",
      "efully adjusted weights. Methodology Our goal is to compare a large number of ways to perform TS evaluation without a reference. To this end, we use the dataset provided in the QATS shared task. We first compare the behaviour of elementary metrics, which range from commonly used metrics such as BLEU to basic metrics based on a single low-level feature such as sentence length.",
      "hich range from commonly used metrics such as BLEU to basic metrics based on a single low-level feature such as sentence length. We then compare the effect of aggregating these elementary metrics into more complex ones and compare our",
      "M1 -gram-based comparison metrics might bias the TS models towards copying the source sentence and applying fewer modifications. Syntactic parsing or language modelling might capture more insightful grammatical information and allow for more flexibility in the simplification model.",
      "BIBREF34 , comparison methods using pre-trained fastText word embeddings BIBREF35 or Skip-thought sentence embeddings BIBREF36 . TABLE TABREF30 lists 30 of the elementary metrics that we compared, which are those that we found to correlate the most with human judgments on one or more of the three dimensions (grammaticality, meaning preservation, simplicity).",
      "compare the behaviour of six different MT metrics when used between the source sentence and the corresponding simplified output. They evaluate these metrics with respect to meaning preservation and grammaticality. We extend their work in two directions. Firstly, we extend the comparison to include the degree of simplicity achieved by the system.",
      "tem output with its corresponding source sentence, but their metric, SARI, also requires to compare the output with a reference. In fact, this metric is designed to take advantage of more than one reference. It can be applied when only one reference is available for each source sentence, but its",
      "terature. These metrics rely on good quality references, something which is often not available in TS, as discussed by BIBREF7 . Moreover, BIBREF19 and BIBREF24 showed that using BLEU to compare the system output with a reference is not a good way to perform TS evaluation, even when good quality references are available.",
      "results\nare better when multiple references are available.",
      "pen challenge. As the task has common points with machine translation (MT), TS is often evaluated using MT metrics such as BLEU. However, such metrics require high quality reference data, which is rarely available for TS. TS has the advantage over MT of being a monolingual task, which allows for direct comparisons to be made between the simplified text and its original version.",
      "Surprisingly, word embedding comparison methods do not perform as well for meaning preservation, even when using word alignment. Methods that give the best"
    ],
    "answer": [
      "Multiple approaches are compared, including BLEU, elementary metrics based on a single low-level feature such as sentence length, M1-gram-based comparison metrics, syntactic parsing or language modelling, comparison methods using pre-trained fastText word embeddings or Skip-thought sentence embeddings, and multi-feature metrics."
    ]
  },
  {
    "title": "A Deep Neural Architecture for Sentence-level Sentiment Classification in Twitter Social Networking",
    "evidence": [
      "weets. The Stanford test set is small. However, it has been widely used in different evaluation tasks BIBREF0 BIBREF5 BIBREF13 . Sanders - Twitter Sentiment Corpus: This dataset consists of hand-classified tweets collected by using search terms: INLINEFORM0 , #google, #microsoft and #twitter. We construct the dataset as BIBREF14 for binary classification.",
      "Introduction\nTwitter sentiment classification have intensively researched in recent years BIBREF0 BIBREF1 .",
      "Introduction\nTwitter sentiment classification have intensively researched in recent years BIBREF0 BIBREF1 . Different approaches were developed for Twitter sentiment classification by using machine learning such as Support Vector Machine (SVM) with rule-based features BIBREF2 and the combination of SVMs and Naive Bayes (NB) BIBREF3 .",
      "Convolutional Neural Network (DeepCNN) for character-level embeddings in order to increase information for word-level embedding. After that, a Bidirectional Long Short-Term Memory Network (Bi-LSTM) produces a sentence-wide feature representation from the word-level embedding. We evaluate our approach on three Twitter sentiment classification datasets. Experimental",
      "racted. In sub-section B, we introduce various kinds of dataset. The modules of our model are constructed in other sub-sections. Data Preparation Stanford - Twitter Sentiment Corpus (STS Corpus): STS Corpus contains 1,600K training tweets collected by a crawler from BIBREF0 . BIBREF0 constructed a test set manually with 177 negative and 182 positive tweets.",
      "ight vectors BIBREF18 . Experimental setups For the Stanford Twitter Sentiment Corpus, we use the number of samples as BIBREF5 . The training data is selected 80K tweets for a training data and 16K tweets for the development set randomly from the training data of BIBREF0 . We conduct a binary prediction for STS Corpus.",
      "s and bi-grams) presented in tweets and the classifiers only use these non-emoticon features to predict the sentiment of tweets. However, there is a problem is that if the test set contains emoticons, they do not influence the classifiers because emoticon features do not contain in its training data.",
      "for word embeddings built on top of Word2Vec or TwitterGlove improves classification accuracy in Tweet sentiment classification. Our",
      "xperiments on two separate models: DeepCNN and Bi-LSTM in order to show the effectiveness of combination of DeepCNN and Bi-LSTM. In addition, the model using TwitterGlove outperform the model using GoogleW2V because TwitterGlove captures more information in Twitter than GoogleW2V. These",
      "results\nshow that our model can improve the classification accuracy of sentence-level sentiment analysis in Twitter social networking.",
      " tweets collected by a crawler from BIBREF0 . BIBREF0 constructed a test set manually with 177 negative and 182 positive tweets. The Stanford test set is small. However, it has been widely used in different evaluation tasks BIBREF0 BIBREF5 BIBREF13 .",
      "results\nfor sentiment analysis BIBREF5 BIBREF6 BIBREF7 . Some researchers used Convolutional Neural Network (CNN) for sentiment classification. CNN models have been shown to be effective for NLP.",
      "atasets because deep learning models can capture more information from emoticon features for increasing classification accuracy. Semantic Rules (SR) In Twitter social networking, people express their opinions containing sub-sentences."
    ],
    "answer": [
      "The three Twitter sentiment classification datasets used for experiments are the Stanford Twitter Sentiment Corpus, the Sanders - Twitter Sentiment Corpus, and the STS Corpus."
    ]
  },
  {
    "title": "Bleaching Text: Abstract Features for Cross-lingual Gender Prediction",
    "evidence": [
      "eatures from the best performing system at the latest PAN evaluation campaign BIBREF17 (word 1-2 grams and character 3-6 grams). For the multilingual embeddings model we use the mean embedding representation from the system of BIBREF27 and add max, std and coverage features.",
      "results\nin models with limited portability. Moreover, performance might be overly optimistic due to topic bias BIBREF18 . Recent work on cross-lingual author profiling has proposed the use of solely language-independent features BIBREF19 , e.g., specific textual elements (percentage of emojis, URLs, etc) and users' meta-data/network (number of followers, etc), but this information is not always available.",
      "n earlier versions of this paper. Furthermore, we are grateful to Chloé Braud for helping with the French human evaluation part. We would like to thank all of our human participants.",
      "riments. For the cross-lingual experiments, we train on all available source language data and test on all target language data. For the lexicalized experiments, we adopt the features from the best performing system at the latest PAN evaluation campaign BIBREF17 (word 1-2 grams and character 3-6 grams).",
      "ve of male users. Human Evaluation We experimented with three different conditions, one within language and two across language. For the latter, we set up an experiment where native speakers of Dutch were presented with tweets written in Portuguese and were asked to guess the poster's gender.",
      "Acknowledgment\ns We would like to thank the three anonymous reviewers and our colleagues for their useful feedback on earlier versions of this paper. Furthermore, we are grateful to Chloé Braud for helping with the French human evaluation part.",
      " performance of humans is 70.5%, which is quite in line with the findings of BIBREF6 , who report an accuracy of 75% on English. Within language, lexicalized models are superior to humans if exposed to enough information (200 tweets setup).",
      "predictive of female users. Quotes, question marks and length features, for example, appear to be more predictive of male users. Human Evaluation We experimented with three different conditions, one within language and two across language.",
      "to a model using multilingual embeddings BIBREF24 . Finally, we elicit human judgments both within language and across language. The latter is to check whether a person with no prior knowledge of (the lexicon of) a given language can predict the gender of a user, and how that compares to an in-language setup and the machine.",
      "abstract\nfeatures are effective and transfer across languages, we set up experiments for gender prediction comparing lexicalized and bleached models for both in- and cross-language experiments.",
      "age of emojis, URLs, etc) and users' meta-data/network (number of followers, etc), but this information is not always available. We propose a novel approach where the actual text is still used, but bleached out and transformed into more",
      "abstract\nfeatures fare surprisingly well and work a lot better across languages. The performance is on average 6% higher across all languages (57.9% for Avg, 63.9% for All) in comparison to their lexicalized counterparts, where Abs All",
      " 75% on English. Within language, lexicalized models are superior to humans if exposed to enough information (200 tweets setup). One explanation for this might lie in an observation by BIBREF6 , according to which people tend to rely too much on stereotypical lexical indicators when assigning gender to the poster of a tweet, while machines model less evident patterns."
    ],
    "answer": [
      "{\n\"word 1-2 grams and character 3-6 grams\", \"mean embedding representation\", \"max, std and coverage features\", \"percentage of emojis, URLs, etc\", \"number of followers, etc\", \"quotes, question marks and length features\"\n}"
    ]
  },
  {
    "title": "Community Question Answering Platforms vs. Twitter for Predicting Characteristics of Urban Neighbourhoods",
    "evidence": [
      "results\non 62 demographic attributes. Additionally, work in urban prediction uses geolocation-based platforms such as Twitter. QA data that has been utilised in this paper does not include geolocation information.",
      "r predicting many real-world attributes. Unlike many current works that focus on predicting one or few selected attributes (e.g. deprivation, race or income) using social media data, we study a wide range of 62 demographic attributes. Furthermore, we test whether terms extracted from both Yahoo!",
      "al media data, we study a wide range of 62 demographic attributes. Furthermore, we test whether terms extracted from both Yahoo! Answers and Twitter are semantically related to these attributes and provide examples of sociocultural profiles of neighbourhoods through the interpretation of the coefficients of the predictive models.",
      "tigate whether text from a Community Question Answering (QA) platform can be used to predict and describe real-world attributes. We experiment with predicting a wide range of 62 demographic attributes for neighbourhoods of London. We use the text from QA platform of Yahoo! Answers and compare our",
      "Discussion\nIn this paper, we investigate predicting values for real-world entities such as demographic attributes of neighbourhoods using",
      "results\nfor a wide range of 62 demographic attributes using Yahoo! Answers and Twitter. For each attribute, we display two terms with the highest coefficient common between the majority of the folds.",
      ". Prediction We investigate how well the demographic attributes can be predicted by using using Yahoo! Ansewrs and Twitter data. We define the task of predicting a continuous-valued demographic attribute for unseen neighbourhoods as a regression task given their normalised tf-idf document representation. A separate regression task is defined for each demographic attribute.",
      "w that on average, a wide range of demographic attributes of the population of neighbourhoods can be predicted using both Yahoo! Answers and Twitter with high performances of $0.54$ and $0.53$ respectively. While Yahoo!",
      "an be used to predict the demographic attributes of the population of those neighbourhoods. We compare the performance of Yahoo! Answers data to the performance of data from Twitter, a platform that has been widely used for predicting many real-world attributes. Unlike many current works that focus on predicting one or few selected attributes (e.g.",
      "discussion\ns on Yahoo! Answers QA platform to make predictions of demographic attribute of city neighbourhoods. Previous work in this domain has mainly focused on predicting the deprivation index of areas BIBREF4 .",
      "butes are very high. On the other hand, terms that describe a religion or an ethnicity are more specific and lower in frequency. Therefore attributes that are related to religion or ethnicity are predicted with a lower accuracy.",
      "results\nto the ones obtained from Twitter microblogs. Outcomes show that the correlation between the predicted demographic attributes using text from Yahoo! Answers",
      "e of city neighbourhoods. Previous work in this domain has mainly focused on predicting the deprivation index of areas BIBREF4 . In this work, we look at a wide range of attributes and report prediction"
    ],
    "answer": [
      "62"
    ]
  },
  {
    "title": "Bias in Semantic and Discourse Interpretation",
    "evidence": [
      "that don't get at the truth but are devised for some other purpose? Our study of interpretive bias relies on three key premises. The first premise is that histories are discursive interpretations of a set of data in the sense that like discourse interpretations, they link together a set of entities with semantically meaningful relations.",
      "able and, if so, under what conditions, and whether an agent builds a history for attaining that norm or for some other purpose. Organization of the paper Our paper is organized as follows. Section SECREF2 introduces our model of interpretive bias. Section SECREF3 looks forward towards some consequences of our model for learning and interpretation. We then draw some",
      "Conclusion\ns In this paper, we have put forward the foundations of a formal model of interpretive bias.",
      "if so, under what conditions. Can we detect and avoid biases that don't get at the truth but are devised for some other purpose? Our study of interpretive bias relies on three key premises.",
      "conclusion\ns in Section SECREF4 . A detailed and formal analysis of interpretive bias has important social implications. Questions of bias are not only timely but also pressing for democracies that are having a difficult time dealing with campaigns of disinformation and a society whose information sources are increasingly fragmented and whose biases are often concealed.",
      "Conclusion\ns In this paper, we have put forward the foundations of a formal model of interpretive bias. Our approach differs from philosophical and AI work on dialogue that links dialogue understanding to the recovery of speaker intentions and beliefs BIBREF56 , BIBREF57 .",
      "exploit statistical models with a set of parameters and random variables, which play the role of our types in interpretive bias. But for us, the interpretive process is already well underway once the model, with its constraints, features and explanatory hypotheses, is posited; at least a partial history, or set of histories, has already been created.",
      "conclusion\ns in Section SECREF4 . A detailed and formal analysis of interpretive bias has important social implications.",
      "Abstract\nIn this paper, we show how game-theoretic work on conversation combined with a theory of discourse structure provides a framework for studying interpretive bias.",
      "d opinions from text BIBREF15 , BIBREF16 , BIBREF17 , and our examples show that this is the case for interpretive bias as well. Epistemic ME games The second ingredient needed to understand interpretive bias is the connection between on the one hand the purpose and assumption behind telling a story and on the other the particular way in which that story is told.",
      "tive decision making and frequency estimation, even though they arguably have the goal of seeking an optimal or “true” solution. In a general analysis of interpretive bias like ours, however, it is an open question whether there is an objective norm or not, whether it is attainable and, if so, under what conditions, and whether an agent builds a history for attaining that norm or for some other purpose.",
      "res an empirical component on exactly how bias is manifested to be complete. This has links to the recent interest in fake news. Modeling interpretive bias can help in detecting fake news by providing relevant types to check in interpretation and by providing an epistemic foundation for fake news detection by exploiting ME truth games where one can draw from various sources to check the credibility of a story.",
      "ic and otherwise, to determine a rational reconstruction of an underlying interpretive bias and what goals a bias would satisfy. In this we also go beyond what current theories of discourse structure like SDRT can accomplish. Our theoretical work also requires an empirical component on exactly how bias is manifested to be complete. This has links to the recent interest in fake news."
    ],
    "answer": [
      "The paper analyzes three types of interpretive biases: (1) the connection between the purpose and assumption behind telling a story and the particular way in which that story is told, (2) the ability to detect and avoid biases that don't get at the truth but are devised for some other purpose, and (3) the conditions under which an agent builds a history for attaining a norm or for some other purpose."
    ]
  },
  {
    "title": "Spatial Concept Acquisition for a Mobile Robot that Integrates Self-Localization and Unsupervised Word Discovery from Spoken Sentences",
    "evidence": [
      "results\nare obtained in a lattice format. Word segmentation is performed by using the lattices of the speech recognition",
      "ed on the robot is SHURE's PG27-USB. Further, an unsupervised morphological analyzer, a latticelm 0.4, is implemented BIBREF22 . In the experiment, we compare the following three types of word segmentation methods. A set of syllable sequences is given to the graphical model of SpCoA by each method.",
      "nsupervised morphological analyzer capable of using lattices improved the accuracy of phoneme recognition and word segmentation. Consequently, this result suggests that this word segmentation method considers the multiple hypothesis of speech recognition as a whole and reduces uncertainty such as variability in recognition by using the syllable recognition",
      "g three types of word segmentation methods. A set of syllable sequences is given to the graphical model of SpCoA by each method. This set is used for the learning of spatial concepts as recognized uttered sentences INLINEFORM0 . The remainder of this section is organized as follows: In Section SECREF43 , the conditions and",
      "ered sentences We compared the performance of three types of word segmentation methods for all the considered uttered sentences. It was difficult to weigh the ambiguous syllable recognition and the unsupervised word segmentation separately. Therefore, this experiment considered the positions of a delimiter as a single letter.",
      "results\nof the three considered methods. We found that the unsupervised morphological analyzer capable of using lattices improved the accuracy of phoneme recognition and word segmentation.",
      "word segmentation method latticelm could reduce the variability and errors in the recognition of phonemes in all the utterances. SpCoA achieved more accurate lexical acquisition by performing word segmentation using the lattices of the speech recognition",
      "results\nof unsupervised word segmentation on the basis of the speech recognition",
      "proposed method\ncan learn words related to places from the utterances of sentences. We use an unsupervised word segmentation method latticelm that can directly segment words from the lattices of the speech recognition",
      "results\nshowed that the model performed speech segmentation, word discovery, and visual categorization. Iwahashi et al.",
      "nsupervised word segmentation separately. Therefore, this experiment considered the positions of a delimiter as a single letter. We calculated the matching rate of a phoneme string of a recognition result of each uttered sentence and the correct phoneme string of the teaching data that was suitably segmented into Japanese morphemes using MeCab, which is an off-the-shelf Japanese morphological analyzer that is widely used for natural language processing.",
      "results\nand the language model by using unsupervised word segmentation BIBREF36 . As a result, they achieved robust lexical acquisition.",
      "results\nof PAR. Table TABREF55 presents examples of the word segmentation"
    ],
    "answer": [
      "unsupervised morphological analyzer capable of using lattices"
    ]
  },
  {
    "title": "Localization of Fake News Detection via Multitask Transfer Learning",
    "evidence": [
      "ch size of 128, and a weight decay of 0.1. We use the Adam optimizer and use slanted triangular learning rate schedules BIBREF5. We train the model on a machine with one NVIDIA Tesla V100 GPU for a total of 11 hours. For each pretraining scheme, we checkpoint models every epoch to preserve a copy of the weights such that we may restore them once the model starts overfitting.",
      "er hand, uses the LIAR dataset BIBREF4, which contains 12,836 labeled short statements as well as sources to support the labels. This requirement for large datasets to effectively train fake news detection models from scratch makes it difficult to adapt these techniques into low-resource languages.",
      " preprocessed and tokenized using Byte-Pair Encoding. Corpus statistics for the pretraining corpora are shown on table TABREF17. Experimental Setup ::: Siamese Network Training We train a siamese recurrent neural network as our baseline. For each twin, we use 300 dimensions for the embedding layer and a hidden size of 512 for all hidden state vectors.",
      "l on our prepared text corpora using language modeling as its sole pretraining task, according to the specifications of BIBREF8. We use an embedding dimension of 410, a hidden dimension of 2100, and a maximum sequence length of 256. We use 10 attention heads per multihead attention block, with 16 blocks composing the encoder of the transformer.",
      "length of 256. We use 10 attention heads per multihead attention block, with 16 blocks composing the encoder of the transformer. We use dropout on all linear layers to a probability of 0.1. We initialize all parameters to a standard deviation of 0.02.",
      "ur baseline. For each twin, we use 300 dimensions for the embedding layer and a hidden size of 512 for all hidden state vectors. To optimize the network, we use a regularized cross-entropy objective of the following form: where y$(x_1, x_2)$ = 1 when $x_1$ and $x_2$ are from the same class and 0 otherwise.",
      "he large size of the model. We train the model for 200 epochs with 1,000 steps of learning rate warmup using the Adam optimizer. The model was pretrained for 178 hours on a machine with one NVIDIA Tesla V100 GPU. For ULMFiT, we pretrain a 3-layer AWD-LSTM model with an embedding size of 400 and a hidden size of 1150.",
      "scratch. Both BIBREF1 and BIBREF3 use the Fake News Challenge dataset, with 49,972 labeled stances for each headline-body pairs. BIBREF2, on the other hand, uses the LIAR dataset BIBREF4, which contains 12,836 labeled short statements as well as sources to support the labels.",
      "Results\nand",
      "Results\nand",
      "Results\nand",
      "l, modern fake news detection techniques assume the existence of large, expertly-annotated corpora to train models from scratch. Both BIBREF1 and BIBREF3 use the Fake News Challenge dataset, with 49,972 labeled stances for each headline-body pairs.",
      "ification heads. We finetune both models to the target task for 3 epochs, using a batch size of 32, and a learning rate of 3e-5. For optimization, we use Adam with a warmup steps of 10% the number of steps, comprising 3 epochs."
    ],
    "answer": [
      "The size of the dataset is not explicitly stated in the provided text. However, it is mentioned that the LIAR dataset contains 12,836 labeled short statements, and the Fake News Challenge dataset contains 49,972 labeled stances for each headline-body pairs."
    ]
  },
  {
    "title": "Learning Word Embeddings from the Portuguese Twitter Stream: A Study of some Practical Aspects",
    "evidence": [
      "Results\nshow that using less than 50% of the available training examples for each vocabulary size might result in overfitting.",
      "Results\nshow that using less than 50% of the available training examples for each vocabulary size might result in overfitting. The resulting embeddings obtain an interesting performance on intrinsic evaluation tests when trained a vocabulary containing the 32768 most frequent words in a Twitter sample of relatively small size. Nevertheless,",
      "rds over 10M training examples while keeping a stable validation loss and approximately linear trend on training time per epoch. We also observed that using less than 50\\% of the available training examples for each vocabulary size might result in overfitting.",
      " from 25% to 100%. Three trends are apparent. As we train with more data, we obtain better validation losses. This was expected. The second trend is that by using less than 50% of the data available the model tends to overfit the data, as indicated by the consistent increase in the validation loss after about 15 epochs (check dashed lines in right side of Figure FIGREF28 ).",
      " specially valuable indication for future experiments and for deciding the dimensionality of the final embeddings to distribute. On the right side of Figure FIGREF28 we show how the number of training (and validation) examples affects the loss. For a fixed INLINEFORM0 = 32768 we varied the amount of data used for training from 25% to 100%. Three trends are apparent.",
      "grams would pass the filter. The number of examples collected for each of the values of INLINEFORM6 is shown in Table TABREF16 . Since one of the goals of our experiments is to understand the impact of using different amounts of training data, for each size of vocabulary to be embedded INLINEFORM0 we will run experiments training the models using 25%, 50%, 75% and 100% of the data available.",
      "roducing embeddings with higher dimensionality (to avoid the cosine skewness effect) and training with even larger vocabularies. Also, there is room for experimenting with some of the hyper-parameters of the model itself (e.g. activation functions, dimensions of the layers), which we know have impact on final",
      "oss. For a fixed INLINEFORM0 = 32768 we varied the amount of data used for training from 25% to 100%. Three trends are apparent. As we train with more data, we obtain better validation losses. This was expected.",
      "aining data to save training time. Finally, when not overfitting, the validation loss seems to stabilize after around 20 epochs. We observed no phase-transition effects (the model seems simple enough for not showing that type of behavior). This indicates we have a practical way of safely deciding when to stop training the model. Intrinsic Evaluation Table TABREF30 presents",
      "ch, with validation loss, although being slightly higher, following the same trend. When using 100% we see no model overfitting. We can also observe that the higher is INLINEFORM1 the higher are the absolute values of the loss sets. This is not surprising because as the number of words to predict becomes higher the problem will tend to become harder.",
      "er used general purpose pre-trained word embeddings, or trained from Tweet 2016 dataset or “from some sort of dataset” BIBREF9 . However, participants neither report the size of vocabulary used neither the possible effect it might have on the task specific",
      "tively small sample and focusing on three challenges: volume of training data, vocabulary size and intrinsic evaluation metrics. Using a single GPU, we were able to scale up vocabulary size from 2048 words embedded and 500K training examples to 32768 words over 10M training examples while keeping a stable validation loss and approximately linear trend on training time per epoch.",
      "ulary to be embedded INLINEFORM0 we will run experiments training the models using 25%, 50%, 75% and 100% of the data available. Metrics related with the Learning Process We tracked metrics related to the learning process itself, as a function of the vocabulary size to be embedded INLINEFORM0 and of the fraction of training data used (25%, 50%, 75% and 100%)."
    ],
    "answer": [
      "The experimental results suggest that using less than 50% of the available training examples might result in overfitting."
    ]
  }
]